title,author,year,type,volume,issue,pages,venue,venueShort,publisher,doi,url,abstract,keywords,citedBy,citedByPerYear,quality,objective,technology,data,participants,maturity,device
Usability and Accessibility Assessment of Saudi Arabia Mobile E-Government Websites,"Al-Sakran, Hasan O.; Alsudairi, Mohammed A.",2021,journalArticle,9,,48254 – 48275,IEEE Access,,IEEE,10.1109/ACCESS.2021.3068917,https://doi.org/10.1109/ACCESS.2021.3068917,"The rapid spread of smart mobile technology is transforming the way how governments provide information and services to their citizens. We all are rely more on our devices, namely smartphones, laptops, or desktop to get information and a wide range of services from government websites. Such heavy usage of government websites results in an increased need for efficient and effective delivery of government services. Therefore, mobile government websites' usability and accessibility are essential dimensions that determine the quality and accessibility of mobile e-government. The main objective of this research is to analyze the accessibility and usability aspects of selected public sector websites in Saudi Arabia. This study investigates how well the Saudi mobile e-government websites comply with usability standards and accessibility guidelines recommended in the Web Content Accessibility Guidelines (WCAG). Websites assessments were conducted using manual evaluation and complemented by different automated analysis tools. This study applies a number of evaluation techniques to assess Saudi government websites accessed from desktop or mobile devices, such as an automated website testing technique, mobile-friendliness testing, and content observation technique. Various tools have been used for site evaluation, such as GTmetrix (PageSpeed Score, YSlow Score), WAVE, Google mobile-friendly test, and Dareboost for mobile websites. The study uncovers shortcomings regarding non-compliance to international web standards recommendations. The findings revealed usability and accessibility problems that affect the performance of government websites. In order to improve these websites within these aspects, several recommendations were suggested for improving the usability and accessibility of websites in Saudi Arabia that will make sure different groups of citizens are satisfied with the website features and services provided by them. © 2013 IEEE.",Accessibility guidelines; Accessibility problems; Automated analysis; e-government; Government services; Government websites; Mobile e governments; Observation techniques; Regulatory compliance; Testing; Usability engineering; Web content accessibility guidelines; Web Design; Websites,51,14.57,Q2,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,mobile
Method for semi-automated evaluation of user experience using brain activity,"Aleksander, B.A.I.; Fuglerud, Kristin S.",2018,journalArticle,256,,811 – 820,Studies in Health Technology and Informatics,,IOS Press,10.3233/978-1-61499-923-2-811,https://doi.org/10.3233/978-1-61499-923-2-811,"There is a large interest in user experience today, both from a usability and accessibility point of view. However, in order to verify what the users actually like and don't like, user testing must be conducted. Traditionally, user experience is measured retrospective with surveys and interviews, but this is not the most optimal approach since it does not measure user experience in the moment and it is prone for human error because of our inaccurate memory recollection. Here we propose a method that does semi-automated evaluation of user experience by utilizing electrophysiological equipment that monitors electrical activity of the brain. We describe an approach that together with brain activity monitoring will collect and quantify user experience in a non-intrusive manner. We demonstrate the method by showing how a low cost device can record brain activity during a user test, and auto-detect where the user has difficulties understand or navigating a solution. All this is done in an unsupervised manner, but an observer must still verify the feedback with the actual user to remove false positives. Our method is not limited to digital solutions and can also be used for evaluating user experience of physical installations. © 2018 The authors and IOS Press.",Usability; User experience; automation; Automation; Surveys; Brain; Electroencephalography; Design; human; Humans; User-Computer Interface; human experiment; quantitative analysis; questionnaire; computer interface; Surveys and Questionnaires; Universal Design; Retrospective Studies; retrospective study; brain function; false positive result; conference paper; monitoring; Neurophysiology; Electrophysiology; brain; EEG; Digital solutions; Electrical activities; electroencephalogram; Low-cost devices; Optimal approaches; Semi-automated; Universal design,6,0.92,Q3,Usability attribute evaluation,Descriptive metrics,Physiological signals,yes,prototype,desktop
Towards a catalog of usability smells,"Almeida, Diogo; Campos, José Creissac; Saraiva, João; Silva, João Carlos",2015,conferencePaper,,,175–181,Proceedings of the 30th Annual ACM Symposium on Applied Computing,SAC,ACM,10.1145/2695664.2695670,https://doi.org/10.1145/2695664.2695670,"This paper presents a catalog of smells in the context of interactive applications. These so-called usability smells are indicators of poor design on an application's user interface, with the potential to hinder not only its usability but also its maintenance and evolution. To eliminate such usability smells we discuss a set of program/usability refactorings. In order to validate the presented usability smells catalog, and the associated refactorings, we present a preliminary empirical study with software developers in the context of a real open source hospital management application. Moreover, a tool that computes graphical user interface behavior models, giving the applications' source code, is used to automatically detect usability smells at the model level.",graphical user interfaces; empirical studies; code smells,29,3.05,B,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,desktop
A semi-automated usability evaluation framework for interactive image segmentation systems,"Amrehn, Mario; Steidl, Stefan; Kortekaas, Reinier; Strumia, Maddalena; Weingarten, Markus; Kowarschik, Markus; Maier, Andreas",2019,journalArticle,2019,,,International Journal of Biomedical Imaging,,Wiley,10.1155/2019/1464592,https://doi.org/10.1155/2019/1464592,"For complex segmentation tasks, the achievable accuracy of fully automated systems is inherently limited. Specifically, when a precise segmentation result is desired for a small amount of given data sets, semi-automatic methods exhibit a clear benefit for the user. The optimization of human computer interaction (HCI) is an essential part of interactive image segmentation. Nevertheless, publications introducing novel interactive segmentation systems (ISS) often lack an objective comparison of HCI aspects. It is demonstrated that even when the underlying segmentation algorithm is the same throughout interactive prototypes, their user experience may vary substantially. As a result, users prefer simple interfaces as well as a considerable degree of freedom to control each iterative step of the segmentation. In this article, an objective method for the comparison of ISS is proposed, based on extensive user studies. A summative qualitative content analysis is conducted via abstraction of visual and verbal feedback given by the participants. A direct assessment of the segmentation system is executed by the users via the system usability scale (SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the findings regarding usability aspects in those studies is introduced, conducted solely from the system-measurable user actions during their usage of interactive segmentation prototypes. The prediction of all questionnaire results has an average relative error of 8.9%, which is close to the expected precision of the questionnaire results themselves. This automated evaluation scheme may significantly reduce the resources necessary to investigate each variation of a prototype's user interface (UI) features and segmentation methodologies. © 2019 Mario Amrehn et al.",Article; Automated usability evaluation; automation; Automation; Average relative error; cognitive model; content analysis; Degrees of freedom (mechanics); human; Human computer interaction; Human computer interaction (HCI); image segmentation; Image segmentation; Interactive image segmentation; Interactive segmentation; Iterative methods; prediction; qualitative analysis; questionnaire; Segmentation algorithms; Semiautomatic methods; Surveys; System Usability Scale (SUS); User interfaces,11,2,Q3,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,desktop
Predicting User Engagement with Direct Displays Using Mouse Cursor Information,"Arapakis, Ioannis; Leiva, Luis A.",2016,conferencePaper,,,,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,SIGIR,ACM,10.1145/2911451.2911505,https://doi.org/10.1145/2911451.2911505,"Predicting user engagement with direct displays (DD) is of paramount importance to commercial search engines, as well as to search performance evaluation. However, understanding within-content engagement on a web page is not a trivial task mainly because of two reasons: (1) engagement is subjective and different users may exhibit different behavioural patterns; (2) existing proxies of user engagement (e.g., clicks, dwell time) suffer from certain caveats, such as the well-known position bias, and are not as effective in discriminating between useful and non-useful components. In this paper, we conduct a crowdsourcing study and examine how users engage with a prominent web search engine component such as the knowledge module (KM) display. To this end, we collect and analyse more than 115k mouse cursor positions from 300 users, who perform a series of search tasks. Furthermore, we engineer a large number of meta-features which we use to predict different proxies of user engagement, including attention and usefulness. In our experiments, we demonstrate that our approach is able to predict more accurately different levels of user engagement and outperform existing baselines.",web search; knowledge module; direct displays; mouse cursor tracking; user engagement,53,6.24,A*,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,desktop
A judgment-based model for usability evaluating of interactive systems using fuzzy Multi Factors Evaluation (MFE),"Asemi, Adeleh; Asemi, Asefeh",2022,journalArticle,117,,108411,Applied Soft Computing,,Elsevier,10.1016/j.asoc.2022.108411,https://doi.org/10.1016/j.asoc.2022.108411,"The study aimed to propose a judgment-based evaluation model for usability evaluating of interactive systems. Human judgment is associated with uncertainty and gray information. We used the fuzzy technique for integration, summarization, and distance calculation of quality value judgment. The proposed model is an integrated fuzzy Multi Factors Evaluation (MFE) model based on experts’ judgments in HCI, ISPD, and AMLMs. We provided a Fuzzy Inference System (FIS) for scoring usability evaluation metrics in different interactive systems. A multi-model interactive system is implemented for experimental testing of the model. The achieved results from the proposed model and experimental tests are compared using statistical correlation tests. The results show the ability of the proposed model for usability evaluation of interactive systems without the need for conducting empirical tests. It is concluded that applying a dataset in a neuro-FIS and training system cause to produce more than a hundred effective rules. The findings indicate that the proposed model can be applied for interactive system evaluation, informative evaluation, and complex empirical tests. Future studies may improve the FIS with the integration of artificial neural networks.",Experts’ judgment; Fuzzy Inference System (FIS); Interactive systems; Multiple Factors Evaluation (MFE); Usability evaluation; Usability testing,8,3.2,Q1,Usability attribute evaluation,Rule-based systems,Audio,yes,prototype,desktop
The Combination of Contextualized Topic Model and MPNet for User Feedback Topic Modeling,"Asnawi, Mohammad Hamid; Pravitasari, Anindya Apriliyanti; Herawan, Tutut; Hendrawati, Triyani",2023,journalArticle,11,,130272-130286,IEEE Access,,IEEE,10.1109/ACCESS.2023.3332644,https://doi.org/10.1109/ACCESS.2023.3332644,"In the era of big data and ubiquitous internet connectivity, user feedback data plays a crucial role in product development and improvement. However, extracting valuable insights from the vast pool of unstructured text data found in user feedback presents significant challenges. In this paper, we propose an innovative approach to tackle this challenge by combining the Contextualized Topic Model (CTM) and the Masked and Permuted Pre-training for Language Understanding (MPNet) model. Our approach aims to create a more accurate and context-aware topic model that enhances the understanding of user experiences and opinions. To achieve this, we first search for the optimal number of topics, focusing on generating distinguishable, general, and unique topics. Next, we perform hyperparameter optimization to fine-tune the model and maximize coherence metrics. The result is an exceptionally effective model that outperforms established topic modeling methods, including LSI, NMF, LDA, HDP, NeuralLDA, ProdLDA, ETM, and the default CTM, achieving the highest coherence CV score of 0.7091. In this study, the combination of CTM and MPNet has proven highly effective in the context of user feedback topic modeling. This model excels in generating coherent, distinguishable, and highly relevant user feedback topics, capturing the nuanced nature of user feedback data. The topics generated from this model include ‘Music and Audio Streaming,’ ’Application Performance,’ ‘Banking, Financial Services, and Customer Support,’ ’User Experience,’ ‘Other Topics,’ ’Application Content,’ and ‘Application Features.’ Our contributions include a powerful tool for developers to gain deeper insights, prioritize actions, and enhance user satisfaction by incorporating feedback into future product iterations. Furthermore, we introduce a new dataset as an open-source resource for further exploration and validation of user feedback analysis techniques and general natural language processing applications. With our proposed approach, we strive to drive business success, improve user experiences, and inform data-driven decision-making processes, ultimately benefiting both developers and users alike.",Analytical models; Coherence; Context modeling; Contextualized topic model; Data mining; Data models; Feedback; Measurement; MPNet; natural language processing; Natural language processing; Predictive models; topic model; user feedback,3,2,Q2,Feedback evaluation,"NLP, LLM",Text,yes,prototype,device independent
Domain Usability Evaluation,"Bacikova, Michaela; Poruban, Jaroslav; Sulir, Matus; Chodarev, Sergej; Steingartner, William; Madeja, Matej",2021,journalArticle,10,16,,Electronics,,MDPI,10.3390/electronics10161963,https://doi.org/10.3390/electronics10161963,"Contemporary software systems focus on usability and accessibility from the point of view of effectiveness and ergonomics. However, the correct usage of the domain dictionary and the description of domain relations and properties via their user interfaces are often neglected. We use the term domain usability (DU) to describe the aspects of the user interface related to the terminology and domain. Our experience showed that poor domain usability reduces the memorability and effectiveness of user interfaces. To address this problem, we describe a method called ADUE (Automatic Domain Usability Evaluation) for the automated evaluation of selected DU properties on existing user interfaces. As a prerequisite to the method, metrics for formal evaluation of domain usability, a form stereotype recognition algorithm, and general application terms filtering algorithm have been proposed. We executed ADUE on several real-world Java applications and report our findings. We also provide proposals to modify existing manual usability evaluation techniques for the purpose of domain usability evaluation.",User experience; Graphical user interfaces; user experience; human-computer interaction; graphical user interfaces; Usability evaluation methods; domain usability; domain-specific languages; usability evaluation methods; Domain usability; Domain-specific languages; Human–computer interaction,5,1.43,Q3,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,desktop
Automated model-based android GUI testing using multi-level GUI comparison criteria,"Baek, Young-Min; Bae, Doo-Hwan",2016,conferencePaper,,,238 – 249,Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,ASE,ACM,10.1145/2970276.2970313,https://doi.org/10.1145/2970276.2970313,"Automated Graphical User Interface (GUI) testing is one of the most widely used techniques to detect faults in mobile applications (apps) and to test functionality and usability. GUI testing exercises behaviors of an application under test (AUT) by executing events on GUIs and checking whether the app behaves correctly. In particular, because Android leads in market share of mobile OS platforms, a lot of research on automated Android GUI testing techniques has been performed. Among various techniques, we focus on model-based Android GUI testing that utilizes a GUI model for systematic test generation and effective debugging support. Since test inputs are generated based on the underlying model, accurate GUI modeling of an AUT is the most crucial factor in order to generate effective test inputs. However, most modern Android apps contain a number of dynamically constructed GUIs that make accurate behavior modeling more challenging. To address this problem, we propose a set of multi-level GUI Comparison Criteria (GUICC) that provides the selection of multiple abstraction levels for GUI model generation. By using multilevel GUICC, we conducted empirical experiments to identify the inuence of GUICC on testing effectiveness. Results show that our approach, which performs model-based testing with multi-level GUICC, achieved higher effectiveness than activity-based GUI model generation. We also found that multi-level GUICC can alleviate the inherent state explosion problems of existing a single-level GUICC for behavior modeling of real-world Android apps by exibly manipulating GUICC. © 2016 ACM.",Graphical user interfaces; GUI testing; Testing; User interfaces; Software engineering; Automation; Embedded systems; Android (operating system); Model checking; Competition; Model-based test; Graphical user interfaces (GUI); Testing effectiveness; Android applications; Application under tests; Comparison criterion; State explosion problems; Android application testing; GUI comparison criteria; GUI model generation; Model-based test input generation,205,24.12,A,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,mobile
Auto-Extraction and Integration of Metrics for Web User Interfaces,"Bakaev, Maxim; Heil, Sebastian; Khvorostov, Vladimir; Gaedke, Martin",2018,journalArticle,17,6–7,561-590,Journal of Web Engineering,,River Publishers,10.13052/jwe1540-9589.17676,https://doi.org/10.13052/jwe1540-9589.17676,"Metric-based assessment of web user interface (WUI) quality attributes is shifting from code (HTML/CSS) analysis to mining webpages' visual representations based on image recognition techniques. In our paper, we describe a visual analysis tool which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of the interface elements' spatial allocation. The implementation is based on OpenCV (image recognition functions), dlib (trained detector for the elements' classification), and Tesseract (label and content text recognition). The JSON representation is used to automatically calculate several metrics related to visual complexity, which is known to have major effect on user experience with UIs. We further describe a WUI measurement platform that allows integration of the currently dispersed sets of metrics from different providers and demonstrate the platform's use with several remote services. We perform statistical analysis of the collected metrics in relation to complexity-related subjective evaluations obtained from 63 human subjects of various nationalities. Finally, we build predictive models for visual complexity and show that their accuracy can be improved by integrating the metrics from different sets. Regressions with the single index of visual complexity metric that we proposed had R2=0.460, while the best joint model with 4 metrics had R2=0.647.",Automated metrics; HCI Vision; web design mining; visual complexity,39,6,Q4,Visual complexity evaluation,"Deep learning, Computer vision",Images,no,tool,desktop
HCI Vision for Automated Analysis and Mining of Web User Interfaces,"Bakaev, Maxim; Heil, Sebastian; Khvorostov, Vladimir; Gaedke, Martin",2018,conferencePaper,,,136-144,Web Engineering,ICWE,Springer Nature,10.1007/978-3-319-91662-0_10,https://doi.org/10.1007/978-3-319-91662-0_10,"Most techniques for webpage structure and design mining are based on code analysis and are detached from a human user’s perception of the web user interface (WUI). Our paper is dedicated to approaches that instead focus on analysis of webpage’s visual representation – the way it is rendered in different browsers and environments and delivered to the end user. Specifically, we describe the software tool that we built, which takes a WUI screenshot and produces structured and machine-readable representation (JSON) of interface elements as made out by a human user. The implementation is based on OpenCV (image recognition functions), dlib (trained detector for the elements’ classification), and Tesseract (label and content text recognition). To demonstrate feasibility of the approach, we describe application of our analyzer tool to auto-calculate certain measures for a WUI and to predict users’ subjective impressions. Particularly, we assess UI visual complexity, which is known to significantly influence both cognitive and affective aspects of interaction. The results suggest the analyzer’s output is mostly characteristic of the users’ visual perception and can be useful for auto-assessing and comparing WUIs.",Web design mining; HCI vision; Image recognition; Human factors; Visual complexity,30,4.62,B,Visual complexity evaluation,"Deep learning, Computer vision",Images,no,tool,desktop
Which feature is unusable? Detecting usability and user experience issues from user reviews,"Bakiu, Elsa; Guzman, Emitza",2017,conferencePaper,,,182 – 187,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017",REW,IEEE,10.1109/REW.2017.76,https://doi.org/10.1109/REW.2017.76,"Usability and user experience (UUX) strongly affect software quality and success. User reviews allow software users to report UUX issues. However, this information can be difficult to access due to the varying quality of the reviews, its large numbers and unstructured nature. In this work we propose an approach to automatically detect the UUX strengths and issues of software features according to user reviews. We use a collocation algorithm for extracting the features, lexical sentiment analysis for uncovering users' satisfaction about a particular feature and machine learning for detecting the specific UUX issues affecting the software application. Additionally, we present two visualizations of the results. An initial evaluation of the approach against human judgement obtained mixed results. © 2017 IEEE.",Application programs; Computer software selection and evaluation; Data mining; Feature extraction; Learning systems; Natural language processing systems; Requirements engineering; Software Evolution; Text mining; Usability; User experience; User feedback,75,10,-,Feedback evaluation,"NLP, Machine learning",Text,yes,prototype,device independent
uxSense: Supporting User Experience Analysis with Visualization and Computer Vision,"Batch, Andrea; Ji, Yipeng; Fan, Mingming; Zhao, Jian; Elmqvist, Niklas",2024,journalArticle,30,7,3841-3856,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2023.3241581,https://doi.org/10.1109/TVCG.2023.3241581,"Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",Annotations; computer vision; Data visualization; deep learning; evaluation; Feature extraction; Gaze tracking; machine learning; Measurement; Usability; video analytics; visual analytics; Visual analytics; Visualization,9,18,Q1,Research assistants,"NLP, Machine learning, Computer vision","User interactions, Audio, Video",yes,prototype,desktop
Detecting and Explaining Usability Issues of Consumer Electronic Products,"Benvenuti, Dario; Buda, Emanuele; Fraioli, Francesca; Marrella, Andrea; Catarci, Tiziana",2021,conferencePaper,,,298-319,Human-Computer Interaction – INTERACT 2021,INTERACT,Springer Nature,10.1007/978-3-030-85610-6_18,https://doi.org/10.1007/978-3-030-85610-6_18,"Usability of a consumer electronic product (CEP) is one of the most important factors that the users consider in purchasing a CEP as well as functionality, price, etc. This has led many companies to realize new shapes of user interfaces (UIs) and styles of interaction for CEPs, ranging from modern touchscreens to physical controls and displays of any kind. Even if the general feeling is that such increased interactivity may enhance the overall user experience, the side effect is that often a CEP’s UI provides too many functions that are difficult to learn and use without referring to the user manual, leading to many usability issues. In this paper, we leverage a case study in the CEPs sector to present a novel log-based evaluation technique in the field of Human-Computer Interaction (HCI). Our technique allows us not only to keep track of the user interactions with a CEP’s UI during its daily use, but also to understand what has gone wrong during a user interaction, detecting which user actions have caused usability issues and suggesting explanations for solving them, thus providing a crucial feedback to improve the design of the CEP’s UI next version.",Heuristic evaluation; Interaction model; Log study; Trace alignment; Usability of consumer electronic products (CEPs); User interface (UI) Log,3,0.86,B,Usability issue encounter detection,Rule-based systems,User interactions,yes,prototype,microwave
Enhancing Interface Design with AI: An Exploratory Study on a ChatGPT-4-Based Tool for Cognitive Walkthrough Inspired Evaluations,"Bisante, Alba; Datla, Venkata Srikanth Varma; Panizzi, Emanuele; Trasciatti, Gabriella; Zeppieri, Stefano",2024,conferencePaper,,,,Proceedings of the 2024 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3656650.3656676,https://doi.org/10.1145/3656650.3656676,"This paper introduces CWGPT, a ChatGPT-4-based tool designed for Cognitive Walkthrough (CW) inspired evaluations of web interfaces. The primary goal is to assist users, particularly students and inexperienced designers, in evaluating web interfaces. Our tool, operating as a conversational agent, provides detailed evaluations of a user-specified task by intelligently guessing the subtasks and actions required to accomplish them, answering the standard CW questions, and providing helpful feedback and practical suggestions to improve the usability of the analyzed interface. For our study, we selected a group of web applications designed by students from a Web and Software Architecture course. We compare the outcome of the CWs we executed on ten web apps against the corresponding CWGPT analyses. We then describe the study we conducted involving five author-students to assess the tool's efficacy in helping them recognize and solve usability issues. In addition to introducing a novel adaptation of ChatGPT, the outcomes of the described experience underscore the promising potential of AI in usability evaluations. © 2024 Owner/Author.",Application programs; ChatGPT; Cognitive walkthrough; Conversational agents; Exploratory studies; GPT; Interface designs; Students; Subtask; Usability engineering; WEB application; Web applications; Web interface,2,4,B,Research assistants,"LLM, ChatBot, Computer vision",Images,no,tool,desktop
Automatic usability and stress analysis in mobile biometrics,"Blanco-Gonzalo, Ramon; Sanchez-Reillo, Raul; Miguel-Hurtado, Oscar; Bella-Pulgarin, Eric",2014,journalArticle,32,12,1173 – 1180,Image and Vision Computing,,Elsevier,10.1016/j.imavis.2014.09.003,https://doi.org/10.1016/j.imavis.2014.09.003,"This article focuses on the usability evaluation of biometric recognition systems in mobile devices. In particular, a behavioural modality has been used: the dynamic handwritten signature. Testing usability in behavioural modalities involves a big challenge due to the number of degrees of freedom that users have in interacting with sensors, as well as the variety of capture devices to be used. In this context we propose a usability evaluation that allows users to interact freely with the system while minimizing errors at the same time. The participants signed in a smartphone with a stylus through the different phases in the use of a biometric system: training, enrolment and verification. In addition, a profound study on the automation of the evaluation processes has been done, so as to reduce the resources employed. The influence of the users' stress has also been studied, to obtain conclusions on its impact on both the usability systems in scenarios where the user may suffer a certain level of stress, such as in courts, banks or even shopping. In brief, the results shown in this paper prove not only that a dynamic handwritten signature is a trustable solution for a large number of applications in the real world, but also that the evaluation of the usability of biometric systems can be carried out at lower costs and shorter duration. © 2014 Elsevier B.V.",Biometric recognition system; Biometric systems; Biometrics; Character recognition; Degrees of freedom (mechanics); Handwritten signatures; Mobile biometrics; Mobile computing; Number of degrees of freedom; Real-world; Stress analysis; Stresses; Usability; Usability engineering; Usability evaluation; Well testing,25,2.38,Q1,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,mobile
Entropy and Compression Based Analysis of Web User Interfaces,"Boychuk, Egor; Bakaev, Maxim",2019,conferencePaper,,,253-261,Web Engineering,ICWE,Springer Nature,10.1007/978-3-030-19274-7_19,https://doi.org/10.1007/978-3-030-19274-7_19,"In our paper we explore whether user visual perception of web interfaces (WUI) can be predicted by certain quantitative characteristics of WUI screenshots. The considered metrics are JPEG file size, PNG file size, and information entropy value calculated with frequency-based MATLAB’s entropy(I) function. We ran survey with 70 subjects who provided subjective evaluations of complexity, aesthetics and orderliness for 497 website homepages. The results suggest that all the three metrics were significant, and the proposed regression models were considerably better than the respective baseline models that only used the popular JPEG-based metric. Remarkably, the entropy metric had significant positive correlations with aesthetic and orderliness evaluations, but not with the size of the image. We believe our findings might be used in development of automated WUI analysis tools to aid web engineers in their work.",Information entropy; Web interfaces; Cognitive models,20,3.64,B,Visual complexity evaluation,Descriptive metrics,Images,no,prototype,desktop
Visualizations of User’s Paths to Discover Usability Problems,"Buono, Paolo; Desolda, Giuseppe; Lanzilotti, Rosa; Costabile, Maria Francesca; Piccinno, Antonio",2019,conferencePaper,,,689–692,"Human-Computer Interaction – INTERACT 2019: 17th IFIP TC 13 International Conference, Paphos, Cyprus, September 2–6, 2019, Proceedings, Part IV",INTERACT,Springer Nature,10.1007/978-3-030-29390-1_64,https://doi.org/10.1007/978-3-030-29390-1_64,This paper reports on an on-going work that investigates the use of visualization techniques to help evaluators discovering usability problems by visualizing data collected during usability tests of web sites. Two visualization techniques are described and some results of the evaluation study that compared the two techniques are provided.,Usability test; Visualization techniques; Usability problems discovery; Web site evaluation,1,0.18,A,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,desktop
Towards the Detection of UX Smells: The Support of Visualizations,"Buono, Paolo; Caivano, Danilo; Costabile, Maria Francesca; Desolda, Giuseppe; Lanzilotti, Rosa",2020,journalArticle,8,,6901-6914,IEEE Access,,IEEE,10.1109/ACCESS.2019.2961768,https://doi.org/10.1109/ACCESS.2019.2961768,"Daily experiences in working with various types of computer systems show that, despite the offered functionalities, users have many difficulties, which affect their overall User eXperience (UX). The UX focus is on aesthetics, emotions and social involvement, but usability has a great influence on UX. Usability evaluation is acknowledged as a fundamental activity of the entire development process in software practices. Research in Human-Computer Interaction has proposed methods and tools to support usability evaluation. However, when performing an evaluation study, novice evaluators still have difficulties to identify usability problems and to understand their causes: they would need easier to use and possibly automated tools. This article describes four visualization techniques whose aim is to support the work of evaluators when performing usability tests to evaluate websites. Specifically, they help detect “usability smells”, i.e. hints on web pages that might present usability problems, by visualizing the paths followed by the test participants when navigating in a website to perform a test task. A user study with 15 participants compared the four techniques and revealed that the proposed visualizations have the potential to be valuable tools for novice usability evaluators. These first results should push researchers towards the development of further tools that are capable to support the detection of other types of UX smells in the evaluation of computer systems and that can be translated into common industry practices.",Usability; Task analysis; Testing; usability; User interfaces; Visualization; Tools; Web pages; Data visualization; design for quality; user eXperience; website evaluation,24,5.33,Q2,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,desktop
Testing the Usability and Accessibility of Smart TV Applications Using an Automated Model-Based Approach,"Bures, Miroslav; MacIk, Miroslav; Ahmed, Bestoun S.; Rechtberger, Vaclav; Slavik, Pavel",2020,journalArticle,66,2,134 – 143,IEEE Transactions on Consumer Electronics,,IEEE,10.1109/TCE.2020.2986049,https://doi.org/10.1109/TCE.2020.2986049,"As the popularity of Smart Televisions (TVs) and interactive Smart TV applications (apps) has recently grown, the usability of these apps has become an important quality characteristic. Previous studies examined Smart TV apps from a usability perspective. However, these methods are mainly manual, and the potential of automated model-based testing methods for usability testing purposes has not yet been fully explored. In this article, we propose an approach to test the usability of Smart TV apps based on the automated generation of a Smart TV user interaction model from an existing app by a specialized automated crawler. By means of this model, defined user tasks in the Smart TV app can be evaluated automatically in terms of their feasibility and estimated user effort, which reflects the usability of the analyzed app. This analysis can be applied in the context of regular users and users with various specific needs. The findings from this model-based automated analysis approach can be used to optimize the user interface of a Smart TV app to increase its usability, accessibility, and quality. © 1975-2011 IEEE.",Usability; Task analysis; Testing; User interfaces; Computational modeling; Usability engineering; Automation; Analytical models; Crawlers; model-based testing; Smart TV; smart TV application; Usability testing; user interface quality; Model-based OPC; Quality control; Model checking; Automated generation; Automated analysis; Automated modeling; Interactive television; Quality characteristic; Smart-TV; User Interaction Modeling,22,4.89,Q2,Usability attribute evaluation,Rule-based systems,"Source code, User interactions",yes,prototype,TV
Empirical validation of an automatic usability evaluation method,"Cassino, Rosanna; Tucci, Maurizio; Vitiello, Giuliana; Francese, Rita",2015,journalArticle,28,,1 – 22,Journal of Visual Languages and Computing,,Elsevier,10.1016/j.jvlc.2014.12.002,https://doi.org/10.1016/j.jvlc.2014.12.002,"Today, the success of a software application strongly depends on the usability of its interface, so the evaluation of interfaces has become a crucial aspect of software engineering. It is recognized that automatic tools for graphical user interface evaluation may greatly reduce the costs of traditional activities performed during expert evaluation or user testing in order to estimate the success probability of an application. However, automatic methods need to be empirically validated in order to prove their effectiveness with respect to the attributes they are supposed to evaluate.In this work, we empirically validate a usability evaluation method conceived to assess consistency aspects of a GUI with no need to analyze the back-end. We demonstrate the validity of the approach by means of a comparative experimental study, where four web sites and a stand-alone interactive application are analyzed and the results compared to those of a human-based usability evaluation. The analysis of the results and the statistical correlation between the tool's rating and humans' average ratings show that the proposed methodology can indeed be a useful complement to standard techniques of usability evaluation. © 2014 Elsevier Ltd.",Interactive systems evaluation; Summative usability evaluation; Usability engineering,31,3.26,Q3,Automatic guideline evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Submitting surveys via a conversational interface: An evaluation of user acceptance and approach effectiveness,"Celino, Irene; Re Calegari, Gloria",2020,journalArticle,139,,102410,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2020.102410,https://doi.org/10.1016/j.ijhcs.2020.102410,"Conversational interfaces are currently on the rise: more and more applications rely on a chat-like interaction pattern to increase their acceptability and to improve user experience. Also in the area of questionnaire design and administration, interaction design is increasingly looked at as an important ingredient of a digital solution. For those reasons, we designed and developed a conversational survey tool to administer questionnaires with a colloquial form through a chat-like Web interface. In this paper, we present the evaluation results of our approach, taking into account both the user point of view – by assessing user acceptance and preferences in terms of survey compilation experience – and the survey design perspective – by investigating the effectiveness of a conversational survey in comparison to a traditional questionnaire. We show that users clearly appreciate the conversational form and prefer it over a traditional approach and that, from a data collection point of view, the conversational method shows the same reliability and a higher response quality with respect to a traditional questionnaire.",Conversational survey; Quantitative research; Questionnaire reliability; Questionnaire response quality; Survey design and administration; User experience,43,9.56,Q1,Research assistants,"Rule-based systems, ChatBot",Text,yes,prototype,desktop
A Novel User Emotional Interaction Design Model Using Long and Short-Term Memory Networks and Deep Learning,"Chen, Xiang; Huang, Rubing; Li, Xin; Xiao, Lei; Zhou, Ming; Zhang, Linghao",2021,journalArticle,12,,,Frontiers in Psychology,,Frontiers Media SA,10.3389/fpsyg.2021.674853,https://doi.org/10.3389/fpsyg.2021.674853,"Emotional design is an important development trend of interaction design. Emotional design in products plays a key role in enhancing user experience and inducing user emotional resonance. In recent years, based on the user's emotional experience, the design concept of strengthening product emotional design has become a new direction for most designers to improve their design thinking. In the emotional interaction design, the machine needs to capture the user's key information in real time, recognize the user's emotional state, and use a variety of clues to finally determine the appropriate user model. Based on this background, this research uses a deep learning mechanism for more accurate and effective emotion recognition, thereby optimizing the design of the interactive system and improving the user experience. First of all, this research discusses how to use user characteristics such as speech, facial expression, video, heartbeat, etc., to make machines more accurately recognize human emotions. Through the analysis of various characteristics, the speech is selected as the experimental material. Second, a speech-based emotion recognition method is proposed. The mel-Frequency cepstral coefficient (MFCC) of the speech signal is used as the input of the improved long and short-term memory network (ILSTM). To ensure the integrity of the information and the accuracy of the output at the next moment, ILSTM makes peephole connections in the forget gate and input gate of LSTM, and adds the unit state as input data to the threshold layer. The emotional features obtained by ILSTM are input into the attention layer, and the self-attention mechanism is used to calculate the weight of each frame of speech signal. The speech features with higher weights are used to distinguish different emotions and complete the emotion recognition of the speech signal. Experiments on the EMO-DB and CASIA datasets verify the effectiveness of the model for emotion recognition. Finally, the feasibility of emotional interaction system design is discussed. © Copyright © 2021 Chen, Huang, Li, Xiao, Zhou and Zhang.",emotion recognition; interaction design; LSTM; self-attention mechanism; speech,10,2.86,Q1,Affective state detection,Deep learning,Audio,yes,prototype,device independent
Visual saliency model based on crowdsourcing eye tracking data and its application in visual design,"Cheng, Shiwei; Fan, Jing; Hu, Yilin",2023,journalArticle,27,3,613-630,Personal and Ubiquitous Computing,,Springer Nature,10.1007/s00779-020-01463-7,https://doi.org/10.1007/s00779-020-01463-7,"The visual saliency models based on low-level features of an image have the problem of low accuracy and scalability, while the visual saliency models based on deep neural networks can effectively improve the prediction performance, but require a large amount of training data, e.g., eye tracking data, to achieve good results. However, the traditional eye tracking method is limited by high equipment and time cost, complex operation process, low user experience, etc. Therefore, this paper proposed a visual saliency model based on crowdsourcing eye tracking data, which was collected by gaze recall with self-reporting from crowd workers. Parameter optimization on our crowdsourcing method was explored, and it came out that the accuracy of gaze data reached 1° of visual angle, which was 3.6% higher than other existed crowdsourcing methods. On this basis, we collected a webpage dataset of crowdsourcing gaze data and constructed a visual saliency model based on a fully convolutional neural network (FCN). The evaluation results showed that after trained by crowdsourcing gaze data, the model performed better, such as prediction accuracy increased by 44.8%. Also, our model outperformed the existing visual saliency models. We also applied our model to help webpage designers evaluate and revise their visual designs, and the experimental results showed that the revised design obtained improved ratings by 8.2% compared to the initial design.",Visual saliency model; Visual attention; Crowd computing; Eye tracking; Human-computer interaction,13,8.67,Q2,Perceived affordance evaluation,"Deep learning, Computer vision",Images,yes,prototype,desktop
Offline and Online Satisfaction Prediction in Open-Domain Conversational Systems,"Choi, Jason Ingyu; Ahmadvand, Ali; Agichtein, Eugene",2019,conferencePaper,,,1281–1290,Proceedings of the 28th ACM International Conference on Information and Knowledge Management,CIKM,ACM,10.1145/3357384.3358047,https://doi.org/10.1145/3357384.3358047,"Predicting user satisfaction in conversational systems has become critical, as spoken conversational assistants operate in increasingly complex domains. Online satisfaction prediction (i.e., predicting satisfaction of the user with the system after each turn) could be used as a new proxy for implicit user feedback, and offers promising opportunities to create more responsive and effective conversational agents, which adapt to the user's engagement with the agent. To accomplish this goal, we propose a conversational satisfaction prediction model specifically designed for open-domain spoken conversational agents, called ConvSAT. To operate robustly across domains, ConvSAT aggregates multiple representations of the conversation, namely the conversation history, utterance and response content, and system- and user-oriented behavioral signals. We first calibrate ConvSAT performance against state of the art methods on a standard dataset (Dialogue Breakdown Detection Challenge) in an online regime, and then evaluate ConvSAT on a large dataset of conversations with real users, collected as part of the Alexa Prize competition. Our experimental results show that ConvSAT significantly improves satisfaction prediction for both offline and online setting on both datasets, compared to the previously reported state-of-the-art approaches. The insights from our study can enable more intelligent conversational systems, which could adapt in real-time to the inferred user satisfaction and engagement.",conversation context representation; conversational system evaluation; satisfaction prediction in conversational systems,32,5.82,A,Usability attribute evaluation,Deep learning,"User interactions, Text",yes,prototype,desktop
User Experience Evaluation through Automatic A/B Testing,"Cruz Gardey, Juan; Garrido, Alejandra",2020,conferencePaper,,,25-26,Companion Proceedings of the 25th International Conference on Intelligent User Interfaces,IUI,ACM,10.1145/3379336.3381514,https://doi.org/10.1145/3379336.3381514,"The goal of this research is to develop an A/B testing method to automatically compare the user experience (UX) of alternative designs for a web application in a real context with a large number of users. The challenge that it poses is to find mechanisms to predict the UX with machine learning techniques. This submission outlines the motivation, research goal, current status and remaining work.",User experience; Machine learning; User interfaces; User Experience; Machine Learning; Learning systems; A/B testing; User Interaction; WEB application; Machine learning techniques; Alternative designs; User experience evaluations; User interaction; Research goals; User experiences (ux); A/b testing,18,4,A,Usability attribute evaluation,Machine learning,User interactions,yes,concept,desktop
"Accessibility, usability, and security evaluation of Hungarian government websites","Csontos, Balázs; Heckl, István",2021,journalArticle,20,1,139-156,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-020-00716-9,https://doi.org/10.1007/s10209-020-00716-9,"Public sector bodies are increasingly relying on the Internet. On this channel, indispensable information is transmitted to the public and a wide range of services is already available. Therefore, the usability, accessibility, and the security of these websites are very important. Accessibility is particularly crucial for persons with disabilities. The accessibility of public service websites is regulated by a number of laws; among others, the directive “on the accessibility of the websites and mobile applications of public sector bodies” adopted by the European Parliament in 2016. This obliges all European Union member states to make all public sector websites and mobile applications accessible by 23 September 2021. In practice, this means that websites must fulfil the level AA recommendations in WCAG 2.1. In our study, a website assessment method is developed by comparing different analytical tools. With this method, we analysed how Hungarian websites of public sector bodies fulfil the requirements of the directive. We have also investigated how well they comply with usability and security guidelines. The results showed that none of the 25 websites of the examined Hungarian public sector bodies could completely fulfil the recommendations of the Web Content Accessibility Guidelines (WCAG) and that half of the websites had only the lowest level of compliance in usability tests. From the security point of view, almost half of the websites use outdated server versions and programming language, which is very critical. We have proposed several suggestions to address the major problems, so website developers and administrators can improve the accessibility, usability, and security aspects of these websites.",Accessibility; Government websites; Hungary; Security; Usability,69,19.71,Q3,Usability attribute evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Usability evaluation of mobile health application from AI perspective in rural areas of Pakistan,"Dahri, Abdul Samad; Al-Athwari, Ahmaed; Hussain, Azham",2019,journalArticle,13,11,213 – 225,International Journal of Interactive Mobile Technologies,,International Association of Online Engineering,10.3991/ijim.v13i11.11513,https://doi.org/10.3991/ijim.v13i11.11513,"Purpose: This study endorses AI enabled Mobile Health application and investigates usability evaluation of the Mobile Health application by patients' task performance evaluation and satisfaction. Materials and Methods: International Organization for Standards (ISO) 9241-11 standard metrics were used and 15 patients performed tasks on task success rate, errors, efficiency (time spent), satisfaction (SUS scale). Results: Getting registered was a top easy task while finding a relevant doctor was the most difficult task for users. The satisfaction scores by SUS suggest good rather excellent application user experience. Male were successful task achievers, while educational level and mobile know-how influence the usability scores in terms of time consumed, task errors occurred, and task completed. Conclusion: Methods used in this study suggest future research from different contexts. Using ISO 9241-11 usability standards, the SUS instrument for satisfaction, and measuring user characteristics influence performance and can provide considerable Mobile Health design. © International Association of Online Engineering.",Artificial intelligence; Developing countries; Healthcare; Mobile health (mHealth); Usability,17,3.09,Q3,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,mobile
Detecting Emotions Through Machine Learning for Automatic UX Evaluation,"Desolda, Giuseppe; Esposito, Andrea; Lanzilotti, Rosa; Costabile, Maria F.",2021,conferencePaper,12934,,270-279,Human-Computer Interaction–INTERACT 2021: 18th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-030-85613-7_19,https://doi.org/10.1007/978-3-030-85613-7_19,"Although User eXperience (UX) is widely acknowledged as an important aspect of software products, its evaluation is often neglected during the development of most software products, primarily because developers think that it is resource-demanding and complain about the fact that is scarcely automated. Various attempts have been made to develop tools that support and automate the execution of tests with users. This paper is about an ongoing research work that exploits Machine Learning (ML) for automatic UX evaluation, specifically for understanding users' emotions by analyzing the log data of the users' interactions with websites. The approach described aims at overcoming some limitations of existing proposals based on ML.",Automatic UX evaluation; Usability; User eXperience; User experience; Machine learning; Human computer interaction; Software products; ITS evaluation; User experiences (ux); Log data,16,4.57,B,Affective state detection,"Machine learning, Computer vision","User interactions, Video",yes,prototype,desktop
An Intelligent Framework for Website Usability,"Dingli, Alexiei; Cassar, Sarah",2014,journalArticle,2014,,,Advances in Human-Computer Interaction,,Wiley,10.1155/2014/479286,https://doi.org/10.1155/2014/479286,"With the major advances of the Internet throughout the past couple of years, websites have come to play a central role in the modern marketing business program. However, simply owning a website is not enough for a business to prosper on the Web. Indeed, it is the level of usability of a website that determines if a user stays or abandons it for another competing one. It is therefore crucial to understand the importance of usability on the web, and consequently the need for its evaluation. Nonetheless, there exist a number of obstacles preventing software organizations from successfully applying sound website usability evaluation strategies in practice. From this point of view automation of the latter is extremely beneficial, which not only assists designers in creating more usable websites, but also enhances the Internet users' experience on the Web and increases their level of satisfaction. As a means of addressing this problem, an Intelligent Usability Evaluation (IUE) tool is proposed that automates the usability evaluation process by employing a Heuristic Evaluation technique in an intelligent manner through the adoption of several research-based AI methods. Experimental results show there exists a high correlation between the tool and human annotators when identifying the considered usability violations.",,69,6.57,Q3,Automatic guideline evaluation,Rule-based systems,Source code,no,tool,desktop
Webthetics: Quantifying webpage aesthetics with deep learning,"Dou, Qi; Zheng, Xianjun Sam; Sun, Tongfang; Heng, Pheng-Ann",2019,journalArticle,124,,56-66,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2018.11.006,https://doi.org/10.1016/j.ijhcs.2018.11.006,"As web has become the most popular media to attract users and customers worldwide, webpage aesthetics plays an increasingly important role for engaging users online and impacting their user experience. We present a novel method using deep learning to automatically compute and quantify webpage aesthetics. Our deep neural network, named as Webthetics, which is trained from the collected user rating data, can extract representative features from raw webpages and quantify their aesthetics. To improve the model performance, we propose to transfer the knowledge from image style recognition task into our network. We have validated that our method significantly outperforms previous method using hand-crafted features such as colorfulness and complexity. These promising results indicate that our method can serve as an effective and efficient means for providing objective aesthetics evaluation during the design process.",Webpage aesthetics; Deep learning; Web visual design; User experience,71,12.91,Q1,Aesthetics evaluation,"Deep learning, Computer vision",Images,no,prototype,desktop
Optimizing User Interface Layouts via Gradient Descent,"Duan, Peitong; Wierzynski, Casimir; Nachman, Lama",2020,conferencePaper,,,1–12,Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3313831.3376589,https://doi.org/10.1145/3313831.3376589,"Automating parts of the user interface (UI) design process has been a longstanding challenge. We present an automated technique for optimizing the layouts of mobile UIs. Our method uses gradient descent on a neural network model of task performance with respect to the model's inputs to make layout modifications that result in improved predicted error rates and task completion times. We start by extending prior work on neural network based performance prediction to 2-dimensional mobile UIs with an expanded interaction space. We then apply our method to two UIs, including one that the model had not been trained on, to discover layout alternatives with significantly improved predicted performance. Finally, we confirm these predictions experimentally, showing improvements up to 9.2 percent in the optimized layouts. This demonstrates the algorithm's efficacy in improving the task performance of a layout, and its ability to generalize and improve layouts of new interfaces.",Optimization; data-driven design; gradient descent; deep learning; mobile interfaces; LSTM; performance modeling;,24,5.33,A*,Usability attribute evaluation,Deep learning,User interactions,yes,prototype,mobile
Towards Generating UI Design Feedback with LLMs,"Duan, Peitong; Warner, Jeremy; Hartmann, Bjoern",2023,conferencePaper,,,,Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3586182.3615810,https://doi.org/10.1145/3586182.3615810,"Feedback on user interface (UI) mockups is crucial for the design process, and designers often seek and leverage feedback to improve their UIs. However, human feedback is not always readily available. Given the recent emergence of LLMs, which have been shown to be proficient in rule-based reasoning, we explore the potential of LLMs to provide feedback automatically. In particular, we investigate automating heuristic evaluation, which currently entails a human expert assessing how well a UI adheres to a given set of design guidelines. We build an LLM-based heuristic evaluation plugin for Figma, which designers can use to evaluate their UI mockups. The plugin queries the LLM with the guidelines and a JSON representation of the UI mockup and then renders the identified guideline violations as constructive suggestions for design improvements. Future work is needed to study what types of usability problems can be successfully identified by LLM-driven heuristic evaluation. © 2023 Owner/Author.",User interfaces; Heuristic evaluation; Mockups; Design-process; Product design; Petroleum reservoir evaluation; User interface designs; Human expert; Usability problems; Design improvements; Plug-ins; Computational user interface design; Design feedbacks; Rule-based reasoning; Computational UI Design; Design Feedback,5,3.33,A*,Automated feedback generation,LLM,Source code,no,tool,desktop
When people and algorithms meet: user-reported problems in intelligent everyday applications,"Eiband, Malin; Völkel, Sarah Theres; Buschek, Daniel; Cook, Sophia; Hussmann, Heinrich",2019,conferencePaper,,,96–106,Proceedings of the 24th International Conference on Intelligent User Interfaces,IUI,ACM,10.1145/3301275.3302262,https://doi.org/10.1145/3301275.3302262,"The complex nature of intelligent systems motivates work on supporting users during interaction, for example through explanations. However, there is yet little empirical evidence on specific problems users face in such systems in everyday use. This paper investigates such problems as reported by users: We analysed 35,448 reviews of three apps on the Google Play Store (Facebook, Netflix and Google Maps) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps' algorithmic decision-making. We enriched this data with users' coping and support strategies through a follow-up online survey (N=286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output, not processes. Our work thus contributes empirical evidence to facilitate understanding of users' everyday problems with intelligent systems.",algorithm; explanations; transparency; user control,81,14.73,A,Feedback evaluation,NLP,Text,yes,prototype,device independent
A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Applications,"Eiband, Malin; Völkel, Sarah Theres; Buschek, Daniel; Cook, Sophia; Hussmann, Heinrich",2020,journalArticle,10,4,,ACM Transactions on Interactive Intelligent Systems,,ACM,10.1145/3370927,https://doi.org/10.1145/3370927,"The complex nature of intelligent systems motivates work on supporting users during interaction, for example, through explanations. However, as of yet, there is little empirical evidence in regard to specific problems users face when applying such systems in everyday situations. This article contributes a novel method and analysis to investigate such problems as reported by users: We analysed 45,448 reviews of four apps on the Google Play Store (Facebook, Netflix, Google Maps, and Google Assistant) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps’ algorithmic decision-making. We enriched this data with users’ coping and support strategies through a follow-up online survey (N = 286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output rather than processes.",Algorithm; explanations; transparency; user control,4,0.89,Q3,Feedback evaluation,NLP,Text,yes,prototype,device independent
Automating GUI testing with image-based deep reinforcement learning,"Eskonen, Juha; Kahles, Julen; Reijonen, Joel",2020,conferencePaper,,,160 – 167,"Proceedings - 2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems, ACSOS 2020",ACSOS,IEEE,10.1109/ACSOS49614.2020.00038,https://doi.org/10.1109/ACSOS49614.2020.00038,"Users interact with modern applications and devices through graphical user interfaces (GUIs). To ensure intuitive and easy usability, the GUIs need to be tested, where developers aim at finding possible bugs and inconsistent functionality. Manual GUI testing requires time and effort, and thus, its efficiency can be improved with automation. Conventional automation tools for GUI testing reduce the burden of manual testing but also introduce challenges in the maintenance of test cases. In order to overcome these issues, we propose a deep-reinforcement-learning-based (DRL) solution for automated and adaptive GUI testing. Specifically, we propose and evaluate the performance of an image-based DRL solution. We adapt the asynchronous advantage actor-critic (A3C) algorithm to GUI testing inspired by how a human uses a GUI. We feed screenshots of the GUI as the input and let the algorithm decide how to interact with GUI components. We observe that our solution can achieve up to six times higher exploration efficiency compared to selected baseline algorithms. Moreover, our solution is more efficient than inexperienced human users and almost as efficient as an experienced human user in our experimental GUI testing scenario. For these reasons, image-based DRL exploration can be considered as a viable GUI testing method. © 2020 IEEE.",Actor critic; Adaptive GUI; Automation; Automation tools; Deep learning; Efficiency; Graphical user interface (GUIs); Graphical user interfaces; Human users; Its efficiencies; Manual testing; Modern applications; Reinforcement learning; Testing,39,8.67,-,Automatic guideline evaluation,"Deep learning, Computer vision",Images,no,prototype,desktop
SERENE: a Web platform for the UX semi-automatic evaluation of website,"Esposito, Andrea; Desolda, Giuseppe; Lanzilotti, Rosa; Costabile, Maria Francesca",2022,conferencePaper,,,,Proceedings of the 2022 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3531073.3534464,https://doi.org/10.1145/3531073.3534464,"This demo presents SERENE, a Web platform for the UX semi-automatic evaluation of websites. It exploits Artificial Intelligence to predict visitors' emotions starting from their interaction logs. The predicted emotions are shown by interactive heatmaps overlapped to the webpage to be analyzed. The concentration of negative emotions in a specific area of the webpage can help the UX experts identify UX problems. © 2022 Owner/Author.",Automatic UX evaluation; User eXperience; Automatic evaluation; Users' experiences; Websites; Semi-automatics; Heatmaps; Web-page; Evaluation of websites; Specific areas; UX smell; automatic UX evaluation; UX Smells,5,2,B,Affective state detection,Machine learning,User interactions,yes,tool,desktop
The fine line between automation and augmentation in website usability evaluation,"Esposito, Andrea; Desolda, Giuseppe; Lanzilotti, Rosa",2024,journalArticle,14,1,,Scientific Reports,,Springer Nature,10.1038/s41598-024-59616-0,https://doi.org/10.1038/s41598-024-59616-0,"Artificial Intelligence (AI) systems are becoming widespread in all aspects of society, bringing benefits to the whole economy. There is a growing understanding of the potential benefits and risks of this type of technology. While the benefits are more efficient decision processes and industrial productivity, the risks may include a potential progressive disengagement of human beings in crucial aspects of decision-making. In this respect, a new perspective is emerging that aims at reconsidering the centrality of human beings while reaping the benefits of AI systems to augment rather than replace professional skills: Human-Centred AI (HCAI) is a novel framework that posits that high levels of human control do not contradict high levels of computer automation. In this paper, we investigate the two antipodes, automation vs augmentation, in the context of website usability evaluation. Specifically, we have analyzed whether the level of automation provided by a tool for semi-automatic usability evaluation can support evaluators in identifying usability problems. Three different visualizations, each one corresponding to a different level of automation, ranging from a full-automation approach to an augmentation approach, were compared in an experimental study. We found that a fully automated approach could help evaluators detect a significant number of medium and high-severity usability problems, which are the most critical in a software system; however, it also emerged that it was possible to detect more low-severity usability problems using one of the augmented approaches proposed in this paper. © The Author(s) 2024.",artificial intelligence; Artificial Intelligence; automation; Automation; human; Humans; Internet; software; Software; User-Computer Interface,7,14,Q1,Affective state detection,Deep learning,User interactions,yes,tool,desktop
Evaluation of the accessibility and usability of university websites: a comparative study of the Gulf region,"Fakrudeen, Mohammed",2024,journalArticle,,,,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-024-01160-9,https://doi.org/10.1007/s10209-024-01160-9,"As websites serve as the cornerstone of educational institutions, evaluating their accessibility and usability is crucial. This study examines the accessibility and usability of 198 university homepages in the Gulf region: UAE, Bahrain, Kuwait, Oman, Qatar, and Saudi Arabia. Based on Webometrics ranking, a sample of websites was analyzed using automated online testing tools. The analysis revealed that universities in Bahrain, Oman, and Kuwait placed the highest focus on website accessibility and usability, followed by UAE and Saudi Arabia. While minimal compliance with the WCAG 2.0 standard was observed, no significant usability or accessibility issues were identified compared to other nations, though areas for improvement do exist. Recommendations for developers and administrators are presented, addressing accessibility and usability concerns and ensuring equal access to information for all stakeholders. This report provides valuable insights for enhancing website accessibility and usability in universities. © The Author(s) 2024.",Websites; Accessibility; Bahrain; Comparatives studies; Computer interaction; Gulf countries; Gulf region; Saudi Arabia; Usability; Website accessibility; Website usability; Human computer interaction,0,0,Q3,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,desktop
Concurrent Think-Aloud Verbalizations and Usability Problems,"Fan, Mingming; Lin, Jinglan; Chung, Christina; Truong, Khai N.",2019,journalArticle,26,5,28:1–28:35,ACM Transactions on Computer-Human Interaction,,ACM,10.1145/3325281,https://doi.org/10.1145/3325281,"The concurrent think-aloud protocol—in which participants verbalize their thoughts when performing tasks—is a widely employed approach in usability testing. Despite its value, analyzing think-aloud sessions can be onerous because it often entails assessing all of a user's verbalizations. This has motivated previous research on developing categories to segment verbalizations into manageable units of analysis. However, the way in which a category might relate to usability problems is currently unclear. In this research, we sought to address this gap in our understanding. We also studied how speech features might relate to usability problems. Through two studies, this research demonstrates that certain patterns of verbalizations are more telling of usability problems than others and that these patterns are robust to different types of test products (i.e., physical devices and digital systems), access to different types of information (i.e., video and audio modality), and the presence or absence of a visualization of verbalizations. The implication is that the verbalization and speech patterns can potentially reduce the time and effort required for analysis by enabling evaluators to focus more on the important aspects of a user's verbalizations. The patterns could also potentially be used to inform the design of systems to automatically detect when in the recorded think-aloud sessions users experience problems.",verbalization categories; verbalization; verbal fillers; usability testing; usability problems; speech rate; speech features; silence; sentiment; pitch; loudness; Concurrent think-aloud,76,13.82,Q2,Usability issue encounter detection,Descriptive metrics,"Audio, Video",yes,prototype,desktop
Automatic Detection of Usability Problem Encounters in Think-aloud Sessions,"Fan, Mingming; Li, Yue; Truong, Khai N.",2020,journalArticle,10,2,,ACM Transactions on Interactive Intelligent Systems,,ACM,10.1145/3385732,https://doi.org/10.1145/3385732,"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users' verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users' verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners' workflow. © 2020 ACM.",machine learning; Testing; AI-assisted UX analysis method; speech features; Think aloud; usability problem; user experience (UX); verbalization; User interfaces; Feature extraction; Usability engineering; Speech recognition; User interface designs; Usability problems; Automatic Detection; Labor intensive; Think-aloud protocol; Recent researches; Speech features; Usability testing methods,27,6,Q3,Usability issue encounter detection,"NLP, Deep learning, Machine learning",Audio,yes,prototype,"desktop, remote control, coffee machine"
VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions,"Fan, Mingming; Wu, Ke; Zhao, Jian; Li, Yue; Wei, Winter; Truong, Khai N.",2020,journalArticle,26,1,343 – 352,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2019.2934797,https://doi.org/10.1109/TVCG.2019.2934797,"Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind). © 2020 IEEE.",machine learning; Usability; Machine learning; Think aloud; User interfaces; Visualization; prediction; Feature extraction; user study; Usability engineering; Tools; Artificial intelligence; artificial intelligence; adult; female; human; article; human experiment; male; User study; videorecording; Usability problems; Visual analytics; Machine intelligence; usability problems; visual analytics; machine intelligence; session review behavior; Think-aloud; UX practices; anticipation,32,7.11,Q1,Usability issue encounter detection,"NLP, Deep learning, Machine learning",Audio,yes,tool,"desktop, remote control, coffee machine"
Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization,"Fan, Mingming; Yang, Xianyou; Yu, TszTung; Liao, Q. Vera; Zhao, Jian",2022,journalArticle,6,CSCW1,,Proceedings of the ACM on Human-Computer Interaction,,ACM,10.1145/3512943,https://doi.org/10.1145/3512943,"Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to the two ways humans and AI collaborate: synchronously and asynchronously. We iteratively designed a tool-AI Assistant-with four versions of UIs corresponding to the two levels of explanations (with/without) and synchronization (sync/async). By adopting a hybrid wizard-of-oz approach to simulating an AI with reasonable performance, we conducted a mixed-method study with 24 UX evaluators identifying UX problems from usability test videos using AI Assistant. Our quantitative and qualitative results show that AI with explanations, regardless of being presented synchronously or asynchronously, provided better support for UX evaluators' analysis and was perceived more positively; when without explanations, synchronous AI better improved UX evaluators' performance and engagement compared to the asynchronous AI. Lastly, we present the design implications for AI-assisted UX evaluation and facilitating more effective human-AI collaboration.",Think aloud; user experience (UX); User interfaces; Usability engineering; Explainable AI; Synchronization; Collaboration; explanation; explainable AI; Iterative methods; Users' experiences; User interface designs; Usability tests; Explanation; User experience (UX); AI-assisted UX evaluation; Intelligent user interface design; Intelligent User Interfaces; Think-aloud usability test; collaboration; intelligent user interface (UI) design; synchronization; think-aloud usability test,52,20.8,Q2,Research assistants,Machine learning,"Audio, User interactions",yes,tool,desktop
"Affective-Ready, Contextual and Automated Usability Test for Mobile Software","Feijo Filho, Jackson; Prata, Wilson; Oliveira, Juan",2016,conferencePaper,,,638-644,Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct,MOBILEHCI,ACM,10.1145/2957265.2961834,https://doi.org/10.1145/2957265.2961834,"This work proposes the use of system to perform affective-ready, contextual and automated usability tests for mobile software. Our proposal augments the traditional methods of software usability evaluation by monitoring users' location, weather conditions, moving/stationary status, data connection availability and spontaneous facial expressions automatically. This aims to identify the moment of negative and positive events. Identifying those situations and systematically associating them to the context of interaction, assisted software creators to overcome design flaws and enhancing interfaces' strengths. The validation of our approach include post-test questionnaires with test subjects. The results indicate that the automated user-context logging can be a substantial supplement to mobile software usability tests.",Usability; Testing; Mobile Software; affective computing; usability; Usability engineering; Automation; Software testing; Surveys; Human computer interaction; Mobile devices; Computer software; Usability tests; Mobile softwares; Affective Computing; Facial Expressions; Data connection; Software usabilities; User context; mobile software,6,0.71,B,Affective state detection,"Machine learning, Computer vision","User movements, Video",yes,prototype,mobile
Extending Mobile App Analytics for Usability Test Logging,"Ferre, Xavier; Villalba, Elena; Julio, Hector; Zhu, Hongming",2017,conferencePaper,10515,III,114-131,Human-Computer Interaction–INTERACT 2017: 16th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-319-67687-6_9,https://doi.org/10.1007/978-3-319-67687-6_9,"Mobile application development is characterized by reduced development cycles and high time-to-market pressure. Usability evaluation in mobile applications calls for the application of cost-effective methods, specially adapted to such constraints. We propose extending the Google Analytics for Mobile Applications basic service to store specific low-level user actions of interest for usability evaluation purposes. The solution can serve both for lab usability testing, automating quantitative data gathering, and for logging real use after application release. It is based on identification of relevant user tasks and the detailed events worth gathering, instrumentation of specific code for data gathering, and subsequent data extraction for calculating relevant usability- related variables. We validated our application in a real usability test by comparing the automatically gathered data with the information gathered by the human observer. Results shows both measurements are statistically exchangeable, opening promising new ways to perform usability testing cost-effectively and at greater scale.",Usability engineering; Mobile applications; Human computer interaction; Mobile computing; Usability testing; Automated usability evaluation; Usability evaluation; Cost effectiveness; Mobile application development; Mobile telecommunication systems; Cost-effective methods; Log-file analysis; Related variables; Log file analysis; Usability evaluation of mobile applications,42,5.6,A,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,mobile
Automated Usability Tests for Mobile Devices through Live Emotions Logging,"Filho, Jackson Feijó; Valle, Thiago; Prata, Wilson",2015,conferencePaper,,,636–643,Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct,MOBILEHCI,ACM,10.1145/2786567.2792902,https://doi.org/10.1145/2786567.2792902,"Usability tests for mobile phones software present several consistent obstacles. The issue of observing users' interaction within a controlled and supervised environment versus unsupervised field tests is a significant one. Additionally, the burden of frequently time-consuming and expensive usability tests for small software development teams are evident. This work proposes the use of a system to implement emotions logging in automated usability tests for mobile phones. We do this by efficiently, easily and cost-effectively assessing the users' affective state by evaluating their expressive reactions during a mobile software usability evaluation process. These reactions are collected using the front camera on mobile devices.",Usability; Testing; Emotion Feedback; Mobile Software; Usability engineering; Software testing; Human computer interaction; Mobile devices; Computer software; Software design; Usability tests; Field test; Mobile softwares; Cellular telephone systems; Cellular telephones; Mobile phones; Software development teams; Telephone sets; Users' affective state,19,2,B,Affective state detection,"Machine learning, Computer vision","User interactions, Video",yes,prototype,mobile
Usability improvement through A/B testing and refactoring,"Firmenich, Sergio; Garrido, Alejandra; Grigera, Julián; Rivero, José Matías; Rossi, Gustavo",2019,journalArticle,27,1,203-240,Software Quality Journal,,Springer Nature,10.1007/s11219-018-9413-y,https://doi.org/10.1007/s11219-018-9413-y,"Usability evaluation is an essential task in web application development. There have been several attempts to integrate user-centered design with agile methods, but it is hard to synchronize their practices. User testing is very valuable to learn from feedback of actual use, but it remains expensive to find and solve usability problems. Furthermore, the high cost of usability evaluation forces small/medium-sized companies to trust the first solution applied, without actually testing the success of the solution or considering a possible regression in usability, as could be highlighted by an iterative testing method. In this article, we advocate for a usability improvement cycle oriented by user feedback, and compatible with an agile development process. We propose an iterative method supported by a toolkit that allows usability experts to design user tests, run them remotely, analyze results, and assess alternative solutions to usability problems similarly to A/B testing. Each solution is created by applying client-side web refactorings, i.e., changes to the web pages in the client which are meant to improve usability. The main benefit of our approach is that it reduces the overall cost of user testing and particularly, A/B testing, by applying refactorings to create alternative solutions without modifying the application’s server code. By making it affordable for usability experts to apply the method in parallel with the development cycle, we aim to encourage them to incorporate user feedback and try different ideas to discover the best-performing solution in terms of the metrics of interest.",A/B testing; Usability evaluation; Agile methods; External quality; Web refactoring,20,3.64,Q3,Usability issue encounter detection,Descriptive metrics,User interactions,yes,prototype,desktop
Framework for Electroencephalography-based Evaluation of User Experience,"Frey, Jérémy; Daniel, Maxime; Castet, Julien; Hachet, Martin; Lotte, Fabien",2016,conferencePaper,,,,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858525,https://doi.org/10.1145/2858036.2858525,"Measuring brain activity with electroencephalography (EEG) is mature enough to assess mental states. Combined with existing methods, such tool can be used to strengthen the understanding of user experience. We contribute a set of methods to estimate continuously the user's mental workload, attention and recognition of interaction errors during different interaction tasks. We validate these measures on a controlled virtual environment and show how they can be used to compare different interaction techniques or devices, by comparing here a keyboard and a touch-based interface. Thanks to such a framework, EEG becomes a promising method to improve the overall usability of complex computer systems.",EEG; HCI Evaluation; Workload; Attention; Interaction errors; Neuroergonomy,90,10.59,A*,Usability attribute evaluation,Machine learning,"User interactions, Physiological signals",yes,prototype,desktop
Software Usability Testing Using EEG-Based Emotion Detection and Deep Learning,"Gannouni, Sofien; Belwafi, Kais; Aledaily, Arwa; Aboalsamh, Hatim; Belghith, Abdelfettah",2023,journalArticle,23,11,5147,Sensors,,MDPI,10.3390/s23115147,https://doi.org/10.3390/s23115147,"It is becoming increasingly attractive to detect human emotions using electroencephalography (EEG) brain signals. EEG is a reliable and cost-effective technology used to measure brain activities. This paper proposes an original framework for usability testing based on emotion detection using EEG signals, which can significantly affect software production and user satisfaction. This approach can provide an in-depth understanding of user satisfaction accurately and precisely, making it a valuable tool in software development. The proposed framework includes a recurrent neural network algorithm as a classifier, a feature extraction algorithm based on event-related desynchronization and event-related synchronization analysis, and a new method for selecting EEG sources adaptively for emotion recognition. The framework results are promising, achieving 92.13%, 92.67%, and 92.24% for the valence–arousal–dominance dimensions, respectively.",Brain-Computer Interface; channel selection; deep-learning; EEG signal processing; emotion detection; recurrent neural network; usability testing,4,2.67,Q2,Affective state detection,Deep learning,Physiological signals,yes,prototype,desktop
Predicting interaction effort in web interface widgets,"Gardey, Juan Cruz; Grigera, Julián; Rodríguez, Andrés; Rossi, Gustavo; Garrido, Alejandra",2022,journalArticle,168,,102919,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2022.102919,https://doi.org/10.1016/j.ijhcs.2022.102919,"The product of good design should render a tool invisible for a user who is executing a task. Unfortunately, web applications are often far from invisible to users, who struggle with poor design of websites and processes in them. We are particularly interested in web processes that involve form filling, so we have been studying how people interact with web forms. Besides cataloguing user interaction problems that are common in web forms, we have noticed that, in many cases, there is a single form element or widget to blame for a certain interaction problem, because such widget is not the most appropriate one for the required input in that particular context. This unfitness of the widget causes an extra burden to the user, which we call interaction effort. In this work we propose measuring the interaction effort of a widget with a unified score based on micro-measures automatically captured from interaction logs. We present the micro-measures that were found relevant to predict the interaction effort in 6 different types of web forms widgets. We describe a large data collection process and prediction models, showing that it is indeed possible to automatically predict a widget interaction effort score by learning from expert human ratings. We consequently believe that the interaction effort could be used as an effective metric to compare small variations in a design in terms of user experience.",Interactivity; User interaction metrics; User experience; Web usability; UX refactoring,12,4.8,Q1,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,desktop
UX-Analyzer: Visualizing the interaction effort for web analytics,"Gardey, Juan Cruz and Grigera, Julian and Rodriguez, Andres and Garrido, Alejandra",2024,conferencePaper,,,1774 – 1780,Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing,SAC,ACM,10.1145/3605098.3636013,https://doi.org/10.1145/3605098.3636013,"Evaluation of User Experience (UX) in online systems is essential but also costly in terms of both time and money. We are interested in building automatic tools to assist development teams in evaluating the UX even with a limited budget and short iteration cycles.Analyzing user interaction may provide a good indication of the effort that users are required to spend on certain page elements, which we consider an important factor of UX. In this paper we present UX-Analyzer, a web tool that automatically calculates and displays the user interaction effort of web pages. The interaction effort is a score that represents the extra burden, difficulty or discomfort of a user when interacting with a specific webpage element. This score has been learned from manual UX expert ratings and different measures of interaction logs, called micro-measures. UX-Analyzer further aggregates individual element scores from different users to compose a single score for each analyzed web page. The tool can also show the compared scores among alternative versions of a page.Thus, UX-Analyzer allows UX experts or other team members to evaluate web pages automatically in a transparent way, since the interaction effort may be calculated with multiple users in their real context of use. Visualizing the interaction effort may provide a good indication towards the level of UX of an online system. It may also be used in the context of an A/B testing approach that instead of revenue or conversions, it compares the UX of alternative versions of a website. That is, this approach enables web analytics based on UX, i.e., measuring, analyzing and improving UX quality in each development cycle. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Budget control; Online systems; User interfaces; Websites; A/B testing; Evaluation of users; In-buildings; Machine-learning; Refactorings; User interaction; Users' experiences; UX refactoring; Web analytics; Web-page; Machine learning,4,8,B,Usability attribute evaluation,Machine learning,User interactions,yes,tool,desktop
UX Heatmaps: Mapping User Experience on Visual Interfaces,"Georges, Vanessa; Courtemanche, François; Senecal, Sylvain; Baccino, Thierry; Fredette, Marc; Leger, Pierre-Majorique",2016,conferencePaper,,,4850–4860,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858271,https://doi.org/10.1145/2858036.2858271,"In this paper, we present an off-the-shelf UX evaluation tool which contextualizes users' physiological and behavioral signals while interacting with a system. The proposed tool triangulates users' gaze data with inferred users' cognitive and emotional states to produce user experience (UX) heatmaps, which show where users were looking when they experienced specific cognitive and emotional states. Results show that for a given cognitive state (i.e., cognitive load), the proposed UX heatmap was able to effectively highlight the areas where users experienced different levels of cognitive load on an interface. The proposed tool enables the visual analysis of users' various emotional and cognitive states for specific areas on a given interface, and also to compare users' states across multiple interfaces, which should be useful for both UX researchers and practitioners.",User experience; interface design; heatmaps; eye tracking; physiological computing; cognitive load; affective computing,38,4.47,A*,Affective state detection,"Machine learning, Computer vision","Video, Gaze data, Physiological signals",yes,tool,desktop
Guidelines for Collecting Automatic Facial Expression Detection Data Synchronized with a Dynamic Stimulus in Remote Moderated User Tests,"Giroux, Felix; Leger, Pierre-Majorique; Brieugne, David; Courtemanche, Francois; Bouvier, Frederique; Chen, Shang-Lin; Tazi, Salima; Rucco, Emma; Fredette, Marc; Coursaris, Constantinos; Senecal, Sylvain",2021,conferencePaper,12762,,243-254,"Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021",HCII,Springer Nature,10.1007/978-3-030-78462-1_18,https://doi.org/10.1007/978-3-030-78462-1_18,"Because of the COVID-19 pandemic, telework policies have required many user experience (UX) labs to restrict their research activities to remote user testing. Automatic Facial Expression Analysis (AFEA) is an accessible psychophysiological measurement that can be easily implemented in remote user tests. However, to date, the literature on Human Computer Interaction (HCI) has provided no guidelines for remote moderated user tests that collect facial expression data and synchronize them with the state of a dynamic stimulus such as a webpage. To address this research gap, this article offers guidelines for effective AFEA data collection that are based on a methodology developed in a concrete research context and on the lessons learned from applying it in four remote moderated user testing projects. Since researchers have less control over test environment settings, we maintain that they should pay greater attention to factors that can affect face detection and \textbackslashor emotion classification prior, during, and after remote moderated user tests. Our study contributes to the development of methods for including psychophysiological and neurophysiological measurements in remote user tests that offer promising opportunities for information systems (IS) research, UX design, and even digital health research.",Human-computer interaction; User experience; Human computer interaction (HCI); Data acquisition; Human computer interaction; Face recognition; User experiences (ux); Research activities; Automatic facial expression analysis; NeurolS; Psychophysiological data; Remote user test; Concrete research; Emotion classification; Facial expression data; Facial expression detections,16,4.57,B,Affective state detection,"Deep learning, Computer vision",Video,yes,prototype,"desktop, mobile"
Supporting adaptation of web applications to the mobile environment with automated usability evaluation,"Gonçalves, Luiz F.; Vasconcelos, Leandro G.; Munson, Ethan V.; Baldochi, Laércio A.",2016,conferencePaper,,,787–794,Proceedings of the ACM Symposium on Applied Computing,SAC,ACM,10.1145/2851613.2851863,https://doi.org/10.1145/2851613.2851863,"The year 2014 marked an important shift in theWeb's history, as users started to spend more time surfing the web using mobile devices than using desktop computers. However, today, a large proportion of websites are still not designed for good mobile device access. To tackle this problem, this paper proposes a new approach to adapting desktop-based web applications to the mobile environment. Our approach is supported by a task-based usability evaluation tool called MOBILICS, which is able to evaluate desktop-based web applications used in mobile devices. Based on the evaluation results, our tool provides detailed recommendations for fixing the detected usability problems. We conducted an experiment which demonstrates that our approach provides useful support for the adaptation of a desktop-based web application into a mobile version.",Usability engineering; usability evaluation; Automated usability evaluation; Usability evaluation; Mobile devices; WEB application; Websites; Evaluation results; Usability problems; Personal computers; Desktop-to-mobile adaptation; Mobile environments; Touch interaction; desktop-to-mobile adaptation; touch interaction,8,0.94,B,Usability issue encounter detection,Rule-based systems,User interactions,yes,tool,"desktop, mobile"
A tool for detecting bad usability smells in an automatic way,"Grigera, Julián; Garrido, Alejandra; Rivero, José Matías",2014,conferencePaper,8541,,490 – 493,Web Engineering: 14th International Conferenc,ICWE,Springer Nature,10.1007/978-3-319-08245-5_34,https://doi.org/10.1007/978-3-319-08245-5_34,"The refactoring technique helps developers to improve not only source code quality, but also other aspects like usability. The problems refactoring helps to solve in the specific field of web usability are considered to be issues that make common tasks complicated for end users. Finding such problems, known in the jargon as bad smells, is often challenging for developers, especially for those who do not have experience in usability. In an attempt to leverage this task, we introduce a tool that automatically finds bad usability smells in web applications. Since bad smells are catalogued in the literature together with their suggested refactorings, it is easier for developers to find appropriate solutions. © Springer International Publishing Switzerland 2014.",Bad smells; End users; Odors; Refactorings; Social networking (online); Source code qualities; Usability engineering; WEB application; Web usability,30,2.86,B,Usability issue encounter detection,Rule-based systems,"Source code, User interactions",yes,tool,desktop
Kobold: Web usability as a service,"Grigera, Julian; Garrido, Alejandra; Rossi, Gustavo",2017,conferencePaper,,,990 – 995,2017 32nd IEEE/ACM International Conference on Automated Software Engineering,ASE,IEEE,10.1109/ASE.2017.8115717,https://doi.org/10.1109/ASE.2017.8115717,"While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q. © 2017 IEEE.",Usability; Usability engineering; Software engineering; Tools; Automation; Servers; Business; Usability testing; Usability evaluation; Concrete; Web Design; Concretes; HTTP; Websites; Usability problems; User interaction; Web usability; Information exchanges; Refactorings; Software as a service (SaaS); Concrete testing; Social interactions; Software as a service; Software as a Service; Usability Refactoring; Web Usability,23,3.07,A,Usability issue encounter detection,Rule-based systems,"Source code, User interactions",yes,tool,"desktop, mobile"
Automatic detection of usability smells in web applications,"Grigera, Julián; Garrido, Alejandra; Rivero, José Matías; Rossi, Gustavo",2017,journalArticle,97,,129 – 148,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2016.09.009,https://doi.org/10.1016/j.ijhcs.2016.09.009,"Usability assessment of web applications continues to be an expensive and often neglected practice. While large companies are able to spare resources for studying and improving usability in their products, smaller businesses often divert theirs in other aspects. To help these cases, researches have devised automatic approaches for user interaction analysis, and there are commercial services that offer automated usability statistics at relatively low fees. However, most existing approaches still fall short in specifying the usability problems concretely enough to identify and suggest solutions. In this work we describe usability smells of user interaction, i.e., hints of usability problems on running web applications, and the process in which they can be identified by analyzing user interaction events. We also describe USF, the tool that implements the process in a fully automated way with minimum setup effort. USF analyses user interaction events on-the-fly, discovers usability smells and reports them together with a concrete solution in terms of a usability refactoring, providing usability advice for deployed web applications. © 2016 Elsevier Ltd",Usability engineering; Usability testing; Concretes; Usability assessment; Automatic Detection; Refactorings; Automatic approaches; Web based; Odors; World Wide Web; Refactoring; Log analysis; Web-based interaction; Commercial services,113,15.07,Q2,Usability issue encounter detection,Rule-based systems,User interactions,yes,tool,desktop
Predicting webpage aesthetics with heatmap entropy,"Gu, Zhenyu; Jin, Chenhao; Chang, Danny; Zhang, Liqun",2021,journalArticle,40,7,,Behaviour & Information Technology,,Taylor & Francis,10.1080/0144929X.2020.1717626,https://doi.org/10.1080/0144929X.2020.1717626,"This paper introduces a descriptive global index for eye-tracking data called heatmap entropy, or visual attention entropy (VAE), and discerns its predictive value for webpage aesthetics. VAE represents the chaos, or uncertainty, in the allocation of visual attention. In the experiment, we tracked and recorded 30 observers' initial landings on 40 web pages displayed for 3 seconds each. The results show that the VAE and aesthetic ratings of the web pages are negatively correlated (r=−0.54, P<0.001). A calibrated form of VAE, known as relative VAE (rVAE), has a more significant correlation with the aesthetic ratings (r=−0.65, P<0.00001). On its own, the rVAE can differentiate between good- and bad-looking pages to a certain degree of accuracy (two-class ANOVA with F=26.84, P<0.00001). Further investigation reveals that the performances of both VAE and rVAE improve steadily after the first second, and could be better, if the tracking duration was longer than 3 seconds or if more observers were recruited.",eye tracking; visual attention; Entropy; aesthetics; web page,32,9.14,Q2,Aesthetics evaluation,Descriptive metrics,Gaze data,yes,prototype,desktop
AI-augmented usability evaluation framework for software requirements specification in cyber physical human systems,"Gupta, Sandeep; Epiphaniou, Gregory; Maple, Carsten",2023,journalArticle,23,,,Internet of Things,,Elsevier,10.1016/j.iot.2023.100841,https://doi.org/10.1016/j.iot.2023.100841,"A number of research in Information and Communication Technology (ICT) have shown that usability is an important goal for cyber-physical human systems for wider acceptance by their end-users. To evaluate the usability of a system under design, usability evaluation methods predominantly rely on subject matter experts or testers' assessments. However, integrating Artificial Intelligence (AI) technology and existing usability evaluation methods' processes can aid to evolve more effective user-centred designs. In this paper, we conceptualize an AI-augmented usability evaluation framework (AIUEF) that aims at replacing ‘end-users’ with ‘personas’ to evaluate requirements involving human-computer interaction (HCI) for a given Software Requirements Specification (SRS). We present a blueprint of AIUEF establishing a theoretical basis for an automated usability evaluation of HCI requirements for a given SRS with an aim to improve the usability of a system under design. © 2023 Elsevier B.V.",Artificial intelligence; Cyber physical human systems; Human-computer interaction; Software requirements specification; Usability evaluation,6,4,Q1,Usability attribute evaluation,"NLP, Deep learning",none,no (synthetic),concept,device independent
How Do Users Like This Feature? A Fine Grained Sentiment Analysis of App Reviews,"Guzman, Emitza; Maalej, Walid",2014,conferencePaper,,,153-162,2014 IEEE 22nd International Requirements Engineering Conference (RE),RE,IEEE,10.1109/RE.2014.6912257,https://doi.org/10.1109/RE.2014.6912257,"App stores allow users to submit feedback for downloaded apps in form of star ratings and text reviews. Recent studies analyzed this feedback and found that it includes information useful for app developers, such as user requirements, ideas for improvements, user sentiments about specific features, and descriptions of experiences with these features. However, for many apps, the amount of reviews is too large to be processed manually and their quality varies largely. The star ratings are given to the whole app and developers do not have a mean to analyze the feedback for the single features. In this paper we propose an automated approach that helps developers filter, aggregate, and analyze user reviews. We use natural language processing techniques to identify fine-grained app features in the reviews. We then extract the user sentiments about the identified features and give them a general score across all reviews. Finally, we use topic modeling techniques to group fine-grained features into more meaningful high-level features. We evaluated our approach with 7 apps from the Apple App Store and Google Play Store and compared its results with a manually, peer-conducted analysis of the reviews. On average, our approach has a precision of 0.59 and a recall of 0.51. The extracted features were coherent and relevant to requirements evolution tasks. Our approach can help app developers to systematically analyze user opinions about single features and filter irrelevant reviews.",Manuals; Feature extraction; Encoding; Google; Sentiment analysis; Educational institutions; Dictionaries,812,77.33,A,Feedback evaluation,"Machine learning, NLP",Text,yes,prototype,device independent
Neural Language Models as What If? -Engines for HCI Research,"Hämäläinen, Perttu; Tavast, Mikke; Kunnari, Anton",2022,conferencePaper,,,77 – 80,"International Conference on Intelligent User Interfaces, Proceedings IUI",IUI,ACM,10.1145/3490100.3516458,https://doi.org/10.1145/3490100.3516458,"Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this poster paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions. We observe that in the best case, GPT-3 can create plausible reflections of video game experiences and emotions, and adapt its responses to given demographic information. Compared to real participants, such synthetic data can be obtained faster and at a lower cost. On the other hand, the quality of generated data has high variance, and future work is needed to rigorously quantify the human-likeness, limitations, and biases of the models in the HCI domain. © 2022 Owner/Author.",Computational linguistics; Computer users; GPT-3; Human computer interaction; Human-computer interaction researches; Interaction experiences; Language model; Large-scales; Research data; User interfaces; User Modelling; Users' experiences; Video-games,8,3.2,A,Automated feedback generation,LLM,none,no (synthetic),concept,device independent
Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study,"Hämäläinen, Perttu; Tavast, Mikke; Kunnari, Anton",2023,conferencePaper,,,1–19,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3544548.3580688,https://doi.org/10.1145/3544548.3580688,"Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.",GPT-3; Language models; User experience; User models,197,131.33,A*,Automated feedback generation,LLM,none,no,prototype,device independent
Automated Usability Evaluation of Virtual Reality Applications,"Harms, Patrick",2019,journalArticle,26,3,,ACM Transactions on Computer-Human Interaction,,ACM,10.1145/3301423,https://doi.org/10.1145/3301423,"Virtual reality (VR) and VR applications have reached the end-user and, hence, the demands on usability, also for novel applications, have increased. This situation requires VR usability evaluation methods that can be applied quickly, even after a first release of an application. In this article, we describe such an approach, which is fully automated and does not ask users to perform predefined tasks in a fixed test setting. Instead, it works on recordings of the actual usage of a VR application from which it generates task trees. Afterwards, it analyzes these task trees to search for usability smells, i.e., user behavior indicating usability issues. Our approach provides detailed descriptions of the usability issues that have been found and how they can be solved. We performed a large case study to evaluate our approach and show that it is capable of correctly identifying usability issues. Although our approach is applicable for different VR interaction modalities, such as gaze, controller, or hand interaction, it also has limitations. For example, it can detect diverse issues related to user efficiency, but specific misunderstandings of users cannot be uncovered.",Usability engineering; Automation; Virtual reality; Forestry; virtual reality; Automated usability evaluation; task tree generation; usability smell detection; Behavioral research; Novel applications; User behaviors; Fully automated; Task tree; Usability evaluation methods; User efficiencies; VR applications,49,8.91,Q2,Usability issue encounter detection,Log-based sequence & pattern mining,User interactions,yes,prototype,VR
Exploring user satisfaction for e-learning systems via usage-based metrics and system usability scale analysis,"Harrati, Nouzha; Bouchrika, Imed; Tari, Abdelkamel; Ladjailia, Ammar",2016,journalArticle,61,,463-471,Computers in Human Behavior,,Elsevier,10.1016/j.chb.2016.03.051,https://doi.org/10.1016/j.chb.2016.03.051,"The use of e-learning technology is incontestably recognized as an important and integral part of the educational process. Considerable research studies are carried out in order to apprehend how effective and usable e-learning systems. In this paper, an empirical-based study is conducted to explore how lecturers interact with an e-learning environment based on a predefined task model describing low-level interactions. Client-side log data is collected from university lecturers from the Electrical and Computer Science departments. Subsequently, data analysis is conducted to infer the usability degree from the estimated usage metrics together with further exploratory analysis from user feedback via System Usability Scale. Experimental results reveal that the System Usability Scale score is not a sufficient measure to express the true acceptance and satisfaction level of lecturers for using the e-learning systems. The evaluation must be fulfilled in tandem with analyzing the usage metrics derived from interaction traces in a non-intrusive fashion. The proposed approach is a milestone towards usability evaluation to improve the acceptance and user experience for academic staff and students.",Usability; Usability evaluation; Moodle; e-Learning,326,38.35,Q1,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,desktop
AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces,"Hasan Mansur, S M; Salma, Sabiha; Awofisayo, Damilola; Moran, Kevin",2023,conferencePaper,,,1958-1970,2023 IEEE/ACM 45th International Conference on Software Engineering,ICSE,IEEE,10.1109/ICSE48619.2023.00166,https://doi.org/10.1109/ICSE48619.2023.00166,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",Dark Pattern; UI Analysis; UI Design,16,10.67,A*,Automatic guideline evaluation,"NLP, Deep learning, Computer vision","Images, Source code",no,tool,"desktop, mobile"
Towards estimating affective states in Virtual Reality based on behavioral data,"Holzwarth, Valentin; Schneider, Johannes; Handali, Joshua; Gisler, Joy; Hirt, Christian; Kunz, Andreas; vom Brocke, Jan",2021,journalArticle,25,4,1139–1152,Virtual Reality,,Springer Nature,10.1007/s10055-021-00518-1,https://doi.org/10.1007/s10055-021-00518-1,"Inferring users’ perceptions of Virtual Environments (VEs) is essential for Virtual Reality (VR) research. Traditionally, this is achieved through assessing users’ affective states before and after being exposed to a VE, based on standardized, self-assessment questionnaires. The main disadvantage of questionnaires is their sequential administration, i.e., a user’s affective state is measured asynchronously to its generation within the VE. A synchronous measurement of users’ affective states would be highly favorable, e.g., in the context of adaptive systems. Drawing from nonverbal behavior research, we argue that behavioral measures could be a powerful approach to assess users’ affective states in VR. In this paper, we contribute by providing methods and measures evaluated in a user study involving 42 participants to assess a users’ affective states by measuring head movements during VR exposure. We show that head yaw significantly correlates with presence, mental and physical demand, perceived performance, and system usability. We also exploit the identified relationships for two practical tasks that are based on head yaw: (1) predicting a user’s affective state, and (2) detecting manipulated questionnaire answers, i.e., answers that are possibly non-truthful. We found that affective states can be predicted significantly better than a naive estimate for mental demand, physical demand, perceived performance, and usability. Further, manipulated or non-truthful answers can also be estimated significantly better than by a naive approach. These findings mark an initial step in the development of novel methods to assess user perception of VEs.",Artificial Intelligence,27,7.71,Q2,Usability attribute evaluation,Machine learning,User movements,yes,prototype,VR
Applying Large Language Model to User Experience Testing,"Hsueh, Nien-Lin; Lin, Hsuen-Jen; Lai, Lien-Chi",2024,journalArticle,13,23,4633,Electronics,,MDPI,10.3390/electronics13234633,https://doi.org/10.3390/electronics13234633,"The maturation of internet usage environments has elevated User Experience (UX) to a critical factor in system success. However, traditional manual UX testing methods are hampered by subjectivity and lack of standardization, resulting in time-consuming and costly processes. This study explores the potential of Large Language Models (LLMs) to address these challenges by developing an automated UX testing tool. Our innovative approach integrates the Rapi web recording tool to capture user interaction data with the analytical capabilities of LLMs, utilizing Nielsen’s usability heuristics as evaluation criteria. This methodology aims to significantly reduce the initial costs associated with UX testing while maintaining assessment quality. To validate the tool’s efficacy, we conducted a case study featuring a tennis-themed course reservation system. The system incorporated multiple scenarios per page, allowing users to perform tasks based on predefined goals. We employed our automated UX testing tool to evaluate screenshots and interaction logs from user sessions. Concurrently, we invited participants to test the system and complete UX questionnaires based on their experiences. Comparative analysis revealed that varying prompts in the automated UX testing tool yielded different outcomes, particularly in detecting interface elements. Notably, our tool demonstrated superior capability in identifying issues aligned with Nielsen’s usability principles compared to participant evaluations. This research contributes to the field of UX evaluation by leveraging advanced language models and established usability heuristics. Our findings suggest that LLM-based automated UX testing tools can offer more consistent and comprehensive assessments.",AI in software development; human–computer interaction (HCI),0,0,Q2,Automatic guideline evaluation,"LLM, Computer vision","Images, User interactions",yes,prototype,desktop
A Multimodal Deep Log-Based User Experience (UX) Platform for UX Evaluation,"Hussain, Jamil; Khan, Wajahat Ali; Hur, Taeho; Bilal, Hafiz Syed Muhammad; Bang, Jaehun; Hassan, Anees Ul; Afzal, Muhammad; Lee, Sungyoung",2018,journalArticle,18,5,1622,Sensors,,MDPI,10.3390/s18051622,https://doi.org/10.3390/s18051622,"The user experience (UX) is an emerging field in user research and design, and the development of UX evaluation methods presents a challenge for both researchers and practitioners. Different UX evaluation methods have been developed to extract accurate UX data. Among UX evaluation methods, the mixed-method approach of triangulation has gained importance. It provides more accurate and precise information about the user while interacting with the product. However, this approach requires skilled UX researchers and developers to integrate multiple devices, synchronize them, analyze the data, and ultimately produce an informed decision. In this paper, a method and system for measuring the overall UX over time using a triangulation method are proposed. The proposed platform incorporates observational and physiological measurements in addition to traditional ones. The platform reduces the subjective bias and validates the user’s perceptions, which are measured by different sensors through objectification of the subjective nature of the user in the UX assessment. The platform additionally offers plug-and-play support for different devices and powerful analytics for obtaining insight on the UX in terms of multiple participants.",EEG; eye-tracking; facial expression; galvanic skin response; interaction tracker; mix-method approach; self-reporting; user experience evaluation; user experience measurement; user experience platform,71,10.92,Q2,Affective state detection,"Machine learning, Computer vision","User interactions, Gaze data, Video, Audio, Physiological signals",yes,tool,desktop
"Web site accessibility, usability and security: a survey of government web sites in Kyrgyz Republic","Ismailova, Rita",2017,journalArticle,16,1,257-264,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-015-0446-8,https://doi.org/10.1007/s10209-015-0446-8,"The information in government web sites, which are widely adopted in many countries, must be accessible for all people, easy to use, accurate and secure. The main objective of this study is to investigate the usability, accessibility and security aspects of e-government web sites in Kyrgyz Republic. The analysis of web government pages covered 55 sites listed in the State Information Resources of the Kyrgyz Republic and five government web sites which were not included in the list. Analysis was conducted using several automatic evaluation tools. Results suggested that government web sites in Kyrgyz Republic have a usability error rate of 46.3 % and accessibility error rate of 69.38 %. The study also revealed security vulnerabilities in these web sites. Although the “Concept of Creation and Development of Information Network of the Kyrgyz Republic” was launched at September 23, 1994, government web sites in the Kyrgyz Republic have not been reviewed and still need great efforts to improve accessibility, usability and security.",Accessibility; e-Government; Kyrgyz Republic; Security; Usability,114,15.2,Q3,Usability attribute evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Universities of the Kyrgyz Republic on the Web: accessibility and usability,"Ismailova, Rita; Kimsanova, Gulida",2017,journalArticle,16,4,1017-1025,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-016-0481-0,https://doi.org/10.1007/s10209-016-0481-0,"Today the Internet is the easiest way to find information about any kind of organization, and the first impression about an organization is almost always based on its Web site. This study investigated whether the Web sites of the universities in the Kyrgyz Republic comply with prevailing standards of accessibility and usability and whether these qualities depend on location and type of ownership of the universities. The analysis was conducted using online evaluation tools. Based on the data collected, the hypotheses were further tested using the SPSS statistical package. The results show a low usability rating for the vast majority of the universities’ Web sites. For 90.47 % of the Web sites upload time exceeds 30 s; 52.38 % of the Web sites have broken links; and 100 % have browser compatibility problems. The results of accessibility tests show low compliance with W3C-WCAG 1.0: error rates for Priority 1, 2, and 3 checkpoints of 83.33, 92.85, and 95.24 %, respectively. The results obtained and the results of an independent t test indicate that most of the issues of all Web sites tested are not of a technical nature, and occur mainly due to human factors related to Web application development.",Higher education; Kyrgyz Republic; Web accessibility; Web usability,76,10.13,Q3,Usability attribute evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Speech emotion recognition with deep convolutional neural networks,"Issa, Dias; Fatih Demirci, M.; Yazici, Adnan",2020,journalArticle,59,,101894,Biomedical Signal Processing and Control,,Elsevier,10.1016/j.bspc.2020.101894,https://doi.org/10.1016/j.bspc.2020.101894,"The speech emotion recognition (or, classification) is one of the most challenging topics in data science. In this work, we introduce a new architecture, which extracts mel-frequency cepstral coefficients, chromagram, mel-scale spectrogram, Tonnetz representation, and spectral contrast features from sound files and uses them as inputs for the one-dimensional Convolutional Neural Network for the identification of emotions using samples from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Berlin (EMO-DB), and Interactive Emotional Dyadic Motion Capture (IEMOCAP) datasets. We utilize an incremental method for modifying our initial model in order to improve classification accuracy. All of the proposed models work directly with raw sound data without the need for conversion to visual representations, unlike some previous approaches. Based on experimental results, our best-performing model outperforms existing frameworks for RAVDESS and IEMOCAP, thus setting the new state-of-the-art. For the EMO-DB dataset, it outperforms all previous works except one but compares favorably with that one in terms of generality, simplicity, and applicability. Specifically, the proposed framework obtains 71.61% for RAVDESS with 8 classes, 86.1% for EMO-DB with 535 samples in 7 classes, 95.71% for EMO-DB with 520 samples in 7 classes, and 64.3% for IEMOCAP with 4 classes in speaker-independent audio classification tasks.",Deep learning; Signal processing; Speech emotion recognition,538,119.56,Q2,Affective state detection,Deep learning,Audio,yes,prototype,device independent
Detecting usability problems in mobile applications on the basis of dissimilarity in user behavior,"Jeong, JongWook; Kim, NeungHoe; In, Hoh Peter",2020,journalArticle,139,,,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2019.10.001,https://doi.org/10.1016/j.ijhcs.2019.10.001,"Usability is one of the critical success factors for mobile applications. However, usability is not easy to improve. Many usability testing studies have been conducted to detect usability problems easily, however they usually carry a substantial cost and require specific expertise. In this paper, we present methods and tools to detect usability problems effectively via usability testing in mobile application. We expect users to interact with mobile applications in dissimilar ways when encountering usability problems. Based on this approach, we propose tracing, modeling and comparing methods for measuring the similarity of user behaviors. We evaluate the similarity of user behavior and use it in detecting problems. In addition, we developed tools for our methods to run in automatic way to save usability testing costs. An experiment was conducted using two mobile applications to confirm that our proposed method is useful. We tested eight tasks and found lower similarity when users encountered a usability problem. The experimental results show that our methods and tools can help find usability problems in mobile applications.",Usability; Usability engineering; Mobile applications; Mobile computing; Usability testing; Behavioral research; User behaviors; Usability problems; Behavior modeling; Critical success factor; Behavior model,41,9.11,Q2,Usability issue encounter detection,Log-based sequence & pattern mining,User interactions,yes,prototype,mobile
Pattern mining of user interaction logs for a post-deployment usability evaluation of a radiology PACS client,"Jorritsma, Wiard; Cnossen, Fokie; Dierckx, Rudi A.; Oudkerk, Matthijs; van Ooijen, Peter M. A.",2016,journalArticle,85,1,36-42,International Journal of Medical Informatics,,Elsevier,10.1016/j.ijmedinf.2015.10.007,https://doi.org/10.1016/j.ijmedinf.2015.10.007,"Objectives To perform a post-deployment usability evaluation of a radiology Picture Archiving and Communication System (PACS) client based on pattern mining of user interaction log data, and to assess the usefulness of this approach compared to a field study. Methods All user actions performed on the PACS client were logged for four months. A data mining technique called closed sequential pattern mining was used to automatically extract frequently occurring interaction patterns from the log data. These patterns were used to identify usability issues with the PACS. The results of this evaluation were compared to the results of a field study based usability evaluation of the same PACS client. Results The interaction patterns revealed four usability issues: (1) the display protocols do not function properly, (2) the line measurement tool stays active until another tool is selected, rather than being deactivated after one use, (3) the PACS's built-in 3D functionality does not allow users to effectively perform certain 3D-related tasks, (4) users underuse the PACS’s customization possibilities. All usability issues identified based on the log data were also found in the field study, which identified 48 issues in total. Conclusions Post-deployment usability evaluation based on pattern mining of user interaction log data provides useful insights into the way users interact with the radiology PACS client. However, it reveals few usability issues compared to a field study and should therefore not be used as the sole method of usability evaluation.",Data mining; Picture archiving and communication system; Usability; User interaction log data; User-computer interface,25,2.94,Q1,Usability issue encounter detection,Log-based sequence & pattern mining,User interactions,yes,prototype,desktop
Usability Testing of Virtual Reality Applications—The Pilot Study,"Kamińska, Dorota; Zwoliński, Grzegorz; Laska-Leśniewicz, Anna",2022,journalArticle,22,4,1342,Sensors,,MDPI,10.3390/s22041342,https://doi.org/10.3390/s22041342,"The need for objective data-driven usability testing of VR applications is becoming more tangible with the rapid development of numerous VR applications and their increased accessibility. Traditional methods of testing are too time and resource consuming and might provide results that are highly subjective. Thus, the aim of this article is to explore the possibility of automation of usability testing of VR applications by using objective features such as HMD built-in head and hands tracking, EEG sensor, video recording, and other measurable parameters in addition to automated analysis of subjective data provided in questionnaires. For this purpose, a simple VR application was created which comprised relatively easy tasks that did not generate stress for the users. Fourteen volunteers took part in the study and their signals were monitored to acquire objective automated data. At the same time the observer was taking notes of subjects’ behaviour, and their subjective opinions about the experience were recorded in a post-experiment questionnaire. The results acquired from signal monitoring and questionnaires were juxtaposed with observation and post-interview results to confirm the validity and efficacy of automated usability testing. The results were very promising, proving that automated usability testing of VR applications is potentially achievable.",usability; UX; human-computer interaction; virtual reality; testing; validation,84,33.6,Q2,Usability issue encounter detection,Machine learning,"User movements, Physiological signals, Audio, User interactions, Video",yes,prototype,VR
Analyzing repetitive action in game based on sequence pattern matching,"Kang, Shin Jin; Kim, Young Bin; Kim, Soo Kyun",2014,journalArticle,9,3,523 – 530,Journal of Real-Time Image Processing,,Springer Nature,10.1007/s11554-013-0347-0,https://doi.org/10.1007/s11554-013-0347-0,"As games become more popular, procedures which can support the analysis and understanding of players' behaviors are necessary for success of commercial games. This paper presents a log-based usability evaluation system to analyze user behavior in a gaming environment. We explore the potential of input log data for automated usability evaluation and visualization of player behavior in a game. We traced the keyboard input value and mouse movement of users using a sequence data mining technique in a gaming environment. And we also constructed 3D body meshes for the behavior analysis using Kinect interface. We visualized the data obtained by tracing and automatically searched repetitive patterns in the game and analyzed them. The result obtained from the analysis can be used for user interface optimization, fun evaluation, and the bot-detection field. © 2013 Springer-Verlag Berlin Heidelberg.",User interfaces; Data visualization; Automated usability evaluation; Usability evaluation; Behavioral research; Pattern matching; Behavior analysis; Mouse movements; Player behavior; Repetitive pattern; Sequence patterns; Sequence-data mining; Log data analysis; Mouse tracking,9,0.86,Q2,Usability attribute evaluation,Log-based sequence & pattern mining,"User interactions, User movements",yes,prototype,desktop
Mouse behavioral patterns and keystroke dynamics in End-User Development: What can they tell us about users’ behavioral attributes?,"Katerina, Tzafilkou; Nicolaos, Protogeros",2018,journalArticle,83,,,Computers in Human Behavior,,Elsevier,10.1016/j.chb.2018.02.012,https://doi.org/10.1016/j.chb.2018.02.012,"Studying human behavior is of particular interest within the field of Human-Computer Interaction (HCI) as it can provide insight into human performance. Prior HCI research suggests that mouse and keyboard monitoring may provide a more complete picture of user behavior under high cognitive loads like decision making and developing tasks. In this exploratory study we investigate the potential correlation between mouse behavioral patterns or keystroke dynamics and a set of End-User Development (EUD) behavioral attributes. We conduct a field test on 30 end-users interacting with a modern web-based EUD tool for the construction of simple web forms. Our findings reveal the existence of several significant correlations between end-users’ behavioral attributes and mouse pattern metrics or keystroke dynamics during the development process. Mouse pattern metrics like random and straight movements, mouse hovers, etc., can be associated with perceived ease use, perceived usefulness, self-efficacy, willingness to learn or risk-perception. Similarly, some keystroke dynamics like key press speed and down-to-down time can be associated with perceived ease of use or self-efficacy. The findings of this work show a new interesting research direction and may motivate the EUD research community to study further the end-users’ mouse and keyboard behavior in today's web-based EUD systems.",End-user behavior; End-User Development (EUD); Keystroke dynamics; Mouse behavioral patterns; Mouse tracking,63,9.69,Q1,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,desktop
An Empirical Performance Evaluation of Universities Website,"Kaur, Sukhpuneet; Kaur, Kulwant; Kaur, Parminder",2016,journalArticle,146,,10-16,International Journal of Computer Applications,,Taylor & Francis,10.5120/ijca2016910922,https://doi.org/10.5120/ijca2016910922,,Usability; Web Service; testing; speed; load time; Pingdom tool; GTMetrix tool; Site Speed Checker tool; Website Grader tool,107,12.59,Q4,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,"desktop, mobile"
"Evaluating the accessibility, usability and security of Hospitals websites: An exploratory study","Kaur, Arvinder; Dani, Diksha; Agrawal, Gaurav",2017,conferencePaper,,,674-680,"2017 7th International Conference on Cloud Computing, Data Science & Engineering - Confluence",Confluence,IEEE,10.1109/CONFLUENCE.2017.7943237,https://doi.org/10.1109/CONFLUENCE.2017.7943237,"Technology offers the potential to improve healthcare service delivery. The objective of healthcare Web site is to provide services and updated information at low cost to all the people regardless of their abilities and disabilities, which can reduce overcrowding in hospitals and reduce spread of disease. In a developing country like India, where hospitals are overcrowded, healthcare Web sites can play a major role in delivering updated healthcare services. Therefore, designing an effective healthcare Web site is becoming essential as numerous people are accessing the Web for gathering information about hospitals and their healthcare services. There are no specific guidelines available for designing the health care Web site. So, it has become extremely important to evaluate hospital Web sites and address the design issues. The objective of this paper is to evaluate the accessibility, usability and security of hospitals Web sites in metro cities of India. Accessibility is evaluated using WCAG 2.0 accessibility guidelines, usability is evaluated using readability score and language analysis. Security analysis takes content management system into account.",Cloud computing; Conferences; Data science; Handheld computers; healthcare; Security Hospital; Usability; Web accessibility; websites,39,5.2,-,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,desktop
Supporting Developers in Addressing Human-Centric Issues in Mobile Apps,"Khalajzadeh, Hourieh; Shahin, Mojtaba; Obie, Humphrey O.; Agrawal, Pragya; Grundy, John",2023,journalArticle,49,4,2149 – 2168,IEEE Transactions on Software Engineering,,IEEE,10.1109/TSE.2022.3212329,https://doi.org/10.1109/TSE.2022.3212329,"Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development. © 1976-2012 IEEE.",Application programs; Australia; Deep learning; E-learning; Github repository; Google play store; Google plays; Human aspects; Human-centric; Human-centric issue; Machine-learning; Mobile applications; Security; Software design; Software development management; Software-systems; Taxonomies,22,14.67,Q1,Feedback evaluation,Machine learning,Text,yes,prototype,device independent
Comparing Data from Chatbot and Web Surveys: Effects of Platform and Conversational Style on Survey Response Quality,"Kim, Soomin; Lee, Joonhwan; Gweon, Gahgene",2019,conferencePaper,,,1–12,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3290605.3300316,https://doi.org/10.1145/3290605.3300316,"This study aims to explore the feasibility of a text-based virtual agent as a new survey method to overcome the web survey's common response quality problems, which are caused by respondents' inattention. To this end, we conducted a 2 (platform: web vs. chatbot) × 2 (conversational style: formal vs. casual) experiment. We used satisficing theory to compare the responses' data quality. We found that the participants in the chatbot survey, as compared to those in the web survey, were more likely to produce differentiated responses and were less likely to satisfice; the chatbot survey thus resulted in higher-quality data. Moreover, when a casual conversational style is used, the participants were less likely to satisfice-although such effects were only found in the chatbot condition. These results imply that conversational interactivity occurs when a chat interface is accompanied by messages with effective tone. Based on an analysis of the qualitative responses, we also showed that a chatbot could perform part of a human interviewer's role by applying effective communication strategies.",user experience design; survey interface; human ai interaction; conversational interface; conversational agent; chatbot,237,43.09,A*,Research assistants,"Rule-based systems, ChatBot",Text,yes,prototype,mobile
Predicting user satisfaction with intelligent assistants,"Kiseleva, Julia; Crook, Aidan C.; Williams, Kyle; Zitouni, Imed; Awadallah, Ahmed Hassan; Anastasakos, Tasos",2016,conferencePaper,,,45 – 54,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,SIGIR,ACM,10.1145/2911451.2911521,https://doi.org/10.1145/2911451.2911521,"There is a rapid growth in the use of voice-controlled intelligent personal assistants on mobile devices, such as Microsoft's Cortana, Google Now, and Apple's Siri. They significantly change the way users interact with search systems, not only because of the voice control use and touch gestures, but also due to the dialogue-style nature of the interactions and their ability to preserve context across different queries. Predicting success and failure of such search dialogues is a new problem, and an important one for evaluating and further improving intelligent assistants. While clicks in web search have been extensively used to infer user satisfaction, their significance in search dialogues is lower due to the partial replacement of clicks with voice control, direct and voice answers, and touch gestures. In this paper, we propose an automatic method to predict user satisfaction with intelligent assistants that exploits all the interaction signals, including voice commands and physical touch gestures on the device. First, we conduct an extensive user study to measure user satisfaction with intelligent assistants, and simultaneously record all user interactions. Second, we show that the dialogue style of interaction makes it necessary to evaluate the user experience at the overall task level as opposed to the query level. Third, we train a model to predict user satisfaction, and find that interaction signals that capture the user reading patterns have a high impact: when including all available interaction signals, we are able to improve the prediction accuracy of user satisfaction from 71% to 81% over a baseline that utilizes only click and query features. © 2016 ACM.",Forecasting; Information retrieval; Intelligent assistants; Mobile devices; Mobile search; Mobile telecommunication systems; Speech processing; Spoken dialogue system; User experience; User interfaces; User satisfaction; User study; World Wide Web,177,20.82,A*,Usability attribute evaluation,Machine learning,"User interactions, Text",yes,prototype,mobile
Computational layout perception using Gestalt laws,"Koch, Janin; Oulasvirta, Antti",2016,conferencePaper,07-12-May-2016,,1423 – 1429,CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2851581.2892537,https://doi.org/10.1145/2851581.2892537,"We present preliminary results on computational perception of interactive layouts. Our goal is to algorithmically estimate how users perceive a layout. Potential applications range from automated usability evaluation to computer-generated and adaptive interfaces. Layout perception is challenging, however, because of diverse features, combinatorial complexity, and absence of approaches. We have explored Gestalt laws as parsing heuristics. Our approach finds a parametrization that optimally resolves conflicts among competing interpretations of a layout. The output is a hierarchical grouping of main elements. The results are promising: an implementation of just four Gestalt laws enables hierarchical grouping that presents promising results in 90% of our (realistic) test cases. © 2016 Authors.",Adaptive interface; Application range; Automated usability evaluation; Combinatorial complexity; Computational perception; Computer generated; Diverse features; Gestalt law; Hierarchical groupings; Human computer interaction; Human engineering; User interface layouts; User interfaces,25,2.94,A*,Perceived affordance evaluation,Rule-based systems,Source code,no,prototype,desktop
Predicting Final User Satisfaction Using Momentary UX Data and Machine Learning Techniques,"Koonsanit, Kitti; Nishiuchi, Nobuyuki",2021,journalArticle,16,7,3136-3156,Journal of Theoretical and Applied Electronic Commerce Research,,MDPI,10.3390/jtaer16070171,https://doi.org/10.3390/jtaer16070171,"User experience (UX) evaluation investigates how people feel about using products or services and is considered an important factor in the design process. However, there is no comprehensive UX evaluation method for time-continuous situations during the use of products or services. Because user experience changes over time, it is difficult to discern the relationship between momentary UX and episodic or cumulative UX, which is related to final user satisfaction. This research aimed to predict final user satisfaction by using momentary UX data and machine learning techniques. The participants were 50 and 25 university students who were asked to evaluate a service (Experiment I) or a product (Experiment II), respectively, during usage by answering a satisfaction survey. Responses were used to draw a customized UX curve. Participants were also asked to complete a final satisfaction questionnaire about the product or service. Momentary UX data and participant satisfaction scores were used to build machine learning models, and the experimental results were compared with those obtained using seven built machine learning models. This study shows that participants’ momentary UX can be understood using a support vector machine (SVM) with a polynomial kernel and that momentary UX can be used to make more accurate predictions about final user satisfaction regarding product and service usage.",user experience; UX; UX evaluation; satisfaction; prediction; machine learning,20,5.71,Q2,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,desktop
Critique Style Guide: Improving Crowdsourced Design Feedback with a Natural Language Model,"Krause, Markus; Garncarz, Tom; Song, JiaoJiao; Gerber, Elizabeth M.; Bailey, Brian P.; Dow, Steven P.",2017,conferencePaper,,,4627–4639,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3025453.3025883,https://doi.org/10.1145/3025453.3025883,"Designers are increasingly leveraging online crowds; yet, online contributors may lack the expertise, context, and sensitivity to provide effective critique. Rubrics help feedback providers but require domain experts to write them and may not generalize across design domains. This paper introduces and tests a novel semi-automated method to support feedback providers by analyzing feedback language. In our first study, 52 students from two design courses created design solutions and received feedback from 176 online providers. Instructors, students, and crowd contributors rated the helpfulness of each feedback response. From this data, an algorithm extracted a set of natural language features (e.g., specificity, sentiment etc.) that correlated with the ratings. The features accurately predicted the ratings and remained stable across different raters and design solutions. Based on these features, we produced a critique style guide with feedback examples - automatically selected for each feature - to help providers revise their feedback through self-assessment. In a second study, we tested the validity of the guide through a between-subjects experiment (n=50). Providers wrote feedback on design solutions with or without the guide. Providers generated feedback with higher perceived helpfulness when using our style-based guidance.",Design; critique; feedback; crowdsourcing; expertise; rubrics,60,8,A*,Feedback evaluation,NLP,Text,yes,prototype,device independent
Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text),"Kuang, Emily; Jahangirzadeh Soure, Ehsan; Fan, Mingming; Zhao, Jian; Shinohara, Kristen",2023,conferencePaper,,,1–15,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3544548.3581247,https://doi.org/10.1145/3544548.3581247,"AI is promising in assisting UX evaluators with analyzing usability tests, but its judgments are typically presented as non-interactive visualizations. Evaluators may have questions about test recordings, but have no way of asking them. Interactive conversational assistants provide a Q&amp;A dynamic that may improve analysis efficiency and evaluator autonomy. To understand the full range of analysis-related questions, we conducted a Wizard-of-Oz design probe study with 20 participants who interacted with simulated AI assistants via text or voice. We found that participants asked for five categories of information: user actions, user mental model, help from the AI assistant, product and task information, and user demographics. Those who used the text assistant asked more questions, but the question lengths were similar. The text assistant was perceived as significantly more efficient, but both were rated equally in satisfaction and trust. We also provide design considerations for future conversational AI assistants for UX evaluation.",Conversational assistants; Human-AI collaboration; UX evaluation; Usability testing; User experience (UX),17,11.33,A*,Research assistants,"Machine learning, ChatBot","User interactions, Audio, Video",yes,prototype,desktop
Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing,"Kuang, Emily; Li, Minghao; Fan, Mingming; Shinohara, Kristen",2024,conferencePaper,,,,Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3613904.3642168,https://doi.org/10.1145/3613904.3642168,"Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful, but they collectively identified more than twice as many problems, underscoring the irreplaceable role of human expertise. Our findings also offer insights into future human-AI collaborative tools for UX evaluation.",User experience; Natural languages; Usability testing; Human-AI collaboration; Timing circuits; Performance; Users' experiences; Interactive system; Human engineering; Wizard-of-oz studies; Usability problems; Human actor; Proactive conversational assistant; Proactive conversational assistants,5,10,A*,Research assistants,"LLM, ChatBot","Audio, Video",yes,prototype,desktop
E-Commerce Website Usability Analysis Using the Association Rule Mining and Machine Learning Algorithm,"Kumar, Biresh; Roy, Sharmistha; Sinha, Anurag; Iwendi, Celestine; Strazovska, Lubomira",2023,journalArticle,11,1,,Mathematics,,MDPI,10.3390/math11010025,https://doi.org/10.3390/math11010025,"The overall effectiveness of a website as an e-commerce platform is influenced by how usable it is. This study aimed to find out if advanced web metrics, derived from Google Analytics software, could be used to evaluate the overall usability of e-commerce sites and identify potential usability issues. It is simple to gather web indicators, but processing and interpretation take time. This data is produced through several digital channels, including mobile. Big data has proven to be very helpful in a variety of online platforms, including social networking and e-commerce websites, etc. The sheer amount of data that needs to be processed and assessed to be useful is one of the main issues with e-commerce today as a result of the digital revolution. Additionally, on social media a crucial growth strategy for e-commerce is the usage of BDA capabilities as a guideline to boost sales and draw clients for suppliers. In this paper, we have used the KMP algorithm-based multivariate pruning method for web-based web index searching and different web analytics algorithm with machine learning classifiers to achieve patterns from transactional data gathered from e-commerce websites. Moreover, through the use of log-based transactional data, the research presented in this paper suggests a new machine learning-based evaluation method for evaluating the usability of e-commerce websites. To identify the underlying relationship between the overall usability of the eLearning system and its predictor factors, three machine learning techniques and multiple linear regressions are used to create prediction models. This strategy will lead the e-commerce industry to an economically profitable stage. This capability can assist a vendor in keeping track of customers and items they have viewed, as well as categorizing how customers use their e-commerce emporium so the vendor can cater to their specific needs. It has been proposed that machine learning models, by offering trustworthy prognoses, can aid in excellent usability. Such models might be incorporated into an online prognostic calculator or tool to help with treatment selection and possibly increase visibility. However, none of these models have been recommended for use in reusability because of concerns about the deployment of machine learning in e-commerce and technical issues. One problem with machine learning science that needs to be solved is explainability. For instance, let us say B is 10 and all the people in our population are even. The hash function's behavior is not random since only buckets 0, 2, 4, 6, and 8 can be the value of h(x). However, if B = 11, we would find that 1/11th of the even integers is transmitted to each of the 11 buckets. The hash function would work well in this situation.",association rule; big data analytics; collaborative filtering; e-commerce usability; hashing; KMP algorithm; machine learning; web mining,44,29.33,Q1,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,"desktop, mobile"
Unmoderated Usability Studies Evolved: Can GPT Ask Useful Follow-up Questions?,"Kuric, Eduard; Demcak, Peter; Krajcovic, Matus",2024,journalArticle,,,,International Journal of Human–Computer Interaction,,Taylor & Francis,10.1080/10447318.2024.2427978,https://doi.org/10.1080/10447318.2024.2427978,"Follow-up questions during usability testing provide crucial insights into the user’s experience with a product or service. In unmoderated usability tests conducted online, artificial intelligence ...",ChatGPT; usability testing; follow-up questions; unmoderated study; website prototype,0,0,Q2,Research assistants,LLM,Text,yes,prototype,desktop
Analytical Framework for Facial Expression on Game Experience Test,"Kwon, Seungjin; Ahn, Jaehyun; Choi, Hyukgeun; Jeon, Jiho; Kim, Doyoung; Kim, Hoyeon; Kang, Shinjin",2022,journalArticle,10,,104486-104497,IEEE Access,,IEEE,10.1109/ACCESS.2022.3210712,https://doi.org/10.1109/ACCESS.2022.3210712,"Game experience testing, an essential process for developing and servicing high-quality games, aims to evaluate and improve the play experience from the user perspective. Previous game experience tests are usually held offline with recruited test participants. Participants play the game in a controlled environment, and their experiences are recorded and analyzed by domain experts. However, this process not only requires substantial resources in terms of time and cost but also misses momentary data that can infer the user’s experience directly and quantitatively, such as facial expressions or body gestures of the participants while playing the game. In this paper, we propose a framework that can automatically collect and analyze video data of participants in a remote environment. We designed various experiments to verify that the proposed framework can automatically collect and analyze facial data in a remote environment. The experimental results show that our framework can be applied in gaming experience tests for popular commercial games currently in service. This paper presents a pioneering work that applies the face analysis method to the commercial game development process, and its purpose is beyond that of a pure research work.",face detection; Face recognition; facial expression recognition; game experience test framework; Game user research; Games; head pose recognition; Pose estimation; Sensors; Streaming media; Testing; User experience; WebRTC,6,2.4,Q2,Affective state detection,"Deep learning, Computer vision",Video,yes,prototype,desktop
Automatic Graphical User Interface aesthetic evaluation tool using the UIED segmentation algorithm,"Kyelem, Yacouba; Zo, Mamadou; Kabore, Kisito K.; Ouedraogo, Frédéric T.",2023,conferencePaper,,,20–26,Proceedings of the 2023 3rd International Conference on Human Machine Interaction,INTERACT,ACM,10.1145/3604383.3604387,https://doi.org/10.1145/3604383.3604387,"In this work, we deal with the issue of automating the evaluation of the aesthetics of the user's applications interfaces. The aesthetics of the user interface is a sub-factor of software quality. The aesthetics of the interface provides several advantages to the users, such as the attraction of the interface, an impact on the usability of the interface, a great pleasure of the interface's use, etc. That is why we deal with the evaluation of the aesthetics interfaces application. Our goal is to suggest a tool for quantitative evaluation of user interfaces using metrics. That is why we have developed a tool for evaluating the aesthetics of application interfaces that takes screenshots of mobile and desktop applications. This tool also generates metrics as output. For the segmentation of the different components of the interfaces we used the UIED algorithm which is an algorithm for the automatic segmentation of images. Our tool uses 10 of the 14 metrics of Ngo et al [1]. We generated the values with our tool by taking the images of a site. We used a total of 351 images. The validation of our tool was done by comparing the data calculated by our tool to those of QUESTIM. We used Pearson correlation test to perform the comparison and we obtained a correlation rate of 0.91. This means that our tool and QUESTIM evaluate the interfaces in almost the same way.",Aesthetic automatic evaluation; Aesthetic metric; Quantitative approach; User Interface Element Detection (UIED),0,0,B,Aesthetics evaluation,Deep learning,Images,no,tool,desktop
A New Approach to Measure User Experience with Voice-Controlled Intelligent Assistants: A Pilot Study,"Le Pailleur, Félix; Huang, Bo; Léger, Pierre-Majorique; Sénécal, Sylvain",2020,conferencePaper,,,197-208,Human-Computer Interaction. Multimodal and Natural Interaction,HCII,Springer Nature,10.1007/978-3-030-49062-1_13,https://doi.org/10.1007/978-3-030-49062-1_13,"Voice-controlled intelligent assistants use a conversational user interface (CUI), a system that relies on natural language processing and artificial intelligence to have verbal interactions with end-users. In this research, we propose a multi-method approach to assess user experience with a smart voice assistant through triangulation of psychometric and psychophysiological measures. The approach aims to develop a richer understanding of what the users experience during the interaction, which could provide new insights to researchers and developers in the field of voice assistant. We apply this new approach in a pilot study, and we show that each method captures a part of emotional variance during the interaction. Results suggest that emotional valence is better captured with psychometric measures, whereas arousal is better detected with psychophysiological measures.",User experience; Emotion; Arousal; Conversational user interface; Human-Computer Interactions; Valence; Vocal assistant,14,3.11,B,Affective state detection,"Deep learning, Computer vision","Physiological signals, Video",yes,prototype,desktop
Continuous Usability Requirements Evaluation based on Runtime User Behavior Mining,"Li, Tong; Zhang, Tianai",2022,conferencePaper,2022-December,,1036 – 1045,"2022 IEEE 22nd International Conference on Software Quality, Reliability and Security",QRS,IEEE,10.1109/QRS57517.2022.00107,https://doi.org/10.1109/QRS57517.2022.00107,"Usability requirements have been widely recognized as an essential quality requirement for systems that interact with people. However, evaluating the satisfaction of usability requirements usually involves user interactions, which is intrusive and time-consuming. In this paper, we propose a novel framework for systematically and automatically evaluating the satisfaction of usability requirements at runtime. Specifically, a behavior-centric conceptual model is proposed to comprehensively characterize user behaviors. An analysis process is then proposed based on the conceptual model, which systematically refines high-level usability requirements into observable and measurable user behaviors in order to automatically evaluate their satisfaction. Moreover, we investigate and mine patterns of user behaviors, which further explain the results of the satisfaction analysis. We systematically design and conduct a case study to evaluate our proposed framework, the results of which show that our approach is able to identify most usability issues and precisely assess the satisfaction of participants' usability requirements. Importantly, our approach enables continuous usability requirements evaluation without interfering with users, pragmatically contributing to trade-off analysis among quality requirements at runtime. © 2022 IEEE.",Usability engineering; Software quality; Behavioral sciences; Analytical models; Systematics; Software reliability; Planning; Behavioral research; Runtimes; Quality control; Economic and social effects; User behaviors; Requirements engineering; Usability requirements; Requirement analysis; Behavior pattern mining; Behaviour patterns; Conceptual model; Pattern mining; Runtime requirement analyse; Usability smell; Usability smells; Runtime; Conceptual modeling; Runtime requirements analysis,1,0.4,B,Usability attribute evaluation,Rule-based systems,User interactions,yes,prototype,"desktop, mobile"
Evaluating User’s Emotional Experience in HCI: The PhysiOBS Approach,"Liapis, Alexandros; Karousos, Nikos; Katsanos, Christos; Xenos, Michalis",2014,conferencePaper,,,758-767,Human-Computer Interaction. Advanced Interaction Modalities and Techniques,HCII,Springer Nature,10.1007/978-3-319-07230-2_72,https://doi.org/10.1007/978-3-319-07230-2_72,"As computing is changing parameters, apart from effectiveness and efficiency in human-computer interaction, such as emotion have become more relevant than before. In this paper, a new tool-based evaluation approach of user’s emotional experience during human-computer interaction is presented. The proposed approach combines user’s physiological signals, observation data and self-reported data in an innovative tool (PhysiOBS) that allows continuous and multiple emotional states analysis. To the best of our knowledge, such an approach that effectively combines all these user-generated data in the context of user’s emotional experience evaluation does not exist. Results from a preliminary evaluation study of the tool were rather encouraging revealing that the proposed approach can provide valuable insights to user experience practitioners.",Emotions; Evaluation; Human Computer Interaction; Physiological Signals; User Emotional Experience,24,2.29,B,Affective state detection,"Deep learning, Computer vision","Physiological signals, Video, Gaze data",yes,tool,desktop
Advancing stress detection methodology with deep learning techniques targeting ux evaluation in aal scenarios: Applying embeddings for categorical variables,"Liapis, Alexandros; Faliagka, Evanthia; Antonopoulos, Christos P.; Keramidas, Georgios; Voros, Nikolaos",2021,journalArticle,10,13,,Electronics,,MDPI,10.3390/electronics10131550,https://doi.org/10.3390/electronics10131550,"Physiological measurements have been widely used by researchers and practitioners in order to address the stress detection challenge. So far, various datasets for stress detection have been recorded and are available to the research community for testing and benchmarking. The majority of the stress-related available datasets have been recorded while users were exposed to intense stressors, such as songs, movie clips, major hardware/software failures, image datasets, and gaming scenarios. However, it remains an open research question if such datasets can be used for creating models that will effectively detect stress in different contexts. This paper investigates the performance of the publicly available physiological dataset named WESAD (wearable stress and affect detection) in the context of user experience (UX) evaluation. More specifically, electrodermal activity (EDA) and skin temperature (ST) signals from WESAD were used in order to train three traditional machine learning classifiers and a simple feed forward deep learning artificial neural network combining continues variables and entity embeddings. Regarding the binary classification problem (stress vs. no stress), high accuracy (up to 97.4%), for both training approaches (deep-learning, machine learning), was achieved. Regarding the stress detection effectiveness of the created models in another context, such as user experience (UX) evaluation, the results were quite impressive. More specifically, the deep-learning model achieved a rather high agreement when a user-annotated dataset was used for validation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",deep learning; Deep learning; UX evaluation; Stress detection; electrodermal activity; entity embeddings; stress detection; Electrodermal activity; Entity embeddings,11,3.14,Q3,Affective state detection,"Deep learning, Machine learning",Physiological signals,yes,prototype,wearables
Detection of Subtle Stress Episodes During UX Evaluation: Assessing the Performance of the WESAD Bio-Signals Dataset,"Liapis, Alexandros; Faliagka, Evanthia; Katsanos, Christos; Antonopoulos, Christos; Voros, Nikolaos",2021,conferencePaper,12934 LNCS,,238 – 247,Human-Computer Interaction–INTERACT 2021: 18th IFIP TC 13 International Conference,INTERACT,Springer Nature,10.1007/978-3-030-85613-7_17,https://doi.org/10.1007/978-3-030-85613-7_17,"Stress is a highly subjective condition and may largely vary in different contexts. Bio-signals have been widely used by researchers and practitioners to monitor stress levels. Consequently, various bio-signals datasets for stress recognition have been recorded. The most of publicly available physiological datasets have been emotionally annotated in a context where users have been exposed to intense stressors, such as movie clips, songs, major hardware/software failures, image datasets, and gaming. However, it remains unexplored how effectively such datasets can be used in different contexts. This paper investigates the performance of the publicly available dataset named WESAD (Wearable Stress and Affect Detection) in the context of UX evaluation. More specifically, skin conductance signal from WESAD was used to train four machine learning classifiers. Regarding the binary classification problem (stress vs. no stress), models’ accuracy was rather high (at least 91.1%). However, it was found that their effectiveness in assessing stress in the context of UX was rather poor when a new bio-signals dataset was used. © 2021, IFIP International Federation for Information Processing.",Affect detection; Binary classification problems; Hardware/software; Human computer interaction; Image datasets; Learning systems; Movie clips; Skin conductance; Stress levels; Stress recognition,19,5.43,B,Affective state detection,"Deep learning, Machine learning",Physiological signals,yes,prototype,wearables
Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Liu, Zhe; Chen, Chunyang; Wang, Junjie; Huang, Yuekai; Hu, Jun; Wang, Qing",2023,journalArticle,49,1,403-418,IEEE Transactions on Software Engineering,,IEEE,10.1109/TSE.2022.3150876,https://doi.org/10.1109/TSE.2022.3150876,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.",deep learning; Deep learning; Graphical user interfaces; Location awareness; Software; Internet; UI display; UI testing; Visual effects; Training data; mobile app; Computer bugs; object detection; Application programs; Program debugging; End-users; Display devices; Fully automated; Heuristic methods; Interface complexity; Screenshots; Software applications,33,22,Q1,Automatic guideline evaluation,"Deep learning, Computer vision",Images,no,tool,mobile
Conversation-based hybrid UI for the repertory grid technique: A lab experiment into automation of qualitative surveys,"Liu, Yunxing; Martens, Jean-Bernard ",2024,journalArticle,184,,103227,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2024.103227,https://doi.org/10.1016/j.ijhcs.2024.103227,"A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language.",Chatbot; Qualitative survey automation; Hybrid UI; Repertory grid technique,4,8,Q1,Research assistants,"LLM, ChatBot","Images, Text, User interactions",yes,prototype,desktop
On the automatic classification of app reviews,"Maalej, Walid; Kurtanović, Zijad; Nabil, Hadeer; Stanik, Christoph",2016,journalArticle,21,3,311-331,Requirements Engineering,,Springer Nature,10.1007/s00766-016-0251-9,https://doi.org/10.1007/s00766-016-0251-9,"App stores like Google Play and Apple AppStore have over 3 million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text—particularly with bigrams and lemmatization—the classification precision for all review types got up to 88–92 % and the recall up to 90–99 %. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize nine interviews with practitioners on how review analytics tools including ours could be used in practice.",Machine learning; Natural language processing; User feedback; Data-driven requirements engineering; Review analytics; Software analytics,331,38.94,Q2,Feedback evaluation,NLP,Text,yes,prototype,device independent
"Accessibility, usability, and security evaluation of universities’ prospective student web pages: a comparative study of Europe, North America, and Oceania","Macakoğlu, Şevval Seray; Peker, Serhat; Medeni, İhsan Tolga",2023,journalArticle,22,2,671 – 683,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-022-00869-9,https://doi.org/10.1007/s10209-022-00869-9,"Universities' prospective student web pages aim to disseminate information about their academic and social opportunities to their stakeholders; therefore, they must be accessible, of high quality of use and reliable. This article presents the accessibility, usage performance, and security analysis of prospective student web pages of 330 universities from three continents, namely Europe, North America, and Oceania. For this purpose, university websites were selected based on the Webometrics ranking, and online automated test tools were used. The results showed that websites at universities in North America paid more attention to accessibility and quality of use on prospective student web pages, followed by Oceanian and European websites. Evaluated websites had low compliance levels according to the WCAG 2.0 guideline. No major problems were identified in terms of usability and security, but there were certain points for improvement. Moreover, we present and discuss recommendations to developers and administrators for websites to resolve accessibility, usability, and security breaches and provide information equally to all stakeholders. Hence, this analysis report provides feedback to web developers to improve accessibility, quality of use, and security issues of university websites and their prospective student web pages. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Human computer interaction; Information dissemination; Performances evaluation; Prospectives; Quality control; Students; Usability and security; Usability engineering; Web accessibility; Web Design; Web information systems; Web performance; Web performance evaluation; WEB security; Web usability; Web-page,48,32,Q3,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,"desktop, mobile"
DeepFlow: Detecting Optimal User Experience From Physiological Data Using Deep Neural Networks,"Maier, Marco; Elsner, Daniel; Marouane, Chadly; Zehnle, Meike; Fuchs, Christoph",2019,conferencePaper,,,1415-1421,Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19),IJCAI,ACM,10.24963/ijcai.2019/196,https://doi.org/10.24963/ijcai.2019/196,Electronic proceedings of IJCAI 2019,Humans and AI: Human-Computer Interaction; Machine Learning Applications: Applications of Supervised Learning,73,13.27,A*,Affective state detection,Deep learning,Physiological signals,yes,prototype,desktop
A Framework for Improving Web Application User Interfaces Through Immediate Evaluation,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2016,journalArticle,291,,283-296,Frontiers in Artificial Intelligence and Applications,,IOS Press,10.3233/978-1-61499-714-6-283,https://doi.org/10.3233/978-1-61499-714-6-283,"More and more web applications are being migrated from desktop platforms to platforms for smartphones and tablets. Usability and User experience are highly different on desktop and portable devices. Every minor modification of user interface (UI) could lead to violations of usability guidelines, e.g. changing the link color or font size could lead to lower usability for people with disabilities. Empirical usability evaluation methods are the approaches that help to verify the usability conformance to guidelines. Nevertheless, there are number of obstacles preventing from using such methods widely. The purpose of our study is to propose a conceptual model and corresponding framework with methodology including category specific metrics for immediate automatic usability evaluation of web application user interfaces during design and implementation phase. We address the gap between usability evaluation and development stage of UI by providing immediate feedback to developers.",User interfaces; Usability engineering; Smart phones; Usability evaluation; WEB application; Web applications; Users' experiences; Web usability; Portable device; Usability guidelines; Web user interface; Desktop devices; web usability; usability guidelines; web user interface,2,0.24,Q3,Automatic guideline evaluation,Descriptive metrics,Source code,no,prototype,desktop
A Study on Immediate Automatic Usability Evaluation of Web Application User Interfaces,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2016,conferencePaper,615,,257-271,Databases and Information Systems: 12th International Baltic Conference,DB&IS,Springer Nature,10.1007/978-3-319-40180-5_18,https://doi.org/10.1007/978-3-319-40180-5_18,"More and more web applications are being migrated from desktop platforms to mobile platforms. User experience is extremely different on desktop and portable devices. Changes in user interfaces (UI) could lead to severe violations of usability rules, e.g. changing the text color could lead to decrease of accessibility for users with low vision or cognitive impairments. Manual usability inspection methods are the approaches that help to verify the usability conformance to guidelines. Nevertheless, there are number of difficulties why the aforementioned approaches could not be always applied. The purpose of our research is to develop a conceptual model and corresponding framework including category specific metrics with methodology for immediate automatic usability evaluation of web application user interfaces during design and implementation phase. We address the gap between usability evaluation and development stage of user interface by providing immediate feedback to UI developers.",User interfaces; Usability engineering; Information systems; Usability evaluation; Design and implementations; Web usability; Immediate feedbacks; Usability inspection; World Wide Web; Usability guidelines; Web user interface; Cognitive impairment,8,0.94,B,Automatic guideline evaluation,Descriptive metrics,Source code,no,prototype,desktop
A tool for design-time usability evaluation of web user interfaces,"Marenkov, Jevgeni; Robal, Tarmo; Kalja, Ahto",2017,conferencePaper,10509 LNCS,,394 – 407,Advances in Databases and Information Systems: 21st European Conference,ADBIS,Springer Nature,10.1007/978-3-319-66917-5_26,https://doi.org/10.1007/978-3-319-66917-5_26,"The diversity of smartphones and tablet computers has become intrinsic part of modern life. Following usability guidelines while designing web user interface (UI) is an essential requirement for each web application. Even a minor change in UI could lead to usability problems, e.g. changing background or foreground colour of buttons could cause usability problems especially for people with disabilities. Empirical evaluation methods such as questionnaires and Card Sorting are effective in finding such problems. Nevertheless, these methods cannot be used widely when time, money and evaluators are scarce. The purpose of our work is to deliver a tool for design-time automatic evaluation of UI conformance to category-specific usability guidelines. The main contribution of this solution is enabling immediate cost-efficient and automatic web UI evaluation that conforms to available and set standards. This approach is being integrated into the Estonian eGovernment authority in order to automate usability evaluation of web applications. © 2017, Springer International Publishing AG.",Automatic evaluation; Category specifics; Empirical evaluations; Information systems; People with disabilities; Surveys; Usability engineering; Usability evaluation; Usability guidelines; User interfaces; Web usability; Web user interface; Websites,5,0.67,B,Automatic guideline evaluation,Rule-based systems,Source code,no,tool,mobile
Usability Evaluation Framework for Mobile Apps using Code Analysis,"Mathur, Neeraj; Karre, Sai Anirudh; Reddy, Y. Raghu",2018,conferencePaper,,,187–192,Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018,EASE,ACM,10.1145/3210459.3210480,https://doi.org/10.1145/3210459.3210480,"The increasing usage of smart-phones has resulted in mobile applications replacing or supplementing traditional web-based applications. Given the limitations of the form factor in smartphones, usability can be considered as one of the important attributes that determine the success of a mobile application. The measures available for assessing the usability of mobile applications tend to focus more on human aspects and less on the functional aspects of usability. As part of this paper, we propose a usability evaluation framework to identify functional usability issues specific to mobile applications. This framework uses usability guidelines and code analysis to improve the usability of a mobile application. As a proof of concept, we have built an end-to-end system using the framework to validate and verify usability issues in Android mobile applications. We also generate code recommendations to implement failed usability guidelines.",Mobile Usability; Usability Guidelines; Code Analysis; Usability Evaluation; Automation; Mobile Apps,11,1.69,A,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,mobile
Emotions detection using facial expressions recognition and EEG,"Matlovic, Tomas; Gaspar, Peter; Moro, Robert; Simko, Jakub; Bielikova, Maria",2016,conferencePaper,,,18-23,2016 11th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP),SMAP,IEEE,10.1109/SMAP.2016.7753378,https://doi.org/10.1109/SMAP.2016.7753378,"The study of emotions in human-computer interaction has increased in the recent years. With successful classification of emotions, we could get instant feedback from users, gain better understanding of the human behavior while using the information technologies and thus make the systems and user interfaces more emphatic and intelligent. In our work, we focused on two approaches, namely emotions detection using facial expressions recognition and electroencephalography (EEG). Firstly, we analyzed existing tools that employ facial expressions recognition for emotion detection and compared them in a case study in order to acquire the notion of the state-of-the-art. Secondly, we proposed a method of emotion detection using EEG that employs existing machine learning approaches. We evaluated it on a standard dataset as well as with an experiment, in which participants watched emotion-evoking music videos. We used Emotiv Epoc to capture participants' brain activity. We achieved 53% accuracy in classifying a correct emotion, which is better compared to 19% accuracy of the existing facial expression based tool Noldus FaceReader.",DEAP dataset; EEG; Electroencephalography; Emotion recognition; emotions detection; Emotiv Epoc; Face; Face recognition; facial expressions recognition; Noldus FaceReader; Sensors; Skin; usability; Videos,71,8.35,C,Affective state detection,"Deep learning, Machine learning, Computer vision","Physiological signals, Video",yes,prototype,desktop
Improving the Design of a User Interface Through Automated Usability Checks,"Meixner, Gerrit",2021,journalArticle,23,4,101-105,IT Professional,,IEEE,10.1109/MITP.2021.3052221,https://doi.org/10.1109/MITP.2021.3052221,"Software engineers usually do not have enough knowledge in the area of usability to develop user interfaces with a high degree of usability on their own. An initial study with 20 software engineers showed that, although usability is an important aspect for software engineers, they only spend an average of 18.14 h per project on usability-related tasks. An integrated software support solution that incorporates relevant usability knowledge would be extremely useful. Based on an analysis, a plug-in for Microsoft Visual Studio has been developed. The plug-in automatically checks usability aspects of a user interface in the background. Usability-related knowledge is stored in an ontology and accessed by the plug-in during runtime. The plug-in has been evaluated with 19 software engineers. The results are mostly positive. Users find that the plug-in is useful and supports their daily engineering work by helping them to quickly enhance the usability of their software solution.",Ontologies; OWL; Runtime; Software engineering; User interfaces; Visualization,0,0,Q3,Automatic guideline evaluation,Rule-based systems,Source code,no,tool,desktop
Visual complexity of graphical user interfaces,"Miniukovich, Aliaksei; Sulpizio, Simone; De Angeli, Antonella",2018,conferencePaper,,,1–9,Proceedings of the 2018 International Conference on Advanced Visual Interfaces,AVI,ACM,10.1145/3206505.3206549,https://doi.org/10.1145/3206505.3206549,"Graphical User Interfaces (GUIs) of low visual complexity tend to have higher aesthetics, usability and accessibility, and result in higher user satisfaction. Despite a few authors recently used or studied visual complexity, the concept of visual complexity still needs to be better defined for the use in HCI research and GUI design, with its underlying aspects systematized and operationalized, and different measures validated. This paper reviews the aspects of GUI visual complexity and operationalizes four aspects with nine computation-based measures in total. Two user studies validated the measures on two types of stimuli - webpages (study 1, n = 55) and book pages (study 2, n = 150) - with two user groups, dyslexics (people with reading difficulties) and typical readers. The same complexity aspects could be expected to determine complexity perception for both GUI types, whereas different complexity aspects could be expected to determine complexity perception for dyslexics, relative to typical readers. However, the studies showed little to no difference between dyslexics and average readers, whereas web pages did differ from book pages in what aspects made them seem complex. It was not the intergroup differences, but the stimulus type that defined criteria to judge visual complexity. Future research and visual design could rely on the visual complexity aspects outlined in this paper.",Facets of Visual Complexity; Computation of Visual Complexity; Dyslexia; eBooks; Webpages; GUI Research and Design,37,5.69,B,Visual complexity evaluation,Descriptive metrics,Images,no,prototype,desktop
Automated reporting of GUI design violations for mobile apps,"Moran, Kevin; Li, Boyang; Bernal-Cárdenas, Carlos; Jelf, Dan; Poshyvanyk, Denys",2018,conferencePaper,,,165–175,Proceedings of the 40th International Conference on Software Engineering,ICSE,ACM,10.1145/3180155.3180246,https://doi.org/10.1145/3180155.3180246,"The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers and developers at Huawei to improve the quality of their mobile apps.",,123,18.92,A*,Automatic guideline evaluation,Rule-based systems,"Images, Source code",no,tool,mobile
A Review of Automated Website Usability Evaluation Tools: Research Issues and Challenges,"Namoun, Abdallah; Alrehaili, Ahmed; Tufail, Ali",2021,conferencePaper,12779 LNCS,,292 – 311,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-030-78221-4_20,https://doi.org/10.1007/978-3-030-78221-4_20,"Web usability is a critical factor for visitors' acceptance and satisfaction. The prevalence of various free and proprietary website testing tools enabled the fast evaluation of the usability of websites. However, their effectiveness in providing meaningful, consistent, and valid results remains questionable. In this study, we initially devised a usability framework that incorporates 19 usability dimensions and investigated the compliance of 10 popular web usability testing tools to this framework. Next, we applied the automated evaluation to nine websites spanning three major categories, namely e-commerce, vacation rentals, and education. On a positive note, the tools inspected a wide range of aspects, including performance, SEO, page size, accessibility, and security. However, our in-depth analysis revealed numerous critical issues. First, usability seems to be ignored or distorted by most of the tools. Second, the automated tools exhibited variable and contradictory scores concerning the analysis of the same websites. Third, the analysis reports were vague and complicated for non-technical users. Fourth, the tools identified and explained issues affecting the technical implementation of the websites, overlooking web usability flaws. Fifth, only four tools (i.e., SEOptimiser, Dareboost, Website Grader, and Sure Oak) gave recommendations to remedy performance bottlenecks and improve the quality of the websites. Lastly, the inner working of the tools does not seem to incorporate the theoretical foundations of usability, which calls for urgent collaboration between industry experts and HCI practitioners and researchers. © 2021, Springer Nature Switzerland AG.",Usability engineering; Automation; Usability testing; Usability evaluation; Automated web evaluation tools; Automatic web usability evaluation; Perceived usability; Web usability issues; Website design; Web Design; Evaluation tool; Automated web evaluation tool; Web evaluations; Web usability; Web usability issue,29,8.29,B,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,"desktop, mobile"
Aalto Interface Metrics (AIM): A Service and Codebase for Computational GUI Evaluation,"Oulasvirta, Antti; De Pascale, Samuli; Koch, Janin; Langerak, Thomas; Jokinen, Jussi; Todi, Kashyap; Laine, Markku; Kristhombuge, Manoj; Zhu, Yuxi; Miniukovich, Aliaksei; Palmas, Gregorio; Weinkauf, Tino",2018,conferencePaper,,,16–19,Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology,UIST,ACM,10.1145/3266037.3266087,https://doi.org/10.1145/3266037.3266087,"Aalto Interface Metrics (AIM) pools several empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs. Users input a GUI design via URL, and select from a list of 17 different metrics covering aspects ranging from visual clutter to visual learnability. AIM presents detailed breakdowns, visualizations, and statistical comparisons, enabling designers and practitioners to detect shortcomings and possible improvements. The web service and code repository are available at interfacemetrics.aalto.fi.",,63,9.69,A,Visual complexity evaluation,Descriptive metrics,Images,no,tool,desktop
A comparative study of accessibility and usability of norwegian university websites for screen reader users based on user experience and automated assessment,"Parajuli, Prabin; Eika, Evelyn",2020,conferencePaper,12188 LNCS,,300 – 310,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-030-49282-3_21,https://doi.org/10.1007/978-3-030-49282-3_21,"Websites are essential for learners’ access to information. However, due to the lack of accessibility and usability of websites, students with disabilities who solely rely on screen readers face challenges accessing webpage contents. This study explores accessibility and usability issues frequently encountered by screen reader students while interacting with Norwegian university webpages. An evaluation using automated tools showed that none of the university websites met the minimum WCAG 2.1 guidelines. Sixteen visually impaired participants were recruited and assigned five usability tasks on four different university websites. The results show that participants encountered usability and accessibility issues on all four websites. Recommendations for increased accessibility are proposed based on the findings. © Springer Nature Switzerland AG 2020.",Automated assessment; Automated tools; Comparative studies; Human computer interaction; Norwegian University; Screen readers; Usability engineering; User experience; Visually impaired; Web Design; Websites,13,2.89,B,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,"desktop, mobile"
Timelines for mobile web usability evaluation,"Paternò, F.; Schiavone, A.G.; Pitardi, P.",2016,conferencePaper,07-10-June-2016,,88 – 91,Proceedings of the Workshop on Advanced Visual Interfaces AVI,AVI,ACM,10.1145/2909132.2909272,https://doi.org/10.1145/2909132.2909272,"In the field of Web usability evaluation, one potential effective approach is the usage of logging tools for remote usability analysis (i.e. tools capable of tracking and recording the users' activities while they interact with a Web site), and then presenting the recorded data to usability experts in such a way to support detection of possible usability problems. In the design of such automated tools, in addition to the problems related to user behavior recording, another important issue is the choice of meaningful visual representations in order to support the usability expert analysis. In this paper we present a timeline-based system for interactive events visualization, which has been exploited in a proxy-based mobile Web usability evaluation tool. We discuss how such visualizations can be exploited in finding usability issues and the type of problems that can be detected through them. We show various ways to compare timelines related to users sessions with ideal timelines representing optimal behavior. © 2016 Copyright held by the owner/author(s).",Behavioral research; Effective approaches; Expert analysis; Mobile web; Remote usability; Remote usability evaluations; Timelines; Usability engineering; Usability problems; Visual representations; Visualization; Well logging,22,2.59,B,Usability issue encounter detection,Descriptive metrics,User interactions,yes,prototype,mobile
Customizable automatic detection of bad usability smells in mobile accessed web applications,"Paternò, Fabio; Schiavone, Antonio Giovanni; Conti, Antonio",2017,conferencePaper,,,,Proceedings of the 19th international conference on human-computer interaction with mobile devices and services,MOBILEHCI,ACM,10.1145/3098279.3098558,https://doi.org/10.1145/3098279.3098558,"Remote usability evaluation enables the possibility of analysing users' behaviour in their daily settings. We present a method and an associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions. Such log analysis is based on the identification of specific usability smells. We describe an example set of bad usability smells, and how they are detected. The tool also allows evaluators to add new usability smells not included in the original set. We also report on the tool use in analysing the usability of a real, widely used application accessed by forty people through their smartphones whenever and wherever they wanted.",Usability engineering; Human computer interaction; Mobile devices; WEB application; Automatic Detection; Odors; remote usability evaluation; Remote usability evaluations; Remote Usability Evaluation; Bad smells; Log analysis; Associated tool; usability bad smells; web mobile application log analysis; Usability Bad Smells; Web Mobile application log analysis; Client sides; Customizable,53,7.07,B,Usability issue encounter detection,Log-based sequence & pattern mining,User interactions,yes,tool,mobile
Accessibility and usability analysis of Indian e-government websites,"Paul, Surjit; Das, Saini",2020,journalArticle,19,4,949 – 957,Universal Access in the Information Society,,Springer Nature,10.1007/s10209-019-00704-8,https://doi.org/10.1007/s10209-019-00704-8,"E-government provides a platform to deliver government services to stakeholders, and many countries have adopted e-government websites for good governance. Successful e-government implementation and service delivery depend on how the underlying information present in government websites is made accessible and usable to every individual. This article investigates the accessibility and usability of e-government sites in India. Our sample consists of 65 Indian e-government websites. The analysis was carried out using automatic evaluation tools. The results of the accessibility tests highlight the existence of accessibility issues based on Web Content Accessibility Guidelines 1.0 (WCAG 1.0) and WCAG 2.0. Usability tests also reveal that e-government websites give low priority to such aspects during website design and development. Hence, there is a need to improve the overall accessibility and usability of Indian e-government websites in order to improve the quality and in turn the E-Government Development Index rank of India. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Automatic evaluation tool; Design and Development; e-government; E-government implementation; Government services; Government websites; Service delivery; Usability analysis; Usability engineering; Web content accessibility guidelines; Web Design; Websites,58,12.89,Q2,Usability attribute evaluation,Descriptive metrics,Source code,no,existing tools usage,"desktop, mobile"
Deep learning for automatic usability evaluations based on images: A case study of the usability heuristics of thermostats,"Ponce, Pedro; Balderas, David; Peffer, Therese; Molina, Arturo",2018,journalArticle,163,,111-120,Energy and Buildings,,Elsevier,10.1016/j.enbuild.2017.12.043,https://doi.org/10.1016/j.enbuild.2017.12.043,"Thermostats are designed for increasing requirements on indoor thermal comfort. Nevertheless, they are critical devices for saving energy in buildings and households. However, when thermostats do not accomplish the usability requirements, the end-users do not save energy. Then, when a thermostat is designed or validated, one of the leading problems that must be tackled is the usability evaluation. Generally, the evaluation is based on usability heuristics that are done by experts and designers and involve a very complicated cycling process in which usability experts need to be included in the complete usability evaluation. On the other hand, there are several proposals for generating an automatic usability analysis that can be used by designers or end-users. However, they are limited by the methodologies that are implemented in the evaluation because usability evaluations necessitate a large amount of data abstraction, and the amount of processed information is enormous; As an alternative, Artificial Intelligence can help to solve this problem, especially machine learning techniques with deep learning capabilities that can reach a high level of data abstraction with a significant amount of information and implement an automatic usability evaluation based on images. Convolutional networks that are included in deep learning can classify complex problems, attain highly accurate results. This paper proposes to train a convolutional network with standard usability heuristics for evaluating usability, which is an easy method for evaluating usability in thermostats, based on images. The proposed automatic method gives excellent results for evaluating usability heuristics in the heuristic assigned. This paper provides a complete methodology, using deep learning, for automatically evaluating the usability heuristics of thermostats. (C) 2017 Elsevier B.V. All rights reserved.",Deep learning; Usability engineering; Automatic usability evaluation; Images; Thermostats; Artificial intelligence; Learning systems; Convolution; Usability evaluation; Machine learning techniques; Heuristic methods; Usability requirements; Indoor thermal comfort; Abstracting; Amount of information; Convolutional networks; Learning capabilities; Problem solving,27,4.15,Q1,Automatic guideline evaluation,"Deep learning, Computer vision",Images,no,prototype,thermostats
Operationalizing Engagement with Multimedia as User Coherence with Context,"Poore, Joshua C.; Webb, Andrea K.; Cunha, Meredith G.; Mariano, Laura J.; Chappell, David T.; Coskren, Mikaela R.; Schwartz, Jana L.",2017,journalArticle,8,1,95-107,IEEE Transactions on Affective Computing,,IEEE,10.1109/TAFFC.2015.2512867,https://doi.org/10.1109/TAFFC.2015.2512867,"Traditional approaches for assessing user engagement within multimedia environments rely on methods that are removed from the human computer interaction itself, such as surveys, interviews and baselined physiology. We propose a context coherence approach that operationalizes engagement as the amount of independent user variation that covaries in time with multimedia contextual events during unscripted interactions. This can address questions about the features of multimedia users are most engaged and how engaged users are without the need for prescribed interactions or baselining. We assessed the validity of this approach in a psychophysiological study. Forty participants played interactive video games. Intake and post-stimulus questionnaires collected subjective engagement reports that provided convergent and divergent validity criteria to evaluate our approach. Estimates of coherence between physiological variation and in-game contextual events predicted subjective engagement and added information beyond physiological metrics computed from baselines taken outside of the multimedia context. Our coherence metric accounted for task-dependent engagement, independent of predispositions; this was not true of a baselined physiological approach that was used for comparison. Our findings show compelling evidence that a context-sensitive approach to measuring engagement overcomes shortcomings of traditional methods by making best use of contextual information sampled from multimedia in time-series analyses.",Coherence; Context; electromyography; Engagement; eye-tracking; Games; human computer interaction; Human computer interaction; immersion; Measurement; Multimedia communication; physiology; Physiology; user experience,4,0.53,Q1,Usability attribute evaluation,Machine learning,"User interactions, Gaze data, Physiological signals",yes,prototype,desktop
Predictive Cognitive Modelling of Applications,"Prezenski, Sabine; Bruechner, Dominik; Russwinkel, Nele",2017,conferencePaper,3,,165-171,International Conference on Human Computer Interaction Theory and Applications,HUCAPP,SCITEPRESS,10.5220/0006273301650171,https://doi.org/10.5220/0006273301650171,"This paper argues that important usability aspects of mobile applications can be automatically evaluated using computational cognitive models based on the cognitive architecture ACT-R. A tool incorporating cognitive models for specific tasks, users, applications and usability aspects is proposed. Explanations provided by the tool for usability flaws are based on simulations of cognitive mechanisms. A use-case of the tool is introduced, which is based on an ACT-R model that simulates how users search and select a specific target in a hierarchical android application and predicts efficiency and learnability for average users. The model has been empirically validated in four studies with two different applications. To fully automate the usability evaluation of the use-case, two basic requirements need to be fulfilled. First, the application and the cognitive model have to be connected. A tool called ACT-Droid acts as an interface between the Android application and the cognitive model. Second, the models knowledge of the world, which is application specific, has to be provided automatically by using an automated user interface analysation approach. Therefore, the open-source tool AppCrawler was extended to allow the extraction of the required information.",Usability; User interfaces; Usability engineering; Tools; Computer vision; Computer graphics; Tool; Android (operating system); Usability tests; Computation theory; ACT-R; Cognitive modelling; Usability Criteria; Usability Test; Cognitive Modelling,5,0.67,B,Usability attribute evaluation,Rule-based systems,Source code,no,prototype,mobile
Predicting task execution times by deriving enhanced cognitive models from user interface development models,"Quade, Michael; Halbrügge, Marc; Engelbrecht, Klaus-Peter; Albayrak, Sahin; Möller, Sebastian",2014,conferencePaper,,,139 – 148,Proceedings of the 2014 acm sigchi symposium on engineering interactive computing systems,EICS,ACM,10.1145/2607023.2607033,https://doi.org/10.1145/2607023.2607033,"Adaptive user interfaces (UI) offer the opportunity to adapt to changes in the context, but this also poses the challenge of evaluating the usability of many different versions of the resulting UI. Consequently, usability evaluations tend to become very complex and time-consuming. We describe an approach that combines model-based usability evaluation with development models of adaptive UIs. In particular, we present how a cognitive user behavior model can be created automatically from UI development models and thus save time and costs when predicting task execution times. With the help of two usability studies, we show that the resulting predictions can be further improved by using information encoded in the UI development models. Copyright © 2014 ACM 978-1-4503-2725-1/14/06.",Adaptive user interface; Automated usability evaluation; Behavioral research; Computer simulation; Computer systems; Human computer interaction; Model based development; Model-based usability evaluation; Simulation; Usability engineering; Usability evaluation; User behavior modeling; User interface development; User interfaces,12,1.14,B,Usability attribute evaluation,Rule-based systems,"Source code, User interactions",yes,prototype,desktop
UnSkEm: Unobtrusive Skeletal-based Emotion Recognition for User Experience,"Razzaq, Muhammad Asif; Bang, Jaehun; Kang, Sunmoo Svenna; Lee, Sungyoung",2020,conferencePaper,,,92-96,2020 International Conference on Information Networking (ICOIN),ICOIN,IEEE,10.1109/ICOIN48656.2020.9016601,https://doi.org/10.1109/ICOIN48656.2020.9016601,"In this paper, the proposed framework utilizes body joint movement patterns extracted from skeletal joint features from Kinect v2 sensor in order to recognize emotions. Instead of using traditional methods for feature learning such as feature clustering, we proposed two methods Mesh Distance Features and Mesh Angular features to represent highly accurate body postures. For these methods, we only considered upper body joints which were 15 in number. Recognition of human emotion is performed using Support Vector Machine (SVM) which is train with Sequential Minimal Optimization (SMO). The contribution of this paper is two-fold. Firstly it uses a limited set of skeletal joints instead of tracking whole-body joint coordinates. Secondly, it uses the proposed methods of MAD and MAF for feature extraction. The proposed framework recognizes six emotions (Anger, Happiness, Sadness, Neutral, Surprise, and Fear) over the dataset collected for evaluating the User Experience platform. The experimental results show promising higher accuracies for emotional state recognition in real-time.",Emotion recognition; Feature extraction; Kinect v2; Shoulder; skeletal joint data; Skeleton; SMO; Support vector machines; SVM; Three-dimensional displays; Training,11,2.44,B,Affective state detection,Machine learning,User movements,yes,prototype,desktop
A Hybrid Multimodal Emotion Recognition Framework for UX Evaluation Using Generalized Mixture Functions,"Razzaq, Muhammad Asif; Hussain, Jamil; Bang, Jaehun; Hua, Cam-Hao; Satti, Fahad Ahmed; Rehman, Ubaid Ur; Bilal, Hafiz Syed Muhammad; Kim, Seong Tae; Lee, Sungyoung",2023,journalArticle,23,9,,Sensors,,MDPI,10.3390/s23094373,https://doi.org/10.3390/s23094373,"Multimodal emotion recognition has gained much traction in the field of affective computing, human-computer interaction (HCI), artificial intelligence (AI), and user experience (UX). There is growing demand to automate analysis of user emotion towards HCI, AI, and UX evaluation applications for providing affective services. Emotions are increasingly being used, obtained through the videos, audio, text or physiological signals. This has led to process emotions from multiple modalities, usually combined through ensemble-based systems with static weights. Due to numerous limitations like missing modality data, inter-class variations, and intra-class similarities, an effective weighting scheme is thus required to improve the aforementioned discrimination between modalities. This article takes into account the importance of difference between multiple modalities and assigns dynamic weights to them by adapting a more efficient combination process with the application of generalized mixture (GM) functions. Therefore, we present a hybrid multimodal emotion recognition (H-MMER) framework using multi-view learning approach for unimodal emotion recognition and introducing multimodal feature fusion level, and decision level fusion using GM functions. In an experimental study, we evaluated the ability of our proposed framework to model a set of four different emotional states (Happiness, Neutral, Sadness, and Anger) and found that most of them can be modeled well with significantly high accuracy using GM functions. The experiment shows that the proposed framework can model emotional states with an average accuracy of 98.19% and indicates significant gain in terms of performance in contrast to traditional approaches. The overall evaluation results indicate that we can identify emotional states with high accuracy and increase the robustness of an emotion classification system required for UX measurement.",user experience; User interfaces; electroencephalography; Electroencephalography; Artificial Intelligence; Human computer interaction; Psychology; artificial intelligence; Speech recognition; procedures; Emotions; Learning; Algorithms; learning; algorithm; Emotion Recognition; human; Humans; Function evaluation; Users' experiences; emotion; physiology; recognition; Recognition; Emotion recognition; Audio-based; Audio-based emotion recognition; Decision fusioning; Feature fusioning; Generalized mixture function; Generalized mixtures; Multimodal emotion recognition; emotion recognition; audio-based emotion recognition; decision fusioning; feature fusioning; generalized mixture function,9,6,Q2,Affective state detection,"Deep learning, Computer vision","Audio, Video, User movements",yes,prototype,desktop
Usability problems discovery based on the automatic detection of usability smells,"Ribeiro, Rafael Fontinele; de Meneses Campanhã Souza, Matheus; de Oliveira, Pedro Almir Martins; de Alcântara dos Santos Neto, Pedro",2019,conferencePaper,,,2328–2335,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,SAC,ACM,10.1145/3297280.3299747,https://doi.org/10.1145/3297280.3299747,"Web applications usage has gone through a big and fast growth in the past few years, becoming part of our daily lives. Large companies use these applications to provide their services. So, it is necessary to ensure the development of high-quality applications. One of the main attributes of a Web application that directly determines its success or failure is their usability, and several methods were developed to evaluate it. The most popular method of usability evaluation is the laboratory testing because it can directly evaluate users' reactions while interacting with the final application. However, performing this test entails high cost and complexity. So, aiming to simplify the execution of this evaluation and to ease the discovery of usability problems, this work proposes an approach to detect indicators of problems (usability smells), based on the capture of the user interaction in the production environment and automatic analysis of this interaction. A study carried out with the approach showed that smells indicated by it were able to aid the detection and correction of usability problems in a real application.",Usability; usability; Usability engineering; usability evaluation; refactoring; Usability evaluation; Laboratory testing; Usability problems; Automatic Detection; Refactorings; Odors; usability smells; Production environments; Usability smells; web; Web,6,1.09,B,Usability issue encounter detection,Rule-based systems,User interactions,yes,prototype,desktop
Ontology Design for Automatic Evaluation of Web User Interface Usability,"Robal, Tarmo; Marenkov, Jevgeni; Kalja, Ahto",2017,conferencePaper,,,1-8,2017 Portland International Conference on Management of Engineering and Technology,PICMET,IEEE,10.23919/PICMET.2017.8125425,https://doi.org/10.23919/PICMET.2017.8125425,"The rapid development of the Internet and associated technologies for content delivery has led to a situation where the Web can be accessed on a multitude of different platforms - from desktop computers, laptops and tablets to smart-phones, which have become a crucial part of our lives, raising the question of usability on this plethora of devices. Testing and validating user experience (UX) throughout the whole development process is costly. However, some of this work done by humans could be executed automatically starting in early development. In this paper we address web user interface (UI) automatic evaluation and in particular discuss the ontology design for capturing knowledge of web usability domain for UI evaluation.",Usability; Testing; User interfaces; Tools; Guidelines; Ontologies; Automatic evaluation; Development process; Websites; Ontology; Interactive devices; Smartphones; Web usability; User experiences (ux); Personal computers; Web user interface; Content delivery; Ontology design; UI evaluations,16,2.13,A,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,"desktop, mobile"
Deep learning-based user experience evaluation in distance learning,"Sadigov, Rahim; Yıldırım, Elif; Kocaçınar, Büşra; Patlar Akbulut, Fatma; Catal, Cagatay",2024,journalArticle,27,1,443-455,Cluster Computing,,Springer Nature,10.1007/s10586-022-03918-3,https://doi.org/10.1007/s10586-022-03918-3,"The Covid-19 pandemic caused uncertainties in many different organizations, institutions gained experience in remote working and showed that high-quality distance education is a crucial component in higher education. The main concern in higher education is the impact of distance education on the quality of learning during such a pandemic. Although this type of education may be considered effective and beneficial at first glance, its effectiveness highly depends on a variety of factors such as the availability of online resources and individuals’ financial situations. In this study, the effectiveness of e-learning during the Covid-19 pandemic is evaluated using posted tweets, sentiment analysis, and topic modeling techniques. More than 160,000 tweets, addressing conditions related to the major change in the education system, were gathered from Twitter social network and deep learning-based sentiment analysis models and topic models based on latent dirichlet allocation (LDA) algorithm were developed and analyzed. Long short term memory-based sentiment analysis model using word2vec embedding was used to evaluate the opinions of Twitter users during distance education and also, a topic model using the LDA algorithm was built to identify the discussed topics in Twitter. The conducted experiments demonstrate the proposed model achieved an overall accuracy of 76%. Our findings also reveal that the Covid-19 pandemic has negative effects on individuals 54.5% of tweets were associated with negative emotions whereas this was relatively low on emotion reports in the YouGov survey and gender-rescaled emotion scores on Twitter. In parallel, we discuss the impact of the pandemic on education and how users’ emotions altered due to the catastrophic changes allied to the education system based on the proposed machine learning-based models.",Deep learning; Sentiment analysis; NLP; Distance learning,18,36,Q2,Feedback evaluation,"Machine learning, NLP, Deep learning",Text,yes,prototype,device independent
Towards automatic evaluation of the Quality-in-Use in context-aware software systems,"Salomón, Sergio; Duque, Rafael; Montaña, José Luis; Tenés, Luis",2023,journalArticle,14,8,10321 – 10346,Journal of Ambient Intelligence and Humanized Computing,,Springer Nature,10.1007/s12652-021-03693-w,https://doi.org/10.1007/s12652-021-03693-w,"Context-aware systems adapt their services to the user’s intentions and environment to improve the user experience. However, how to evaluate the quality of these systems in terms of user perception and context recognition is still an open problem. Our goal in this work is to evaluate the Quality-in-Use (QinU) for context-aware software systems according to the ISO/IEC 25010 standard and in an automated manner. This evaluation is oriented to be model-based, with domain specification and log data as input, while quality metrics and representations of users’ behavior as output. In this process, we use probabilistic models to discover user patterns, heuristic metrics as QinU estimation, clustering techniques to obtain user profiles according to their QinU, and feature selection to identify relevant factors of context. We propose a framework for assessing the QinU in context-aware software systems called Framework for Assessing Quality-in-use of Software (FAQuiS). FAQuiS includes a set of models to represent all dimensions of context, a methodology to apply the quality analysis to any system, and a set of tools and metrics to support and automate the process. We seek to test the impact and ease of integration in the industry for this framework. A case study in a company allows us to validate the applicability in a real environment. We analyze the mechanisms that support the QinU evaluation in context-aware systems, the feasibility of the QinU quantification, and the suitability of the integration in companies. Compared to previous works, our proposal offers a novel data-driven approach with general-purpose and industrial viability. FAQuiS can be used as a solution to assess the QinU based on the ISO 25010 standard and the models of user behaviors in different contexts. This solution analyzes the context changes in the user interaction, can quantify the quality loss in these contexts, and does not require big efforts to be integrated into a software development process. © 2022, The Author(s).",Automatic evaluation; Behavioral research; Computer software selection and evaluation; Context models; Context-Aware; Context-aware systems; In contexts; Interaction analysis; ISO Standards; Quality assurance; Quality control; Quality in use; Software design; Software quality assurance; Software-systems; User behaviors; User profile,14,9.33,Q1,Usability attribute evaluation,Descriptive metrics,User interactions,yes,prototype,mobile
Applying Sentiment Analysis with Cross-Domain Models to Evaluate User eXperience in Virtual Learning Environments,"Sanchis-Font, Rosario; Castro-Bleda, Maria Jose; González, José-Ángel",2019,conferencePaper,11506 LNCS,,609 – 620,International Work-Conference on Artificial Neural Networks,IWANN,Springer Nature,10.1007/978-3-030-20521-8_50,https://doi.org/10.1007/978-3-030-20521-8_50,"Virtual Learning Environments are growing in importance as fast as e-learning is becoming highly demanded by universities and students all over the world. This paper investigates how to automatically evaluate User eXperience in this domain. Two Learning Management Systems have been evaluated, one system is an ad-hoc system called “Conecto” (in Spanish and English languages), and the other one is an open-source Moodle personalized system (in Spanish). We have applied machine learning tools to all the comments given by a total of 133 users (37 English speakers and 96 Spanish speakers) to obtain their polarity (positive, negative, or neutral) using cross-domain models trained with a corpus of a different domain (tweets for each language) and general models for the language. The obtained results are very promising and they give an insight to keep going the research of applying sentiment analysis tools on User eXperience evaluation. This is a pioneering idea to provide a better and accurate understanding on human needs in the interaction with Virtual Learning Environments. The ultimate goal is to develop further tools of automatic feed-back of user perception for designing Virtual Learning Environments centered in user’s emotions, beliefs, preferences, perceptions, responses, behaviors and accomplishments that occur before, during and after the interaction. © 2019, Springer Nature Switzerland AG.",User eXperience; User experience; Machine learning; Virtual reality; Neural networks; Learning systems; Data mining; Sentiment analysis; E-learning; Behavioral research; Computer aided instruction; Open systems; User experience evaluations; English languages; Cross-domain models; Learning management system; Virtual learning environments; Applied machine learning; Polarity; Learning Management Systems; Virtual Learning Environments,8,1.45,B,Feedback evaluation,Deep learning,Text,yes,prototype,device independent
Cross-Domain Polarity Models to Evaluate User eXperience in E-learning,"Sanchis-Font, Rosario; Castro-Bleda, Maria Jose; Gonzalez, Jose-Angel; Plal, Ferran; Hurtado, Lluis-F",2021,journalArticle,53,5,3199-3215,Neural Processing Letters,,Springer Nature,10.1007/s11063-020-10260-5,https://doi.org/10.1007/s11063-020-10260-5,"Virtual learning environments are growing in importance as fast as e-learning is becoming highly demanded by universities and students all over the world. This paper investigates how to automatically evaluate User eXperience in this domain using sentiment analysis techniques. For this purpose, a corpus with the opinions given by a total of 583 users (107 English speakers and 476 Spanish speakers) about three learning management systems in different courses has been built. All the collected opinions were manually labeled with polarity information (positive, negative or neutral) by three human annotators, both at the whole opinion and sentence levels. We have applied our state-of-the-art sentiment analysis models, trained with a corpus of a different semantic domain (a Twitter corpus), to study the use of cross-domain models for this task. Cross-domain models based on deep neural networks (convolutional neural networks, transformer encoders and attentional BLSTM models) have been tested. In order to contrast our results, three commercial systems for the same task (MeaningCloud, Microsoft Text Analytics and Google Cloud) were also tested. The obtained results are very promising and they give an insight to keep going the research of applying sentiment analysis tools on User eXperience evaluation. This is a pioneering idea to provide a better and accurate understanding on human needs in the interaction with virtual learning environments and a step towards the development of automatic tools that capture the feed-back of user perception for designing virtual learning environments centered in user's emotions, beliefs, preferences, perceptions, responses, behaviors and accomplishments that occur before, during and after the interaction.",Deep learning; User experience; Machine learning; Semantics; Convolutional neural networks; Artificial neural networks; Learning systems; Sentiment analysis; Deep neural networks; E-learning; Behavioral research; State of the art; Computer aided instruction; User experience evaluations; User perceptions; Analysis techniques; Commercial systems; Cross-domain models; Learning management system; Virtual learning environments; Learning management systems,18,5.14,Q3,Feedback evaluation,Deep learning,Text,yes,prototype,device independent
A Framework to Semi-automated Usability Evaluations Processing Considering Users' Emotional Aspects,"Santos, Flavia de Souza; Treviso, Marcos Vinicius; Gama, Sandra Pereira; de Mattos Fortes, Renata Pontin",2022,conferencePaper,13302,,419-438,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-031-05311-5_29,https://doi.org/10.1007/978-3-031-05311-5_29,"The concern with providing a good experience for users increases simultaneously with technological evolution and dissemination. Different evaluation methods are presented in the literature to help developers evaluate and verify the interfaces they develop. However, for the application of evaluation methods, it is often necessary to have an expert, or users, which can make the assessment costly, both time-consumingly and monetarily. Consequently, developers may launch their products without carefully checking some critical aspects beforehand, causing anything from the non-acceptance of the technology to even its abandonment. Carrying out part of the evaluations with users, or totally with them, in an automated way, considering the diversity of data obtained and the support of analyzes processed by a computer, presents itself as a possible alternative to be investigated to support developers of interactive systems. At the same time, considering the emotional aspects during the interaction can provide the developer with valuable information, including the acceptance of the technology. In this way, considering users' emotions can improve the automatic evaluation results, capturing and processing the user experience in the system. Thus, we present a framework (EmotiUsing) composed of semi-automated usability evaluations, considering the emotional aspects of users. The framework aims to make the analytical evaluations less subjective and streamline the evaluation process, reducing costs and time for the evaluators.",User experience; User interfaces; Automation; Automated usability evaluation; Usability evaluation; Emotions; Semi-automated support; Interactive system; Evaluation methods; Automated support; Emotion; Emotional aspect; Technological evolution; User emotions,1,0.4,B,Affective state detection,Machine learning,"User interactions, Physiological signals",yes,prototype,"desktop, mobile"
Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis,"Schoop, Eldon; Zhou, Xin; Li, Gang; Chen, Zhourong; Hartmann, Bjoern; Li, Yang",2022,conferencePaper,,,1–21,Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/3491102.3517497,https://doi.org/10.1145/3491102.3517497,"UI designers often correct false affordances and improve the discoverability of features when users have trouble determining if elements are tappable. We contribute a novel system that models the perceived tappability of mobile UI elements with a vision-based deep neural network and helps provide design insights with dataset-level and instance-level explanations of model predictions. Our system retrieves designs from similar mobile UI examples from our dataset using the latent space of our model. We also contribute a novel use of an interpretability algorithm, XRAI, to generate a heatmap of UI elements that contribute to a given tappability prediction. Through several examples, we show how our system can help automate elements of UI usability analysis and provide insights for designers to iterate their designs. In addition, we share findings from an exploratory evaluation with professional designers to learn how AI-based tools can aid UI design and evaluation for tappability issues.",Mobile UIs; Deep Learning; Explainable AI; Interpretability,30,12,A*,Perceived affordance evaluation,"Deep learning, Computer vision",Images,no,prototype,mobile
Detecting task demand via an eye tracking machine learning system,"Shojaeizadeh, Mina; Djamasbi, Soussan; Paffenroth, Randy C.; Trapp, Andrew C.",2019,journalArticle,116,,,Decision Support Systems,,Elsevier,10.1016/j.dss.2018.10.012,https://doi.org/10.1016/j.dss.2018.10.012,"Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed.",Eye tracking; Machine learning; Human computer interaction; Adaptive decision making; Cognitive effort; Task demand,94,17.09,Q1,Usability attribute evaluation,Machine learning,Gaze data,yes,prototype,desktop
Predicting Behaviour Patterns in Online and PDF Magazines with AI Eye-Tracking,"Šola, Hedda Martina and Qureshi, Fayyaz Hussain and Khawaja, Sarwar",2024,journalArticle,14,8,677,Behavioral Sciences,,MDPI,10.3390/bs14080677,https://doi.org/10.3390/bs14080677,"This study aims to improve college magazines, making them more engaging and user-friendly. We combined eye-tracking technology with artificial intelligence to accurately predict consumer behaviours and preferences. Our analysis included three college magazines, both online and in PDF format. We evaluated user experience using neuromarketing eye-tracking AI prediction software, trained on a large consumer neuroscience dataset of eye-tracking recordings from 180,000 participants, using Tobii X2 30 equipment, encompassing over 100 billion data points and 15 consumer contexts. An analysis was conducted with R programming v. 2023.06.0+421 and advanced SPSS statistics v. 27, IBM. (ANOVA, Welch’s Two-Sample t-test, and Pearson’s correlation). Our research demonstrated the potential of modern eye-tracking AI technologies in providing insights into various types of attention, including focus, engagement, cognitive demand, and clarity. The scientific accuracy of our findings, at 97–99%, underscores the reliability and robustness of our research, instilling confidence in the audience. This study also emphasizes the potential for future research to explore automated datasets, enhancing reliability and applicability across various fields and inspiring hope for further advancements in the field. © 2024 by the authors.",neuromarketing; consumer-behaviour research; AI eye tracking; AI; EEG; consumer neuroscience; college magazine research,5,10,Q2,Usability attribute evaluation,Deep learning,Gaze data,yes,existing tools usage,desktop
What Can Self-Reports and Acoustic Data Analyses on Emotions Tell Us?,"Soleimani, Samaneh; Law, Effie Lai-Chong",2017,conferencePaper,,,489–501,Proceedings of the 2017 Conference on Designing Interactive Systems,DIS,ACM,10.1145/3064663.3064770,https://doi.org/10.1145/3064663.3064770,"There are two approaches to measure people's experiences: memory-based and moment-based. Whereas the memory-based approaches are susceptible to the peak-end effect, the moment-based approaches appear to better reflect the experience of the present. In this paper, we propose that emotional assessment of think aloud verbalisations is a moment-based methodology for measuring UX. We conducted an empirical study with 46 participants in the domain of online shopping to evaluate their emotional experiences. Acoustic analysis of verbal data was used as the moment-based approach and self-report questionnaires as the retrospective or memory-based approach. Results of the study confirmed the previous finding that retrospective assessments did not reflect the actual experience. The results also suggested that retrospective evaluations of emotions were significantly correlated with the most frequently elicited emotion (i.e. modal emotion) during the interaction. In conclusion these results support the use of acoustic data analysis as an alternative approach to measuring UX.",User experience; evaluation; emotion; acoustic analysis,19,2.53,A,Affective state detection,Machine learning,Audio,yes,prototype,mobile
Assessing the quality of mobile graphical user interfaces using multi-objective optimization,"Soui, Makram; Chouchane, Mabrouka; Mkaouer, Mohamed Wiem; Kessentini, Marouane; Ghedira, Khaled",2020,journalArticle,24,10,7685-7714,Soft Computing,,Springer Nature,10.1007/s00500-019-04391-8,https://doi.org/10.1007/s00500-019-04391-8,"Aesthetic defects are a violation of quality attributes that are symptoms of bad interface design programming decisions. They lead to deteriorating the perceived usability of mobile user interfaces and negatively impact the User’s eXperience (UX) with the mobile app. Most existing studies relied on a subjective evaluation of aesthetic defects depending on end-users feedback, which makes the manual evaluation of mobile user interfaces human-centric, time-consuming, and error-prone. Therefore, recent studies have dedicated their effort to focus on the definition of mathematical formulas that each targets a specific structural quality of the interface. As the UX is tightly dependent on the user profile, the combination and calibration of quality attributes, formulas, and user’s characteristics, when defining a defect, are not straightforward. In this context, we propose a fully automated framework which combines literature quality attributes with the user’s profile to identify aesthetic defects of MUI. More precisely, we consider the mobile user interface evaluation as a multi-objective optimization problem where the goal is to maximize the number of detected violations while minimizing the detection complexity of detection rules and enhancing the interfaces overall quality in means of guidance and coherence coverage. We conducted a comparative study of several evolutionary algorithms in terms of accurately identifying aesthetic defects. We reported their performance in solving the proposed search-based multi-objective optimization problem. The results confirm the efficiency of the indicator-based evolutionary algorithm in terms of assessing the developers in detecting typical defects and also in generating the most accurate detection rules.",Mobile user interface; Aesthetic quality; Evaluation,42,9.33,Q2,Automatic guideline evaluation,Rule-based systems,Source code,no,prototype,mobile
CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces,"Soure, Ehsan Jahangirzadeh; Kuang, Emily; Fan, Mingming; Zhao, Jian",2022,journalArticle,28,1,643-653,IEEE Transactions on Visualization and Computer Graphics,,IEEE,10.1109/TVCG.2021.3114822,https://doi.org/10.1109/TVCG.2021.3114822,"Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.",Acoustics; collaboration; Collaboration; Feature extraction; machine learning; Machine learning; think-aloud; Tools; Usability; usability problems; User experience; video analysis; Videos; visual analytics,23,9.2,Q1,Research assistants,"NLP, Computer vision","Audio, Video",yes,tool,mobile
User Experience Evaluation Using Mouse Tracking and Artificial Intelligence,"Souza, Kennedy E. S.; Seruffo, Marcos C. R.; De Mello, Harold D.; Souza, Daniel Da S.; Vellasco, Marley M. B. R.",2019,journalArticle,7,,96506 – 96515,IEEE Access,,IEEE,10.1109/ACCESS.2019.2927860,https://doi.org/10.1109/ACCESS.2019.2927860,"Business platform models frequently require continuous adaptation and agility to allow new experiences to be created and delivered to customers. To understand user behavior in online systems, researchers have taken advantage of a combination of traditional and recently developed analysis techniques. Earlier studies have shown that user behavior monitoring data, as obtained by mouse tracking, can be utilized to improve user experience (UX). Many mouse-tracking solutions exist; however, the vast majority is proprietary, and open-source packages do not provide the resources and data needed to support UX research. Thus, this paper presents: 1) the development of an interaction monitoring application titled Artificial Intelligence and Mouse Tracking-based User eXperience Tool (AIMT-UXT); 2) the validation of the tool in a case study conducted on the Website of the Brazilian Federal Revenue Service (BFR); 3) the definition of a new relationship pattern of variables that determine user behavior; 4) the construction of a fuzzy inference system for measuring user performance using the defined variables and the data captured in the case study; and 5) the application of a clustering algorithm to complement the analysis. A comparison of the results of the applied quantitative methodologies indicates that the developed framework was able to infer UX scores similar to those reported by users in questionnaires. © 2013 IEEE.",Usability; User experience; User interfaces; Tools; Surveys; Computer science; Artificial intelligence; Monitoring; artificial intelligence; Clustering algorithms; ergonomics; Fuzzy logic; Ergonomics; Behavioral research; Mice; Online systems; Fuzzy inference; Fuzzy inference systems; User experience evaluations; Mammals; User experiences (ux); Analysis techniques; Inference engines; Interaction monitoring; Open source package; Quantitative methodology; Taxation; User behavior monitoring; computer science,47,8.55,Q1,Usability attribute evaluation,Machine learning,User interactions,yes,tool,desktop
"An Evaluation Framework for User Experience Using Eye Tracking, Mouse Tracking, Keyboard Input, and Artificial Intelligence: A Case Study","Souza, Kennedy Edson Silva de; Aviz, Igor Leonardo de; Mello, Harold Dias de; Figueiredo, Karla; Vellasco, Marley Maria Bernardes Rebuzzi; Costa, Fernando Augusto Ribeiro; Seruffo, Marcos Cesar da Rocha",2022,journalArticle,38,7,646-660,International Journal of Human-Computer Interaction,,Taylor & Francis,10.1080/10447318.2021.1960092,https://doi.org/10.1080/10447318.2021.1960092,"User eXperience (UX) has been used to achieve improvements in digital information systems based on how people perceive them. In particular, this paper establishes a framework that employs methods for eye and mouse tracking, keyboard input, self-assessment questionnaire and artificial intelligence algorithms to evaluate user experience and categorize users in terms of performance profiles. The results obtained with this framework are artifacts that can be used to support customizations of the User Interface (UI) on the websites. Moreover, the established framework is generic and flexible and can be applied to any information system, such as the case study shown in the website of the Federal Revenue of Brazil (RFB). The main objectives of this paper are as follows: (i) to set out a powerful UX framework based on three tracking techniques - the AIT2-UX; (ii) to provide the T2-UXT to collect, collate, process and visualize data obtained from users' interactions (iii) to use and compare machine learning algorithms with the classification of user performance profiles; (iv) to use the artifacts generated by the framework to manually customize the UI with the website.",Eye tracking; User experience; Machine learning; User interfaces; Information systems; Learning algorithms; Information use; Websites; Evaluation framework; Mammals; User experiences (ux); Artificial intelligence algorithms; Digital information systems; Performance profile; Self assessment; Tracking techniques; User performance,27,10.8,Q2,Usability attribute evaluation,"Deep learning, Machine learning","User interactions, Gaze data",yes,tool,"desktop, mobile"
Ensuring Web Interface Quality through Usability-Based Split Testing,"Speicher, Maximilian; Both, Andreas; Gaedke, Martin",2014,conferencePaper,,,93-110,Web Engineering: 14th International Conference,ICWE,Springer Nature,10.1007/978-3-319-08245-5_6,https://doi.org/10.1007/978-3-319-08245-5_6,"Usability is a crucial quality aspect of web applications, as it guarantees customer satisfaction and loyalty. Yet, effective approaches to usability evaluation are only applied at very slow iteration cycles in today’s industry. In contrast, conversion-based split testing seems more attractive to e-commerce companies due to its more efficient and easy-to-deploy nature. We introduce Usability-based Split Testing as an alternative to the above approaches for ensuring web interface quality, along with a corresponding tool called WaPPU. By design, our novel method yields better effectiveness than using conversions at higher efficiency than traditional evaluation methods. To achieve this, we build upon the concept of split testing but leverage user interactions for deriving quantitative metrics of usability. From these interactions, we can also learn models for predicting usability in the absence of explicit user feedback. We have applied our approach in a split test of a real-world search engine interface. Results show that we are able to effectively detect even subtle differences in usability. Moreover, WaPPU can learn usability models of reasonable prediction quality, from which we also derived interaction-based heuristics that can be instantly applied to search engine results pages.",Usability; Metrics; Heuristics; Interaction Tracking; Search Engines; Interfaces; Context-Awareness,50,4.76,B,Usability attribute evaluation,Machine learning,User interactions,yes,tool,desktop
Towards detection of usability issues by measuring emotions,"Stefancova, Elena; Moro, Robert; Bielikova, Maria",2018,conferencePaper,909,,63 – 70,New Trends in Databases and Information Systems,ADBIS,Springer Nature,10.1007/978-3-030-00063-9_8,https://doi.org/10.1007/978-3-030-00063-9_8,"User Experience is one of the most important criteria when designing and testing user interfaces with emotions as its essential element. To assess, how emotions could be used for automatic detection of usability issues, we carried out a user study with a website which included intentionally inserted usability issues. We classified valence of emotions, i.e., negative vs. positive ones based on data from electroencephalography (EEG) and facial expressions recognition. The study results confirmed that usability issues cause negative emotional response of the user and that presence of a negative emotion is a good predictor of a usability issue presence. When detecting negative and positive emotional states from the acquired dataset, we achieved the accuracy of 94% for samples with seconds granularity and 70% for the task granularity. © Springer Nature Switzerland AG 2018.",Usability; User interfaces; Usability engineering; Electroencephalography; Data analysis; Information systems; Emotions; Information use; Data reduction; Automatic Detection; Electrophysiology; Facial Expressions; EEG; Emotional response; Essential elements; Facial expressions recognition; Negative emotions; Facial expressions,6,0.92,B,Affective state detection,"Machine learning, Computer vision","Video, Physiological signals",yes,prototype,desktop
A Process to Support the Remote Tree Testing Technique for Evaluating the Information Architecture of User Interfaces in Software Projects,"Tapia, Alejandro; Moquillaza, Arturo; Aguirre, Joel; Falconi, Fiorella; Lecaros, Adrian; Paz, Freddy",2022,conferencePaper,13321 LNCS,,75 – 92,International Conference on Human-Computer Interaction,HCII,Springer Nature,10.1007/978-3-031-05897-4_6,https://doi.org/10.1007/978-3-031-05897-4_6,"Nowadays, due to technological advancement, people are bound to use multiple digital tools and interact with them through a User Interface. For this reason, User Experience (UX) is one of the most important keys to success. UX includes the design and evaluation of an adequate Information Architecture. To design Information Architecture, the best-known technique is Card Sorting. For evaluation, there is the Tree Testing or Reverse Card Sorting technique. This technique can be applied in remote or non-remote ways. We identified three main issues: 1. The remote processes to apply Tree Testing are not standardized; 2. the tools were modeled and built after a process defined by the supplier of those tools, most of which are now discontinued. 3. In many development projects these techniques are left aside, the Information Architecture is assessed using other methods and techniques (i.e. User Testing) in later phases of the project. These result in Information Architecture errors and defects found late or not found at all. Hence, this study is focused on developing and validating a standardized remote Tree Testing process with an emphasis on automation. To meet the objectives, we conducted interviews and literature reviews to find out how Tree Testing is currently carried out and what tools are used to support the technique (AS-IS workflow). Subsequently, we proposed a process that can support the technique, considering the pain points found on the current processes, resulting in a TO-BE workflow. As a result, a proposal of this standardized process was modeled using the notation BPMN, and validated by the expert judgment of specialists in Human-Computer Interaction (HCI). Finally, it is important to mention that Tree Testing is useful, practical in its remote application, and should be applied in the early stages of software projects; a tool to support the whole process should be implemented. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Application programs; Architecture information; BPMN; Card-sorting; Digital devices; Forestry; Human computer interaction; Information architectures; Information management; Information retrieval; Software project; Software testing; Technological advancement; Testing; Testing technique; Tree testing; User interfaces; Users' experiences; Work-flows,6,2.4,B,Usability issue encounter detection,Descriptive metrics,User interactions,yes,concept,"desktop, mobile"
Predicting User Engagement in Longitudinal Interventions with Virtual Agents,"Trinh, Ha; Shamekhi, Ameneh; Kimani, Everlyne; Bickmore, Timothy W.",2018,conferencePaper,,,9–16,Proceedings of the 18th International Conference on Intelligent Virtual Agents,IVA,ACM,10.1145/3267851.3267909,https://doi.org/10.1145/3267851.3267909,"Longitudinal agent-based interventions only work if people continue using them on a regular basis, thus identifying users who are at risk of disengaging from these applications is important for retention and efficacy. We develop machine learning models that predict long-term user engagement in three longitudinal virtual agent-based health interventions. We achieve accuracies of 74% to 90% in predicting user dropout in a given prediction period of the intervention based on the user's past interactions with the agent. Our models contain features related to session frequency and duration, health behavior, and user-agent dialogue content. We find that the features most predictive of dropout include number of user utterances, percent of user utterances that are questions, and the percent of user health behavior goals met during the observation period. Ramifications for the design of virtual agents for longitudinal applications are discussed.",longitudinal interventions; health interventions; dropout prediction; conversational agents; Engagement prediction,25,3.85,B,Usability attribute evaluation,Machine learning,User interactions,yes,prototype,"desktop, mobile"
LLM-powered Multimodal Insight Summarization for UX Testing,"Turbeville, Kelsey and Muengtaweepongsa, Jennarong and Stevens, Samuel and Moss, Jason and Pon, Amy and Lee, Kyra and Mehra, Charu and Villalobos, Jenny Gutierrez and Kumar, Ranjitha",2024,conferencePaper,,,4 – 11,Proceedings of the 26th International Conference on Multimodal Interaction,ICMI,ACM,10.1145/3678957.3685701,https://doi.org/10.1145/3678957.3685701,"User experience (UX) testing platforms capture many data types related to user feedback and behavior, including clickstream, survey responses, screen recordings of participants performing tasks, and participants’ think-aloud audio. Analyzing these multimodal data channels to extract insights remains a time-consuming, manual process for UX researchers. This paper presents a large language model (LLM) approach for generating insights from multimodal UX testing data. By unifying verbal, behavioral, and design data streams into a novel natural language representation, we construct LLM prompts that generate insights combining information across all data types. Each insight can be traced back to behavioral and verbal evidence, allowing users to quickly verify accuracy. We evaluate LLM-generated insight summaries by deploying them in a popular remote UX testing platform, and present evidence that they help UX researchers more efciently identify key fndings from UX tests. © 2024 Copyright held by the owner/author(s).",Data streams; Modeling languages; Natural language processing systems; Network security; Usability engineering; Datatypes; Language model; Large language model; Multi-modal; Multimodal insight; Testing platforms; Usability testing; User feedback; Users' experiences; UX research; Economic and social effects,0,0,B,Research assistants,LLM,"User interactions, Audio, Video, Source code",yes,prototype,desktop
A usability assessment of e-government websites in Sub-Saharan Africa,"Verkijika, Silas Formunyuy; De Wet, Lizette",2018,journalArticle,39,,20-29,International Journal of Information Management,,Elsevier,10.1016/j.ijinfomgt.2017.11.003,https://doi.org/10.1016/j.ijinfomgt.2017.11.003,"E-government holds enormous potential for improving the administrative efficiency of public institutions, encouraging democratic governance, and building trust between citizens/private sector and governments. However, most e-government initiatives to date have failed to attain their full potential, because they are increasingly plagued by usability issues. Consequently, there have been increasing calls for evaluating the usability of e-government websites, as they are widely considered to be the primary platform for government interaction with citizens. This study, therefore, seeks to contribute to extant knowledge by evaluating the usability of e-government websites from Sub-Saharan Africa (SSA). This is particularly important as little is known about the usability of e-government websites in the region, and worst still, it is the least advanced region in terms of e-government development. This study evaluated a total of 279 e-government websites from 31 SSA countries. The findings showed that most e-government websites in SSA were characterised by poor usability. The average usability score for the websites was 36.2%, with the most usable website having a score of only 64.8%. The study also showed that the usability of e-government websites was positively associated with the E-Government Development Index (EGDI) and the E-Participation Index (EPI).",E-government; Sub-Saharan Africa; Usability; Websites,152,23.38,Q1,Usability attribute evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Real-time detection of navigation problems on the World `Wild' Web,"Vigo, Markel; Harper, Simon",2017,journalArticle,101,,1-9,International Journal of Human-Computer Studies,,Elsevier,10.1016/j.ijhcs.2016.12.002,https://doi.org/10.1016/j.ijhcs.2016.12.002,"We propose a set of algorithms to detect navigation problems in real-time. To do so, we operationalise some navigation strategies suggested by the literature and investigate the extent to which the exhibition of these strategies is an indicator of navigation problems. Our Firefox extension senses behaviour indicative of a user experiencing interaction problems. Once these problems are detected we can suggest changes to these sites, and eventually adapt the site in real time to better accommodate the user. A remote longitudinal study monitored real website user behaviour, analysing every application event on the client side both individually and in combination. The study was conducted with 34 participants over 400 days totalling 567 h of normal usage and with no task restriction. Our sensing algorithms detected 374 issues with a 85% precision for purposeful Web use, suggesting that, indeed, when users search for specific information the exhibition of these strategies indicates the presence of problems. This contribution is novel in that, as opposed to a post-hoc analysis of user interaction, real-time detection of navigation problems at the user end opens up new research avenues in the realm of adaptive interfaces and usability analysis.",Graphical user interfaces; Navigation; User interfaces; Signal detection; Usability testing; Behavioral research; Navigation strategies; Specific information; Interaction problems; Real-time detection; Sensing algorithms; Web interactions; Web usage; Automated usability testing; Web interaction,27,3.6,Q2,Usability issue encounter detection,Rule-based systems,User interactions,yes,tool,desktop
Facilitating and automating usability testing of educational technologies,"Villamane, Mikel; Alvarez, Ainhoa",2024,journalArticle,32,3,,Computer Applications in Engineering Education,,Wiley,10.1002/cae.22725,https://doi.org/10.1002/cae.22725,"Usability evaluation is a key element to ensure a positive user experience with any software and it is especially important in educational software tools where there are many different actors involved (lecturers, students, administrators, etc.). However, evaluating usability is not an easy task for nonexpert evaluators. To facilitate this evaluation task, this article presents a Methodology for Usability Testing (MUT) and a system (CALMUT) that assists nonexpert evaluators in the application of the methodology by automatizing the calculations and facilitating their interpretation. This can be very useful for learning and instructional designers but also to people involved in the decision of introducing or not a new educational software. To develop the proposal, a literature review of different usability metrics, methods, and systems was carried out first, followed by a selection and adaptation for novice usability evaluators. This article also presents a case study where lecturers tested the usability of an educational software following the proposal and shows that using MUT and CALMUT helps people without previous experience detect the main usability problems of educational systems before deciding whether to use them or not.",Usability engineering; Software testing; Usability testing; Usability evaluation; usability testing; educational technologies; Case-studies; Users' experiences; Literature reviews; Key elements; Educational technology; Educational software; Instructional designer; Software-tools; Usability metrics,3,6,Q3,Usability attribute evaluation,Descriptive metrics,User interactions,yes,tool,desktop
ArguLens: Anatomy of Community Opinions on Usability Issues Using Argumentation Models,"Wang, Wenting; Arya, Deeksha; Novielli, Nicole; Cheng, Jinghui; Guo, Jin L.C.",2020,conferencePaper,,,,Conference on Human Factors in Computing Systems - Proceedings,CHI,ACM,10.1145/3313831.3376218,https://doi.org/10.1145/3313831.3376218,"In open-source software (OSS), the design of usability is often influenced by the discussions among community members on platforms such as issue tracking systems (ITSs). However, digesting the rich information embedded in issue discussions can be a major challenge due to the vast number and diversity of the comments. We propose and evaluate ArguLens, a conceptual framework and automated technique leveraging an argumentation model to support effective understanding and consolidation of community opinions in ITSs. Through content analysis, we anatomized highly discussed usability issues from a large, active OSS project, into their argumentation components and standpoints. We then experimented with supervised machine learning techniques for automated argument extraction. Finally, through a study with experienced ITS users, we show that the information provided by ArguLens supported the digestion of usability-related opinions and facilitated the review of lengthy issues. ArguLens provides the direction of designing valuable tools for high-level reasoning and effective discussion about usability. © 2020 ACM.",Argumentation model; Automated techniques; Conceptual frameworks; Content analysis; High-level reasoning; Human engineering; Issue Tracking; Learning systems; Open source software; Open systems; Supervised learning; Supervised machine learning; Usability engineering,38,8.44,A*,Feedback evaluation,Machine learning,Text,yes,tool,device independent
Enhancing Software User Interface Testing Through Few Shot Deep Learning: A Novel Approach for Automated Accuracy and Usability Evaluation,"Widodo, Aris Puji; Wibowo, Adi; Kurniawan, Kabul",2023,journalArticle,14,12,578 – 585,International Journal of Advanced Computer Science and Applications,,The Science and Information Organization,10.14569/IJACSA.2023.0141260,https://doi.org/10.14569/IJACSA.2023.0141260,"Traditional user interface (UI) testing methods in software development are time-consuming and prone to human error, requiring more efficient and accurate approaches. Moreover, deep learning requires extensive data training to develop accurate automated UI software testing. This paper proposes an efficient and accurate method for automating UI software testing using Deep learning with training data limitations. We propose a novel deep learning-based framework suitable for UI element analysis in data-scarce situations, focusing on Few-shot learning. Our framework initiates with several robust feature extraction modules that employ and compare sophisticated encoder models to be adept at capturing complex patterns from a sparse dataset. The methodology employs the Enrico and UI screen mistake datasets, overcoming training data limitations. Utilizing encoder models, including CNN, VGG-16, ResNet-50, MobileNet-V3, and EfficientNet-B1, the EfficientNet-B1 model excelled in the setting of Few-Shot learning with five-shot with an average accuracy of 76.05%. Our proposed model's accuracy was improved and compared to the state-of-the-art method. Our findings demonstrate the effectiveness of few-shot learning in UI screen classification, setting new benchmarks in software testing and usability evaluation, particularly in limited data scenarios. © (2023), (Science and Information Organization). All Rights Reserved.",Deep learning; User interfaces; Software testing; Training data; software testing; Learning systems; Usability evaluation; Software design; Interface testings; Software testings; Efficientnet; Few-shot; Screen classification; Signal encoding; User interface screen classification; User interface software; efficientnet; few-shot; UI screen classification,0,0,Q3,Automatic guideline evaluation,"Deep learning, Computer vision",Images,no,prototype,mobile
Modeling user psychological experience and case study in online e-learning,"Wu, Xiyuan; Liu, Min; Zheng, Qinghua; Zhang, Yunqiang; Li, Haifei",2015,journalArticle,10,6,53 – 61,International Journal of Emerging Technologies in Learning,,Kassel University Press GmbH,10.3991/ijet.v10i6.5114,https://doi.org/10.3991/ijet.v10i6.5114,"In the post WWW era, the research of e-learning focuses on facilitating intelligent and proactive services for learners. The quality of user experience determines whether e-learning services would be accepted by learners. However, many researchers traditionally focus on the effectiveness of computer systems or the accuracy of algorithms themselves rather than on user-centric psychological experience. How to model and evaluate user experience taking into account user psychological and cognitive properties are challenging research topics. There are some traditional methods typically proposed to evaluate users' psychological experience, such as interview, questionnaire etc. They are qualitative and easy to conduct but need more time and resource. And they are liable to subjective views. Based on user web log data, the current paper presents a quantitative approach of modeling user psychological experience in the context of intelligent e-learning. The properties and elements, which affect user experience, are analyzed and quantified. The holistic user experience is quantified through the fusion of analytic hierarchy process (AHP) and Delphi methods. A case study, at a university in China, is conducted for diagnosing whether the result of the proposed approach can be uniform with user subjective experience, and indicates that the proposed approach is effective and complements existing user experience research in intelligent e-learning.",Analytic hierarchy process; Analytic hierarchy process (ahp); Blogs; Cognitive properties; E-learning; Intelligent e-learning; Learning systems; Quantitative approach; Subjective experiences; Surveys; User experience; User experience research; User-centric evaluations; Web log analysis,7,0.74,Q3,Usability attribute evaluation,Rule-based systems,User interactions,yes,prototype,desktop
SimUser: Generating Usability Feedback by Simulating Various Users Interacting with Mobile Applications,"Xiang, Wei; Zhu, Hanfei; Lou, Suqi; Chen, Xinli; Pan, Zhenghua; Jin, Yuping; Chen, Shi; Sun, Lingyun",2024,conferencePaper,,,,Conference on Human Factors in Computing Systems - Proceedings,CHI,ACM,10.1145/3613904.3642481,https://doi.org/10.1145/3613904.3642481,"The confict between the rapid iteration demand of prototyping and the time-consuming nature of user tests has led researchers to adopt AI methods to identify usability issues. However, these AI-driven methods concentrate on evaluating the feasibility of a system, while often overlooking the infuence of specifed user characteristics and usage contexts. Our work proposes a tool named SimUser based on large language models (LLMs) with the Chain-of-Thought structure and user modeling method. It generates usability feedback by simulating the interaction between users and applications, which is infuenced by user characteristics and contextual factors. The empirical study (48 human users and 21 designers) validated that in the context of a simple smartwatch interface, SimUser could generate heuristic usability feedback with the similarity varying from 35.7% to 100% according to the user groups and usability category. Our work provides insights into simulating users by LLM to improve future design activities. © 2024 Copyright held by the owner/author(s)",Computational linguistics; Infuence; Iterative methods; Language model; Large language model; Mobile applications; Modeling languages; Thought structures; Usability feedback; Usage context; User characteristics; User interfaces; User simulation; User tests,2,4,A*,Automated feedback generation,"LLM, Deep learning","Source code, Images",no (synthetic),tool,wearables
Computational model for predicting user aesthetic preference for GUI using DCNNs,"Xing, Baixi; Si, Huahao; Chen, Junbin; Ye, Minchao; Shi, Lei",2021,journalArticle,3,2,147-169,CCF Transactions on Pervasive Computing and Interaction,,Springer Nature,10.1007/s42486-021-00064-4,https://doi.org/10.1007/s42486-021-00064-4,"Visual aesthetics is vital in determining the usability of the graphical user interface (GUI). It can strengthen the competitiveness of interactive online applications. Human aesthetic preferences for GUI are implicit and linked to various aspects of perception. In this study, an aesthetic GUI image database was constructed with 38,423 design works collected from Huaban.com, a popular social network website for art and design sharing, collection, and exhibition in China. The numbers of user collection and likes of each design work were used as the annotation to represent user preference levels. Deep convolutional neural networks were applied to evaluate the aesthetic preferences of GUIs, based on a large dataset of user interface design images with the ground-truth annotations. The experimental result indicated the feasibility of the proposed method, with a mean squared error (MSE) of 0.0222 for user collection prediction and an MSE of 0.0644 for user likes prediction in the best model performance of Squeeze-and-Excitation-VGG19 networks (SE-VGG19). This study aims to build a large aesthetic image database, and to explore a practical and objective evaluation model of GUI aesthetics.",Computational aesthetics; GUI aesthetic assessment; DCNNs,6,1.71,Q3,Aesthetics evaluation,"Deep learning, Computer vision",Images,no,prototype,"desktop, mobile"
Spatio-Temporal Modeling and Prediction of Visual Attention in Graphical User Interfaces,"Xu, Pingmei; Sugano, Yusuke; Bulling, Andreas",2016,conferencePaper,,,3299–3310,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,CHI,ACM,10.1145/2858036.2858479,https://doi.org/10.1145/2858036.2858479,"We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation.",Visual attention; saliency; interactive environment; graphical user interfaces; physical action; spatio-temporal modeling,58,6.82,A*,Perceived affordance evaluation,Machine learning,User interactions,yes,prototype,desktop
Measuring and Improving User Experience Through Artificial Intelligence-Aided Design,"Yang, Bin; Wei, Long; Pu, Zihan",2020,journalArticle,11,,,Frontiers in Psychology,,Frontiers Media SA,10.3389/fpsyg.2020.595374,https://doi.org/10.3389/fpsyg.2020.595374,"This paper aims to propose a methodology for measuring user experience (UX) by using artificial intelligence-aided design (AIAD) technology in mobile application design. Unlike the traditional assistance design tools, AIAD focuses on the rational use of artificial intelligence (AI) technology to measure and improve UX since conventional data collection methods (such as user interview and user observation) for user behavior data are inefficient and time-consuming. We propose to obtain user behavior data from logs of mobile application. In order to protect the privacy of users, only a few dimensions of information is used in the process of browsing and operating mobile application. The goal of the proposed methodology is to make the deep neural network model simulate the user’s experience in the process of operating a mobile application as much as possible. We design and use projected pages of application to train neural networks for specific tasks. These projected pages consist of the click information of all users in the process of completing a certain task. Thus, features of user behavior can be aggregated and mapped in the connection layers and the hidden layers. Finally, the optimized design is executed on the social communication application to verify the efficiency of the proposed methodology.",user experience; artificial intelligence aided design; human computer interaction; mobile application design; deep neural network; usability evaluation,58,12.89,Q2,Usability attribute evaluation,Deep learning,User interactions,yes,prototype,mobile
Don’t Do That! Hunting Down Visual Design Smells in Complex UIs Against Design Guidelines,"Yang, Bo; Xing, Zhenchang; Xia, Xin; Chen, Chunyang; Ye, Deheng; Li, Shanping",2021,conferencePaper,,,761-772,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),ICSE,IEEE,10.1109/ICSE43902.2021.00075,https://doi.org/10.1109/ICSE43902.2021.00075,"Just like code smells in source code, UI design has visual design smells. We study 93 don't-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don't-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design. The detection accuracy of UIS-Hunter is high (precision=0.81, recall=0.90) on the 60,756 UIs of 9,286 apps. We build a guideline gallery with real-world UI design smells that UIS-Hunter detects for developers to learn the best Material Design practices. Our user studies show that UIS-Hunter is more effective than manual detection of UI design smells, and the UI design smells that are detected by UIS-Hunter have severely negative impacts on app users.",GUI testing; UI design smell; Violation detection; Material design,52,14.86,A*,Automatic guideline evaluation,"Deep learning, Computer vision",Images,no,tool,mobile
A Hybrid User Experience Evaluation Method for Mobile Games,"Yu, Qi; Che, Xiaoping; Ma, Siqi; Pan, Shirui; Yang, Yuxiang; Xing, Weiwei; Wang, Ximeng",2018,journalArticle,6,,49067-49079,IEEE Access,,IEEE,10.1109/ACCESS.2018.2859440,https://doi.org/10.1109/ACCESS.2018.2859440,"With the development of the mobile phone industry, mobile applications market becomes thriving. However, the immature information technology and unfriendly interface bring negative user experience (UX) to the mobile users and thus affect the service life of mobile applications, especially for the most concerned entertaining applications, mobile games. As a result, the evaluating UX and finding crucial factors of UX become a challenge. Over the last decades, numerous researches have tried to deal with this issue, but none of them has clearly identified the relations among positive-negative UX, sufficient human characteristics and specific events of applications. This paper proposes a subjective-objective evaluation method. Subjective UX of mobile games is sufficiently obtained and objective UX is verified through the electrocardiogram signals and heart rate variability. In order to reveal distinct relations among UX factors, sufficient user characteristics and categories of game events, an improved RIPPER algorithm is proposed by this paper to obtain the relations. Experiments are performed with 300 testers who played mobile parkour games for at least five minutes. The accuracy and efficiency of the proposed method have been verified through real experiments and objective measures. In addition, this paper provides an effective sampling method and a data analysis algorithm to obtain crucial UX factors for mobile applications.",data analysis; Electrocardiography; Games; Heart rate variability; HRV; Land mobile radio; mobile games; Mobile handsets; Physiology; ripper; Skin; User experience,34,5.23,Q1,Usability attribute evaluation,Rule-based systems,Physiological signals,yes,prototype,desktop
Analyzing Usability of Educational Websites Using Automated Tools,"Zarish, Syed Saqib; Habib, Shabana; Islam, Muhammad",2019,conferencePaper,,,1-4,2019 International Conference on Computer and Information Sciences (ICCIS),ICCIS,IEEE,10.1109/ICCISci.2019.8716462,https://doi.org/10.1109/ICCISci.2019.8716462,"As websites are developed by organization to share information with their users/visitors, so it is very important for organization that their websites must have accessibility and usability. There are some parameters against which an organization can check its websites accessibility and usability. It can help organizations to develop an efficient and up to date website as a result they can achieve their goals. To find out the accessibility and usability of websites, in this paper we are using three automated tools Qualidator, Website Grader and Website Analyzer. This paper evaluates ten public universities of KPK, Pakistan and grade the sites according to their scores",accessibility; Agriculture; Optimization; Organizations; Qualidator; Search engines; Security; Tools; usability; Usability; Website Analyzer; Website Grader; website/websites,28,5.09,-,Usability attribute evaluation,Rule-based systems,Source code,no,existing tools usage,desktop
Seenomaly: vision-based linting of GUI animation effects against design-don't guidelines,"Zhao, Dehai; Xing, Zhenchang; Chen, Chunyang; Xu, Xiwei; Zhu, Liming; Li, Guoqiang; Wang, Jinshui",2020,conferencePaper,,,1286–1297,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,ICSE,ACM,10.1145/3377811.3380411,https://doi.org/10.1145/3377811.3380411,"GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can ""see"" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by ""seeing"" lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and real-world GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.",GUI animation; Design guidelines; Lint; Unsupervised learning,66,14.67,A*,Automatic guideline evaluation,"Deep learning, Machine learning, Computer vision",Video,no,tool,mobile
Intelligent Exploration for User Interface Modules of Mobile App with Collective Learning,"Zhou, Jingbo; Tang, Zhenwei; Zhao, Min; Ge, Xiang; Zhuang, Fuzhen; Zhou, Meng; Zou, Liming; Yang, Chenglei; Xiong, Hui",2020,conferencePaper,,,3346–3355,Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining,KDD,ACM,10.1145/3394486.3403387,https://doi.org/10.1145/3394486.3403387,"A mobile app interface usually consists of a set of user interface modules. How to properly design these user interface modules is vital to achieving user satisfaction for a mobile app. However, there are few methods to determine design variables for user interface modules except for relying on the judgment of designers. Usually, a laborious post-processing step is necessary to verify the key change of each design variable. Therefore, there is only a very limited amount of design solutions that can be tested. It is time-consuming and almost impossible to figure out the best design solutions as there are many modules. To this end, we introduce FEELER, a framework to fast and intelligently explore design solutions of user interface modules with a collective machine learning approach. FEELER can help designers quantitatively measure the preference score of different design solutions, aiming to facilitate the designers to conveniently and quickly adjust user interface module. We conducted extensive experimental evaluations on two real-life datasets to demonstrate its applicability in real-life cases of user interface module design in the Baidu App, which is one of the most popular mobile apps in China.",user interface design; collective learning; gaussian process; preference learning; user interface exploration,11,2.44,A*,Automated feedback generation,Machine learning,Source code,no,prototype,mobile